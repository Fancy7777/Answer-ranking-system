{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the python script containing the same code as the load the data notebook\n",
    "%run loadData.py\n",
    "# now we can access train, dev, and test\n",
    "# along with trainSents, devSents testSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim names for later clean\n",
    "\n",
    "train_question_set = train\n",
    "train_document_set = trainSents\n",
    "\n",
    "dev_question_set = dev\n",
    "dev_document_set = devSents\n",
    "\n",
    "test_question_set = test\n",
    "test_document_set = testSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rapid_size = 1\n",
    "\n",
    "rapid_question_set = train_question_set[:rapid_size]\n",
    "rapid_document_set = train_document_set[:rapid_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim for easier name spacing\n",
    "\n",
    "DATA = {\n",
    "    \"rapid\" : {\n",
    "            \"question_set\": rapid_question_set,\n",
    "            \"document_set\": rapid_document_set,\n",
    "    },\n",
    "    \"train\" : {\n",
    "            \"question_set\": train_question_set,\n",
    "            \"document_set\": train_document_set,\n",
    "    },\n",
    "    \"dev\" : {\n",
    "            \"question_set\": dev_question_set,\n",
    "            \"document_set\": dev_document_set,\n",
    "    },\n",
    "    \"test\" : {\n",
    "            \"question_set\": test_question_set,\n",
    "            \"document_set\": test_document_set,\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from string import punctuation  \n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import csv\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)\n",
    "\n",
    "punct_tokens = set(punctuation)\n",
    "extra_tokens = set([\"what\", \"where\", \"how\", \"when\", \"who\"])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filter_tokens = extra_tokens.union(punct_tokens).union(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_train_set = getStanfordTagging('train')\n",
    "tagged_dev_set = getStanfordTagging('dev')\n",
    "tagged_test_set = getStanfordTagging('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_rapid_set = tagged_train_set[:rapid_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim for easier name spacing\n",
    "\n",
    "DATA[\"rapid\"][\"tagged_set\"] = tagged_rapid_set\n",
    "DATA[\"train\"][\"tagged_set\"] = tagged_train_set\n",
    "DATA[\"dev\"][\"tagged_set\"] = tagged_dev_set\n",
    "DATA[\"test\"][\"tagged_set\"] = tagged_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing tuning functions\n",
    "\n",
    "# Follow lemmatize function from guide notebook: WSTA_N1B_preprocessing.ipynb\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.WordPunctTokenizer() #word_tokenize #tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process_tf_idf(line):\n",
    "    tokenized_sentence = word_tokenizer.tokenize(line.lower())\n",
    "    lemmatized_sentence = [lemmatize(token) for token in tokenized_sentence]\n",
    "    filtered_sentence = [token for token in lemmatized_sentence if token not in filter_tokens]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "def vectorize_documents(text_documents):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', tokenizer=pre_process_tf_idf)\n",
    "    vector_documents = vectorizer.fit_transform(text_documents)\n",
    "    \n",
    "    return [vector_documents, vectorizer]\n",
    "\n",
    "def vectorize_query(vectorizer, text_query):\n",
    "    return vectorizer.transform([text_query])\n",
    "\n",
    "def process_neighbours(vector_documents):\n",
    "    \n",
    "    neighbours = NearestNeighbors(1, algorithm=\"brute\", metric=\"cosine\")\n",
    "    neighbours.fit(vector_documents)\n",
    "    \n",
    "    return neighbours\n",
    "\n",
    "def closest_document(neighbours, vector_query):\n",
    "\n",
    "    result = neighbours.kneighbors(vector_query, 1, return_distance=True)\n",
    "\n",
    "    result_index = result[1][0][0]\n",
    "    result_distance = result[0][0][0]\n",
    "    \n",
    "    return [result_distance, result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_part_a_output(name, data):\n",
    "    \n",
    "    question_set = data[name][\"question_set\"]\n",
    "    document_set = data[name][\"document_set\"]\n",
    "    \n",
    "    part_a_output = []\n",
    "    \n",
    "    for i, questions in enumerate(question_set):\n",
    "        \n",
    "        sentences = document_set[i]\n",
    "\n",
    "        vector_sentences, vectorizer = vectorize_documents(sentences)\n",
    "        neighbours = process_neighbours(vector_sentences)\n",
    "\n",
    "        for j, question in enumerate(questions):\n",
    "            \n",
    "            text_query = question[\"question\"]\n",
    "            vector_query = vectorize_query(vectorizer, text_query)\n",
    "            result_similarity, result_index  = closest_document(neighbours, vector_query)\n",
    "            \n",
    "            result = {\n",
    "                \"set_index\" : i,\n",
    "                \"question_index\" : j,\n",
    "                \"sentence_index\" : result_index\n",
    "            }\n",
    "            \n",
    "            part_a_output.append(result)\n",
    "            \n",
    "    return part_a_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_part_a(name, data):\n",
    "    \n",
    "    data[name][\"a_output_answer_set\"] = generate_part_a_output(name, data)\n",
    "    print\n",
    "    print \"Part A Output: \"\n",
    "    pp.pprint(data[name][\"a_output_answer_set\"][:rapid_size])\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def evaluate_retrieval(name, data):\n",
    "    \n",
    "    question_set = data[name][\"question_set\"]\n",
    "    a_output_answer_set = data[name][\"a_output_answer_set\"]\n",
    "    \n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    for result_a in a_output_answer_set:\n",
    "        \n",
    "        question = question_set[result_a[\"set_index\"]][result_a[\"question_index\"]]\n",
    "        \n",
    "        answer_sentence = question[\"answer_sentence\"]\n",
    "        predicted_answer_sentence = result_a[\"sentence_index\"]\n",
    "        \n",
    "        if answer_sentence == predicted_answer_sentence:\n",
    "            correct.append(result_a)\n",
    "        else :\n",
    "            wrong.append(result_a)\n",
    "            \n",
    "    return (correct, wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_generic(name, data, process_type, process_func):\n",
    "\n",
    "    (correct, wrong) = process_func(name, data)\n",
    "    \n",
    "    data[name][process_type + \"_correct\"] = correct\n",
    "    data[name][process_type + \"_wrong\"] = wrong\n",
    "#     data[name][process_type + \"_full\"] = full\n",
    "    \n",
    "    total = len(correct) + len(wrong)\n",
    "    avg = len(correct) * 1.0 / total\n",
    "    \n",
    "    print process_type.capitalize() + \" Correct: \", len(correct)\n",
    "    print process_type.capitalize() + \" Wrong: \", len(wrong)\n",
    "    print process_type.capitalize() + \" Total: \", total\n",
    "    print process_type.capitalize() + \" Overall Average %: \", avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_retrieval(name, data, stats=False):\n",
    "    print \"Processing retrieval: \", name\n",
    "    process_part_a(name, data)\n",
    "    if stats:\n",
    "        process_generic(name, data, \"retrieval\", evaluate_retrieval)\n",
    "        \n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing retrieval:  rapid\n",
      "\n",
      "Part A Output: \n",
      "[{   'question_index': 0, 'sentence_index': 149, 'set_index': 0}]\n",
      "\n",
      "Retrieval Correct:  156\n",
      "Retrieval Wrong:  248\n",
      "Retrieval Total:  404\n",
      "Retrieval Overall Average %:  0.386138613861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_retrieval(\"rapid\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing retrieval:  train\n",
      "\n",
      "Part A Output: \n",
      "[{   'question_index': 0, 'sentence_index': 149, 'set_index': 0}]\n",
      "\n",
      "Retrieval Correct:  43679\n",
      "Retrieval Wrong:  26480\n",
      "Retrieval Total:  70159\n",
      "Retrieval Overall Average %:  0.622571587394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_retrieval(\"train\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing retrieval:  dev\n",
      "\n",
      "Part A Output: \n",
      "[{   'question_index': 0, 'sentence_index': 71, 'set_index': 0}]\n",
      "\n",
      "Retrieval Correct:  5060\n",
      "Retrieval Wrong:  3403\n",
      "Retrieval Total:  8463\n",
      "Retrieval Overall Average %:  0.597896726929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_retrieval(\"dev\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing retrieval:  test\n",
      "\n",
      "Part A Output: \n",
      "[{   'question_index': 0, 'sentence_index': 283, 'set_index': 0}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_retrieval(\"test\", DATA, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "# Thanks for this list to save me typing it : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "punctuation = ['.',',',';',':']\n",
    "\n",
    "def isPunctuation(word):\n",
    "    return word in punctuation\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Obtained from training data\n",
    "postUnits = [u'%', u'century', u'years', u'percent', u'years ago', u'days', u'months', u'km', u'hours', u'times', u'inches', u'\\xb0C', u'minutes', u'acres', u'\\xb0F', u'weeks', u'people', u'sq mi', u'mi', u'ft', u'feet', u'metres', u'mm', u'square miles', u'miles', u'pm', u'per cent', u'year', u'copies', u'yuan', u'men', u'square feet', u'third', u'kilometres', u'nm', u'tonnes', u'species', u'decades', u'barrels', u'tons', u'largest', u'centuries', u'km2']\n",
    "preUnits = [u'$', u'around', u'late', u'early', u'nearly', u'since', u'approximately', u'number']\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word.lower() in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def refine_word_tags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif isCapitalised(word):\n",
    "                tag = 'OTHERCAP'\n",
    "            elif word in preUnits:\n",
    "                tag = 'PRENUM'\n",
    "            elif word in postUnits:\n",
    "                tag = 'POSTNUM'\n",
    "            elif isStopWord(word):\n",
    "                tag = 'STOPWORD'\n",
    "            elif isPunctuation(word):\n",
    "                tag = 'PUNC'\n",
    "\n",
    "        newWordTags.append((word, tag))\n",
    "    \n",
    "    newWordTags = combineTags (newWordTags)\n",
    "    return newWordTags\n",
    "        \n",
    "def combineTags(wordTags):\n",
    "    \n",
    "    newTags = []\n",
    "    prevWord = wordTags[0][0]\n",
    "    prevTag = wordTags[0][1]\n",
    "    \n",
    "    for (word, tag) in wordTags[1:]:\n",
    "        if tag == 'NUMBER' and prevTag == 'PRENUM':\n",
    "            prevTag = 'NUMBER'\n",
    "        elif prevTag == 'PRENUM':\n",
    "            prevTag = 'O'\n",
    "        if tag == 'POSTNUM' and prevTag == \"NUMBER\":\n",
    "            tag = \"NUMBER\"\n",
    "        elif tag == \"POSTNUM\":\n",
    "            tag = \"O\"\n",
    "        newTags.append((prevWord, prevTag))\n",
    "        prevWord = word\n",
    "        prevTag = tag\n",
    "    newTags.append((prevWord, prevTag))\n",
    "    \n",
    "#     print newTags\n",
    "    \n",
    "    newNewTags = []\n",
    "    prevWord = newTags[0][0]\n",
    "    prevTag = newTags[0][1]\n",
    "    if (prevTag == \"OTHERCAP\"):\n",
    "        prevTag = \"O\"\n",
    "        \n",
    "    for (word, tag) in newTags[1:]:\n",
    "#         print tag, prevTag\n",
    "        if tag == prevTag :\n",
    "            prevWord += ' ' + word\n",
    "        else :\n",
    "            newNewTags.append((prevWord, prevTag))\n",
    "            prevWord = word\n",
    "            prevTag = tag\n",
    "            \n",
    "    newNewTags.append((prevWord, prevTag))\n",
    "    \n",
    "    return newNewTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tags1 = [   (u'Because', 'O'),\n",
    "#     (u'of', 'STOPWORD'),\n",
    "#     (u'financial hardships', u'O'),\n",
    "#     (u'that', 'STOPWORD'),\n",
    "#     (u'plagued', u'O'),\n",
    "#     (u'the', 'STOPWORD'),\n",
    "#     (u'recording industry', u'O'),\n",
    "#     (u'during that', 'STOPWORD'),\n",
    "#     (u'period (', u'O'),\n",
    "#     (u'and', 'STOPWORD'),\n",
    "#     (u'RCA', 'OTHERCAP'),\n",
    "#     (u\"'s\", u'O'),\n",
    "#     (u'own', 'STOPWORD'),\n",
    "#     (u'parched revenues )', u'O'),\n",
    "#     (u',', 'PUNC'),\n",
    "#     (u'Victor', u'PERSON'),\n",
    "#     (u\"'s long-playing records\", u'O'),\n",
    "#     (u'were', 'STOPWORD'),\n",
    "#     (u'discontinued', u'O'),\n",
    "#     (u'by', 'STOPWORD'),\n",
    "#     (u'early', 'PRENUM'),\n",
    "#     (u'1933', 'NUMBER'),\n",
    "#     (u'.', 'PUNC')]\n",
    "\n",
    "# tags2 = [   (u'At', 'O'),\n",
    "#     (u'the', 'STOPWORD'),\n",
    "#     (u'beginning', u'O'),\n",
    "#     (u'of the', 'STOPWORD'),\n",
    "#     (u'20th', 'NUMBER'),\n",
    "#     (u'century', 'POSTNUM'),\n",
    "#     (u',', 'PUNC'),\n",
    "#     (u'the', 'STOPWORD'),\n",
    "#     (u'early', 'PRENUM'),\n",
    "#     (u'discs played', u'O'),\n",
    "#     (u'for', 'STOPWORD'),\n",
    "#     (u'two', 'NUMBER'),\n",
    "#     (u'minutes', 'POSTNUM'),\n",
    "#     (u',', 'PUNC'),\n",
    "#     (u'the same as', 'STOPWORD'),\n",
    "#     (u'early', 'PRENUM'),\n",
    "#     (u'cylinder records', u'O'),\n",
    "#     (u'.', 'PUNC')]\n",
    "\n",
    "# tags3 = [       (u'early', 'PRENUM'),\n",
    "#     (u'1933', 'NUMBER'), (u'for', 'STOPWORD'),\n",
    "#     (u'two', 'NUMBER'),\n",
    "#     (u'minutes', 'POSTNUM'), ]\n",
    "\n",
    "# pp.pprint(combineTags(tags3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_part_b_output(name, data):\n",
    "    \n",
    "    question_set = data[name][\"question_set\"]\n",
    "    a_output_answer_set = data[name][\"a_output_answer_set\"]\n",
    "    tagged_set = data[name][\"tagged_set\"]\n",
    "    \n",
    "    part_b_output = []\n",
    "    \n",
    "    for result_a in a_output_answer_set:\n",
    "        \n",
    "        stanford_tags = tagged_set[result_a[\"set_index\"]][result_a[\"sentence_index\"]]\n",
    "        \n",
    "        filtered_tags = refine_word_tags(stanford_tags)\n",
    "        \n",
    "        question = question_set[result_a[\"set_index\"]][result_a[\"question_index\"]][\"question\"]\n",
    "        \n",
    "        result_b = {\n",
    "            \"set_index\"  : result_a[\"set_index\"],\n",
    "            \"question_index\" : result_a[\"question_index\"],\n",
    "            \"sentence_index\" : result_a[\"sentence_index\"],\n",
    "            \"candidates\" : filtered_tags\n",
    "        }\n",
    "        \n",
    "        part_b_output.append(result_b)\n",
    "        \n",
    "    return part_b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_part_b(name, data):\n",
    "    \n",
    "    data[name][\"b_output_answer_set\"] = generate_part_b_output(name, data)\n",
    "    \n",
    "    print\n",
    "    print \"Part B Output: \"\n",
    "    pp.pprint(data[name][\"b_output_answer_set\"][:rapid_size])\n",
    "    print    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval setup stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def getAnswerDict(qss):\n",
    "    sentDicts = defaultdict(list)\n",
    "    for docID in range(0, len(qss)):\n",
    "        qs = qss[docID]\n",
    "        for q in qs:\n",
    "            answer = q[\"answer\"]\n",
    "            answerSent = (docID, q[\"answer_sentence\"])\n",
    "            sentDicts[answerSent].append(answer)\n",
    "    return sentDicts\n",
    "trainSentDicts = getAnswerDict(train)\n",
    "devSentDicts = getAnswerDict(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 28)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function returns a list of the indexes of words that are an answer to a question in the train set\n",
    "def getAnswerPos(docID, sentID, sents, answerDict):\n",
    "    sententence = sents[docID][sentID]\n",
    "    listAnswers = answerDict[(docID, sentID)]\n",
    "    tokenisedSent =  nltk.word_tokenize(sententence)\n",
    "    answerPosList = []\n",
    "    for ans in listAnswers:\n",
    "        tokenisedAns = nltk.word_tokenize(ans)\n",
    "        # Inefficient\n",
    "        \n",
    "        for i in range (0, len(tokenisedSent) - len(tokenisedAns)):\n",
    "            answerFragment = tokenisedSent[i:i+len(tokenisedAns)]\n",
    "            if (answerFragment == tokenisedAns):\n",
    "                answerPosList.append((i,i+len(tokenisedAns)))\n",
    "\n",
    "                break\n",
    "    return answerPosList\n",
    "\n",
    "getAnswerPos(0,0,trainSents, trainSentDicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 31], [5, 6], [8]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getAnswerPos(0,9,trainSents, trainSentDicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partBoutput = generate_part_b_output(\"train\", DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidates': [(u'They', 'O'),\n",
       "  (u'had a', 'STOPWORD'),\n",
       "  (u'playing time', u'O'),\n",
       "  (u'of', 'STOPWORD'),\n",
       "  (u'eight minutes', 'NUMBER'),\n",
       "  (u'.', 'PUNC')],\n",
       " 'question_index': 0,\n",
       " 'sentence_index': 149,\n",
       " 'set_index': 0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partBoutput[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'answer': u'long playing', u'question': u'What does LP stand for when it comes to time capacity?', u'answer_sentence': 2}\n"
     ]
    }
   ],
   "source": [
    "print train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def convertCandidatesToRange(candidates):\n",
    "    candidatesIndexRange = []\n",
    "    index = 0\n",
    "    for (words, tag) in candidates:\n",
    "        tokenisedWords = nltk.word_tokenize(words)\n",
    "        nextIndex = index + len(tokenisedWords)\n",
    "        candidatesIndexRange.append((index,nextIndex ))\n",
    "        index = nextIndex\n",
    "    return candidatesIndexRange\n",
    "\n",
    "def nerAdvancedEvaluation(partBoutput,train,trainSents,trainSentDicts):\n",
    "    correctAnswers = []\n",
    "    incorrectAnswers = []\n",
    "    for partBOut in partBoutput[7:8]:\n",
    "        candidates = partBOut[\"candidates\"]\n",
    "        set_index = partBOut['set_index']\n",
    "        sentence_index = partBOut['sentence_index']\n",
    "        question_index = partBOut['question_index']\n",
    "        answer_positions = getAnswerPos(set_index,sentence_index,trainSents, trainSentDicts)\n",
    "        question= train[set_index][question_index][\"question\"]\n",
    "        correct_answer = train[set_index][question_index][\"answer\"]\n",
    "\n",
    "        if train[set_index][question_index][\"answer_sentence\"] != sentence_index:\n",
    "            continue\n",
    "\n",
    "\n",
    "        candidatesIndexRange = convertCandidatesToRange(candidates)\n",
    "        print answer_positions\n",
    "        print candidatesIndexRange\n",
    "        print\n",
    "\n",
    "        for (ans_start, ans_end) in answer_positions:\n",
    "            incorrectCandidates = []\n",
    "            candidate_ID = -1\n",
    "            for (cand_start, cand_end) in candidatesIndexRange:\n",
    "                candidate_ID += 1\n",
    "                if cand_start == ans_start and cand_end ==ans_end : #If the candidate is the answer\n",
    "                    correctAnswers.append([candidates[candidate_ID]])\n",
    "                    break\n",
    "                if cand_start >= ans_start and cand_end >= ans_end :\n",
    "                    incorrectAnswers.append([candidates[candidate_ID], correct_answer])\n",
    "                    break\n",
    "                if cand_start >= ans_start:\n",
    "                    if (cand_end <= ans_end):\n",
    "                        incorrectCandidates.append(candidates[candidate_ID])\n",
    "                    else :\n",
    "                        incorrectAnswers.append((incorrectCandidates, correct_answer))\n",
    "                        break\n",
    "        \n",
    "    return (correctAnswers,incorrectAnswers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25, 31), (47, 48)]\n",
      "[(0, 1), (1, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 9), (9, 11), (11, 12), (12, 13), (13, 14), (14, 16), (16, 19), (19, 20), (20, 22), (22, 24), (24, 25), (25, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35), (35, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 44), (44, 46), (46, 47), (47, 48), (48, 49)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(correctAnswers,incorrectAnswers) = nerAdvancedEvaluation(partBoutput,train,trainSents,trainSentDicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u's', 'STOPWORD'), u'disc jockeys (DJ)s']\n"
     ]
    }
   ],
   "source": [
    "print incorrectAnswers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def evaluate_ner(name, data):\n",
    "    \n",
    "    question_set = data[name][\"question_set\"]\n",
    "    b_output_answer_set = data[name][\"b_output_answer_set\"]\n",
    "    \n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    for result_b in b_output_answer_set:\n",
    "        \n",
    "        answer = question_set[result_b[\"set_index\"]][result_b[\"question_index\"]][\"answer\"]\n",
    "        \n",
    "        possible_candidates = result_b[\"candidates\"]\n",
    "        \n",
    "        answer_exists_in_candidates = False\n",
    "        \n",
    "        for candidate in possible_candidates:\n",
    "            \n",
    "            candidate_string = candidate[0]\n",
    "            \n",
    "            if candidate_string == answer:\n",
    "                \n",
    "                answer_exists_in_candidates = True\n",
    "                \n",
    "                break\n",
    "        \n",
    "        if answer_exists_in_candidates:\n",
    "            correct.append(result_b)\n",
    "        else :\n",
    "            wrong.append(result_b)\n",
    "            \n",
    "    return (correct, wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_ner(name, data, stats=False):\n",
    "    print \"Processing ner: \", name\n",
    "    process_part_b(name, data)\n",
    "    if stats:\n",
    "        process_generic(name, data, \"ner\", evaluate_ner)\n",
    "        \n",
    "        correct_ner = len(data[name][\"ner_correct\"])\n",
    "        correct_ret = len(data[name][\"retrieval_correct\"])\n",
    "        \n",
    "        avg = correct_ner * 1.0 / correct_ret\n",
    "        \n",
    "        print \"ner\".capitalize() + \" Correct Average of Previous %: \", avg\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ner:  rapid\n",
      "\n",
      "Part B Output: \n",
      "[   {   'candidates': [   (u'They', 'O'),\n",
      "                          (u'had a', 'STOPWORD'),\n",
      "                          (u'playing time', u'O'),\n",
      "                          (u'of', 'STOPWORD'),\n",
      "                          (u'eight minutes', 'NUMBER'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'question_index': 0,\n",
      "        'sentence_index': 149,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Ner Correct:  67\n",
      "Ner Wrong:  337\n",
      "Ner Total:  404\n",
      "Ner Overall Average %:  0.165841584158\n",
      "Ner Correct Average of Previous %:  0.429487179487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_ner(\"rapid\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ner:  train\n",
      "\n",
      "Part B Output: \n",
      "[   {   'candidates': [   (u'They', 'O'),\n",
      "                          (u'had a', 'STOPWORD'),\n",
      "                          (u'playing time', u'O'),\n",
      "                          (u'of', 'STOPWORD'),\n",
      "                          (u'eight minutes', 'NUMBER'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'question_index': 0,\n",
      "        'sentence_index': 149,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Ner Correct:  20067\n",
      "Ner Wrong:  50092\n",
      "Ner Total:  70159\n",
      "Ner Overall Average %:  0.286021750595\n",
      "Ner Correct Average of Previous %:  0.459419858513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_ner(\"train\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ner:  dev\n",
      "\n",
      "Part B Output: \n",
      "[   {   'candidates': [   (u'Infrared', 'O'),\n",
      "                          (u'is', 'STOPWORD'),\n",
      "                          (u'used', u'O'),\n",
      "                          (u'in', 'STOPWORD'),\n",
      "                          (u'night vision equipment', u'O'),\n",
      "                          (u'when there is', 'STOPWORD'),\n",
      "                          (u'insufficient visible light', u'O'),\n",
      "                          (u'to', 'STOPWORD'),\n",
      "                          (u'see', u'O'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'question_index': 0,\n",
      "        'sentence_index': 71,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Ner Correct:  2415\n",
      "Ner Wrong:  6048\n",
      "Ner Total:  8463\n",
      "Ner Overall Average %:  0.285359801489\n",
      "Ner Correct Average of Previous %:  0.477272727273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_ner(\"dev\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ner:  test\n",
      "\n",
      "Part B Output: \n",
      "[   {   'candidates': [   (u'a', 'STOPWORD'),\n",
      "                          (u'forgotten theatre', u'O'),\n",
      "                          (u'of the', 'STOPWORD'),\n",
      "                          (u'Crimean War', 'OTHERCAP'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'question_index': 0,\n",
      "        'sentence_index': 283,\n",
      "        'set_index': 0}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_ner(\"test\", DATA, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def getQuestionType(question):\n",
    "    if 'Who' in question:\n",
    "        return \"PERSON\"\n",
    "    if 'where' in question:\n",
    "        return \"LOCATION\"\n",
    "    if 'How many' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'How much' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'When' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'what year' in question:\n",
    "        return \"NUMBER\"\n",
    "    else:\n",
    "        return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, answers whose content words all appear in the question should be ranked lowest.\n",
    "\n",
    "def first_filter(question, answer_entities):\n",
    "   \n",
    "    ranked_list = []\n",
    "    \n",
    "    question = set(pre_process_tf_idf(question))\n",
    "    \n",
    "#     print question\n",
    "#     print\n",
    "    \n",
    "    for entity in answer_entities:\n",
    "\n",
    "        raw_span = entity[0]\n",
    "        span_tag = entity[1]\n",
    "        \n",
    "        set_span = set(pre_process_tf_idf(raw_span))\n",
    "        \n",
    "        if span_tag != \"O\" and span_tag != \"STOPWORD\" and span_tag !=\"PUNC\":\n",
    "            \n",
    "            if set_span.issubset(question):\n",
    "                \n",
    "                ranked_list.append([entity, 1])\n",
    "#                 print \"IN\", raw_span, span_tag, set_span, question\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                ranked_list.append([entity, 2])\n",
    "#                 print \"OUT\", raw_span, span_tag, set_span, question\n",
    "    \n",
    "    return sorted(ranked_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, answers whose content words all appear in the question should be ranked lowest.\n",
    "\n",
    "def first_filter_object(question, answer_entities):\n",
    "   \n",
    "    ranked_list = []\n",
    "    \n",
    "    question = set(pre_process_tf_idf(question))\n",
    "    \n",
    "#     print question\n",
    "#     print\n",
    "    \n",
    "    for entity in answer_entities:\n",
    "\n",
    "        raw_span = entity[0]\n",
    "        span_tag = entity[1]\n",
    "        \n",
    "        set_span = set(pre_process_tf_idf(raw_span))\n",
    "        \n",
    "        if span_tag != \"STOPWORD\" and span_tag !=\"PUNC\": #span_tag != \"O\" and\n",
    "            \n",
    "            if span_tag == \"O\":\n",
    "                \n",
    "                if len(set_span) > 2:\n",
    "                    ranked_list.append([entity, 0])\n",
    "            \n",
    "            elif set_span.issubset(question):\n",
    "                \n",
    "                ranked_list.append([entity, 1])\n",
    "#                 print \"IN\", raw_span, span_tag, set_span, question\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                ranked_list.append([entity, 2])\n",
    "#                 print \"OUT\", raw_span, span_tag, set_span, question\n",
    "    \n",
    "    return sorted(ranked_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, answers whose content words all appear in the question should be ranked lowest.\n",
    "\n",
    "def first_filter_object_stop(question, answer_entities):\n",
    "   \n",
    "    ranked_list = []\n",
    "    \n",
    "    question = set(pre_process_tf_idf(question))\n",
    "    \n",
    "#     print question\n",
    "#     print\n",
    "    \n",
    "    for entity in answer_entities:\n",
    "\n",
    "        raw_span = entity[0]\n",
    "        span_tag = entity[1]\n",
    "        \n",
    "        set_span = set(pre_process_tf_idf(raw_span))\n",
    "        \n",
    "        if span_tag !=\"PUNC\": #span_tag != \"O\" and\n",
    "            \n",
    "            if span_tag == \"O\" or span_tag == \"STOPWORD\":\n",
    "                \n",
    "                if len(set_span) > 2:\n",
    "                    \n",
    "                    ranked_list.append([entity, 0])\n",
    "            \n",
    "            elif set_span.issubset(question):\n",
    "                \n",
    "                ranked_list.append([entity, 1])\n",
    "#                 print \"IN\", raw_span, span_tag, set_span, question\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                ranked_list.append([entity, 2])\n",
    "#                 print \"OUT\", raw_span, span_tag, set_span, question\n",
    "    \n",
    "    return sorted(ranked_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second, answers which match the question type should be ranked higher than those that don't; for this, you\n",
    "# should build a simple rule-based question type classifier based on key words (e.g. questions which contain \"who\" are\n",
    "# people).\n",
    "\n",
    "# First, answers whose content words all appear in the question should be ranked lowest.\n",
    "\n",
    "def second_filter(question, ranked_list):\n",
    "   \n",
    "    question_type = getQuestionType(question)\n",
    "#     print question_type\n",
    "    \n",
    "    for index, answer in enumerate(ranked_list):\n",
    "        \n",
    "        entity_tag = answer[0][1]\n",
    "        \n",
    "        if entity_tag == question_type:\n",
    "#             print \"MATCH\", answer[0], question_type, question\n",
    "            ranked_list[index].append(2)\n",
    "#             ranked_list[index][1] += 1\n",
    "        else:\n",
    "            ranked_list[index].append(1)\n",
    "#             ranked_list[index][1] -= 1\n",
    "            \n",
    "    return ranked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_process_open_class(line):\n",
    "    tokenized_sentence = word_tokenizer.tokenize(line.lower())\n",
    "    lemmatized_sentence = [lemmatize(token) for token in tokenized_sentence]\n",
    "    filtered_sentence = [token for token in lemmatized_sentence if token not in filter_tokens]\n",
    "    tagged_sent = nltk.pos_tag(lemmatized_sentence)\n",
    "    final = []\n",
    "    for word, tag in tagged_sent:\n",
    "        if \"V\" in tag or \"NN\" in tag:\n",
    "#             final.append((word,tag))\n",
    "            final.append(word)\n",
    "            \n",
    "#     print \"RESULT: \", final\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Third, among entities of the same type, the prefered entity should be the one which is closer in the sentence to a\n",
    "# closed-class word from the question.\n",
    "\n",
    "def third_filter(question, possAnswers, ranked_list):\n",
    "    \n",
    "    question = pre_process_open_class(question)\n",
    "\n",
    "    answer_sent = \" \".join([x[0] for x in possAnswers])\n",
    "    answer_sent = pre_process_tf_idf(answer_sent)\n",
    "    raw_answer_sent = \" \".join(answer_sent)\n",
    "    \n",
    "#     print \"QUESTION: \"\n",
    "#     pp.pprint(question)\n",
    "#     print \"ANSWER: \"\n",
    "#     pp.pprint(answer_sent)\n",
    "#     pp.pprint(raw_answer_sent)\n",
    "    \n",
    "    for index, answer in enumerate(ranked_list):\n",
    "\n",
    "        span_tag = answer[0][1]\n",
    "        raw_span = answer[0][0]\n",
    "\n",
    "        proc_span = pre_process_tf_idf(raw_span)\n",
    "\n",
    "        raw_proc_span = \" \".join(proc_span)\n",
    "        new_raw_proc_span = \"-\".join(proc_span)\n",
    "\n",
    "        raw_answer_sent = raw_answer_sent.replace(raw_proc_span, new_raw_proc_span)\n",
    "    \n",
    "    answer_sent = raw_answer_sent.split(\" \")\n",
    "    \n",
    "    avg_dict = defaultdict(float)\n",
    "    \n",
    "    for open_class in question:\n",
    "        \n",
    "        if open_class in answer_sent:\n",
    "            \n",
    "            open_class_locations = [i for i, x in enumerate(answer_sent) if x == open_class]\n",
    "            \n",
    "#             print \"OPEN CLASS: \", repr(open_class)\n",
    "\n",
    "            for index, answer in enumerate(ranked_list):\n",
    "\n",
    "                span_tag = answer[0][1]\n",
    "                raw_span = answer[0][0]\n",
    "\n",
    "                proc_span = pre_process_tf_idf(raw_span)\n",
    "                \n",
    "                raw_proc_span = \" \".join(proc_span)\n",
    "                new_raw_proc_span = \"-\".join(proc_span)\n",
    "                \n",
    "                proc_span_locations = [i for i, x in enumerate(answer_sent) if x == new_raw_proc_span]\n",
    "                \n",
    "                min_dist = len(answer_sent)\n",
    "                min_dist_ind = (None, None)\n",
    "                \n",
    "                for loc1 in proc_span_locations:\n",
    "                    \n",
    "                    for loc2 in open_class_locations:\n",
    "                        \n",
    "                        dist = abs(loc1 - loc2)\n",
    "                        \n",
    "                        if dist < min_dist:\n",
    "                            \n",
    "                            min_dist = dist\n",
    "                            min_dist_ind = (loc1, loc2)\n",
    "                \n",
    "#                 print \"PROC: \", proc_span_locations\n",
    "#                 print \"OPEN CLASS: \", open_class_locations                \n",
    "                scale = (len(answer_sent) - min_dist) * 1.0 / len(answer_sent)\n",
    "#                 print \"JOINT: \", min_dist_ind, scale\n",
    "                avg_dict[index] += scale\n",
    "#                 ranked_list[index][1] *= scale\n",
    "    \n",
    "    for key, value in avg_dict.iteritems():\n",
    "        ranked_list[key].append(value / len(question))\n",
    "\n",
    "    return ranked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_rank(ranking_list):\n",
    "    \n",
    "    new_ranking = []\n",
    "    \n",
    "    for rank in ranking_list:\n",
    "        \n",
    "        new_rank = ( rank[1] + rank[2] )\n",
    "        \n",
    "        if len(rank) == 4:\n",
    "             new_rank *= rank[3]\n",
    "        \n",
    "        new_ranking.append([rank[0], new_rank])\n",
    "        \n",
    "    return sorted(new_ranking, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_part_c_output(name, data):\n",
    "    \n",
    "    part_c_output = []\n",
    "\n",
    "    question_set = data[name][\"question_set\"]\n",
    "    document_set = data[name][\"document_set\"]\n",
    "        \n",
    "    b_output_answer_set = data[name][\"b_output_answer_set\"]\n",
    "    \n",
    "    for result_b in b_output_answer_set:\n",
    "        \n",
    "        question = question_set[result_b[\"set_index\"]][result_b[\"question_index\"]][\"question\"]\n",
    "        \n",
    "        first_pass = first_filter(question, result_b[\"candidates\"])\n",
    "        \n",
    "        second_pass = second_filter(question, first_pass)\n",
    "        \n",
    "        third_pass = third_filter(question, result_b[\"candidates\"], second_pass)\n",
    "        \n",
    "        fourth_pass = reduce_rank(third_pass)\n",
    "                \n",
    "#         pp.pprint(third_pass)\n",
    "\n",
    "        predicted_answer = None\n",
    "\n",
    "        if len(fourth_pass) > 0:\n",
    "\n",
    "            top_answer = fourth_pass[0]\n",
    "    #         pp.pprint(top_answer)        \n",
    "            predicted_answer = top_answer[0][0]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            first_pass = first_filter_object(question, result_b[\"candidates\"])\n",
    "\n",
    "            second_pass = second_filter(question, first_pass)\n",
    "\n",
    "            third_pass = third_filter(question, result_b[\"candidates\"], second_pass)\n",
    "\n",
    "            fourth_pass = reduce_rank(third_pass)   \n",
    "            \n",
    "            if len(fourth_pass) > 0:\n",
    "\n",
    "                top_answer = fourth_pass[0]\n",
    "        #         pp.pprint(top_answer)        \n",
    "                predicted_answer = top_answer[0][0]\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                first_pass = first_filter_object_stop(question, result_b[\"candidates\"])\n",
    "\n",
    "                second_pass = second_filter(question, first_pass)\n",
    "\n",
    "                third_pass = third_filter(question, result_b[\"candidates\"], second_pass)\n",
    "\n",
    "                fourth_pass = reduce_rank(third_pass)   \n",
    "\n",
    "                if len(fourth_pass) > 0:\n",
    "\n",
    "                    top_answer = fourth_pass[0]\n",
    "            #         pp.pprint(top_answer)        \n",
    "                    predicted_answer = top_answer[0][0]                \n",
    "                \n",
    "                else:\n",
    "\n",
    "        #         pp.pprint(top_answer)        \n",
    "                    predicted_answer = random.choice(result_b[\"candidates\"])[0]\n",
    "            \n",
    "        \n",
    "        result_c = {\n",
    "            \"set_index\"  : result_b[\"set_index\"],\n",
    "            \"question_index\" : result_b[\"question_index\"],\n",
    "            \"sentence_index\" : result_b[\"sentence_index\"],\n",
    "            \"candidates\": result_b[\"candidates\"],\n",
    "            \"ranked_answers\": fourth_pass,\n",
    "            \"predicted_answer\" : predicted_answer\n",
    "        }\n",
    "        \n",
    "        part_c_output.append(result_c)        \n",
    "\n",
    "    return part_c_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_part_c(name, data):\n",
    "    \n",
    "    data[name][\"c_output_answer_set\"] = generate_part_c_output(name, data)\n",
    "    \n",
    "    print\n",
    "    print \"Part C Output: \"\n",
    "    pp.pprint(data[name][\"c_output_answer_set\"][:rapid_size])\n",
    "    print    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluate_rank(name, data):\n",
    "    \n",
    "    question_set = data[name][\"question_set\"]\n",
    "    document_set = data[name][\"document_set\"]\n",
    "    \n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    c_output_answer_set = data[name][\"c_output_answer_set\"]\n",
    "    \n",
    "    for result_c in c_output_answer_set:\n",
    "        \n",
    "        question = question_set[result_c[\"set_index\"]][result_c[\"question_index\"]][\"question\"]\n",
    "        answer =  question_set[result_c[\"set_index\"]][result_c[\"question_index\"]][\"answer\"]\n",
    "        \n",
    "        predicted_answer = result_c[\"predicted_answer\"]\n",
    "\n",
    "        if (predicted_answer == answer):\n",
    "            correct.append(result_c)\n",
    "        else :\n",
    "            wrong.append(result_c)\n",
    "#         break\n",
    "        #print correct\n",
    "    return (correct, wrong)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_rank(name, data, stats=False):\n",
    "    print \"Processing rank: \", name\n",
    "    process_part_c(name, data)\n",
    "    if stats:\n",
    "        process_generic(name, data, \"rank\", evaluate_rank)\n",
    "        \n",
    "        \n",
    "        correct_rank = len(data[name][\"rank_correct\"])\n",
    "        correct_ner = len(data[name][\"ner_correct\"])\n",
    "        \n",
    "        avg = correct_rank * 1.0 / correct_ner\n",
    "        \n",
    "        print \"rank\".capitalize() + \" Correct Average of Previous %: \", avg        \n",
    "        \n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rank:  rapid\n",
      "\n",
      "Part C Output: \n",
      "[   {   'candidates': [   (u'They', 'O'),\n",
      "                          (u'had a', 'STOPWORD'),\n",
      "                          (u'playing time', u'O'),\n",
      "                          (u'of', 'STOPWORD'),\n",
      "                          (u'eight minutes', 'NUMBER'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'predicted_answer': u'eight minutes',\n",
      "        'question_index': 0,\n",
      "        'ranked_answers': [[(u'eight minutes', 'NUMBER'), 0.4]],\n",
      "        'sentence_index': 149,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Rank Correct:  36\n",
      "Rank Wrong:  368\n",
      "Rank Total:  404\n",
      "Rank Overall Average %:  0.0891089108911\n",
      "Rank Correct Average of Previous %:  0.537313432836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_rank(\"rapid\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rank:  train\n",
      "\n",
      "Part C Output: \n",
      "[   {   'candidates': [   (u'They', 'O'),\n",
      "                          (u'had a', 'STOPWORD'),\n",
      "                          (u'playing time', u'O'),\n",
      "                          (u'of', 'STOPWORD'),\n",
      "                          (u'eight minutes', 'NUMBER'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'predicted_answer': u'eight minutes',\n",
      "        'question_index': 0,\n",
      "        'ranked_answers': [[(u'eight minutes', 'NUMBER'), 0.4]],\n",
      "        'sentence_index': 149,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Rank Correct:  9895\n",
      "Rank Wrong:  60264\n",
      "Rank Total:  70159\n",
      "Rank Overall Average %:  0.141036787868\n",
      "Rank Correct Average of Previous %:  0.493098121294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_rank(\"train\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rank:  dev\n",
      "\n",
      "Part C Output: \n",
      "[   {   'candidates': [   (u'Infrared', 'O'),\n",
      "                          (u'is', 'STOPWORD'),\n",
      "                          (u'used', u'O'),\n",
      "                          (u'in', 'STOPWORD'),\n",
      "                          (u'night vision equipment', u'O'),\n",
      "                          (u'when there is', 'STOPWORD'),\n",
      "                          (u'insufficient visible light', u'O'),\n",
      "                          (u'to', 'STOPWORD'),\n",
      "                          (u'see', u'O'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'predicted_answer': u'night vision equipment',\n",
      "        'question_index': 0,\n",
      "        'ranked_answers': [   [   (u'night vision equipment', u'O'),\n",
      "                                  0.26666666666666666],\n",
      "                              [   (u'insufficient visible light', u'O'),\n",
      "                                  0.19999999999999998]],\n",
      "        'sentence_index': 71,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Rank Correct:  1135\n",
      "Rank Wrong:  7328\n",
      "Rank Total:  8463\n",
      "Rank Overall Average %:  0.134113198629\n",
      "Rank Correct Average of Previous %:  0.469979296066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_rank(\"dev\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_wrong_debug(name, data):\n",
    "    \n",
    "    question_set = data[name][\"question_set\"]\n",
    "    document_set = data[name][\"document_set\"]\n",
    "    rank_wrong = data[name][\"rank_wrong\"]\n",
    "    \n",
    "    for result_wrong in rank_wrong:\n",
    "        \n",
    "        question = question_set[result_wrong[\"set_index\"]][result_wrong[\"question_index\"]]\n",
    "        candidate_sentence = document_set[result_wrong[\"set_index\"]][result_wrong[\"sentence_index\"]]\n",
    "        correct_sentence = document_set[result_wrong[\"set_index\"]][question[\"answer_sentence\"]]\n",
    "        \n",
    "        candidates = result_wrong[\"candidates\"]\n",
    "        ranked_answers = result_wrong[\"ranked_answers\"]\n",
    "        predicted_answer = result_wrong[\"predicted_answer\"]\n",
    "        \n",
    "        if question[\"answer_sentence\"] == result_wrong[\"sentence_index\"]:\n",
    "            \n",
    "            print \"=\" * 20\n",
    "            print \"=\" * 20\n",
    "        \n",
    "            print \"Question: \"\n",
    "            print\n",
    "            pp.pprint(question[\"question\"])\n",
    "\n",
    "            print\n",
    "            print \"Correct Sentence: (Part A)\"\n",
    "            print\n",
    "            pp.pprint(correct_sentence)\n",
    "            print\n",
    "            print \"Chosen Sentence: (Part A)\"\n",
    "            print\n",
    "            pp.pprint(candidate_sentence)\n",
    "            print\n",
    "\n",
    "            print \"Candidate Answers: (Part B)\"\n",
    "            print\n",
    "            pp.pprint(candidates)\n",
    "            print\n",
    "            print \"Ranked Answers: (Part C)\"\n",
    "            print\n",
    "            pp.pprint(ranked_answers)\n",
    "            print\n",
    "            print \"Predicted Answer: (Part C)\"\n",
    "            print\n",
    "            pp.pprint(predicted_answer)\n",
    "            print\n",
    "            print \"Correct Answer: (Part C)\"\n",
    "            print\n",
    "            pp.pprint(question[\"answer\"])     \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "Question: \n",
      "\n",
      "u'What was the primary use of a phonographic disc record?'\n",
      "\n",
      "Correct Sentence: (Part A)\n",
      "\n",
      "u'The phonograph disc record was the primary medium used for music reproduction until late in the 20th century, replacing the phonograph cylinder record\\u2013with which it had co-existed from the late 1880s through to the 1920s\\u2013by the late 1920s.'\n",
      "\n",
      "Chosen Sentence: (Part A)\n",
      "\n",
      "u'The phonograph disc record was the primary medium used for music reproduction until late in the 20th century, replacing the phonograph cylinder record\\u2013with which it had co-existed from the late 1880s through to the 1920s\\u2013by the late 1920s.'\n",
      "\n",
      "Candidate Answers: (Part B)\n",
      "\n",
      "[   (u'The phonograph disc record', 'O'),\n",
      "    (u'was the', 'STOPWORD'),\n",
      "    (u'primary medium used', u'O'),\n",
      "    (u'for', 'STOPWORD'),\n",
      "    (u'music reproduction', u'O'),\n",
      "    (u'until', 'STOPWORD'),\n",
      "    (u'late', 'O'),\n",
      "    (u'in the', 'STOPWORD'),\n",
      "    (u'20th century', 'NUMBER'),\n",
      "    (u',', 'PUNC'),\n",
      "    (u'replacing', u'O'),\n",
      "    (u'the', 'STOPWORD'),\n",
      "    (u'phonograph cylinder record\\u2013with', u'O'),\n",
      "    (u'which it had', 'STOPWORD'),\n",
      "    (u'co-existed', u'O'),\n",
      "    (u'from the', 'STOPWORD'),\n",
      "    (u'late 1880s', 'NUMBER'),\n",
      "    (u'through to the', 'STOPWORD'),\n",
      "    (u'1920s\\u2013by', 'NUMBER'),\n",
      "    (u'the', 'STOPWORD'),\n",
      "    (u'late 1920s', 'NUMBER'),\n",
      "    (u'.', 'PUNC')]\n",
      "\n",
      "Ranked Answers: (Part C)\n",
      "\n",
      "[   [(u'20th century', 'NUMBER'), 1.6500000000000001],\n",
      "    [(u'late 1880s', 'NUMBER'), 1.05],\n",
      "    [(u'1920s\\u2013by', 'NUMBER'), 0.9375],\n",
      "    [(u'late 1920s', 'NUMBER'), 0.8250000000000001]]\n",
      "\n",
      "Predicted Answer: (Part C)\n",
      "\n",
      "u'20th century'\n",
      "\n",
      "Correct Answer: (Part C)\n",
      "\n",
      "u'music reproduction'\n"
     ]
    }
   ],
   "source": [
    "log_wrong_debug(\"rapid\", DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rank:  test\n",
      "\n",
      "Part C Output: \n",
      "[   {   'candidates': [   (u'a', 'STOPWORD'),\n",
      "                          (u'forgotten theatre', u'O'),\n",
      "                          (u'of the', 'STOPWORD'),\n",
      "                          (u'Crimean War', 'OTHERCAP'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'predicted_answer': u'Crimean War',\n",
      "        'question_index': 0,\n",
      "        'ranked_answers': [[(u'Crimean War', 'OTHERCAP'), 2]],\n",
      "        'sentence_index': 283,\n",
      "        'set_index': 0}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_rank(\"test\", DATA, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_submit(name, data):\n",
    "    \n",
    "    headers = ['id', 'answer']\n",
    "    \n",
    "    c_output_answer_set = data[name][\"c_output_answer_set\"]       \n",
    "\n",
    "    with open(name + '.submit.csv', 'w') as f:\n",
    "\n",
    "        f_csv = csv.DictWriter(f, headers)\n",
    "        f_csv.writeheader()\n",
    "\n",
    "        for index, result_c in enumerate(c_output_answer_set):\n",
    "            \n",
    "            predicted_answer = result_c[\"predicted_answer\"]\n",
    "            \n",
    "            if predicted_answer is not None:\n",
    "                f_csv.writerows([{'id':index+1,'answer':predicted_answer.encode(\"utf-8\")}])\n",
    "            else:\n",
    "                f_csv.writerows([{'id':index+1,'answer':\"NONE\"}])\n",
    "            \n",
    "#             if isinstance( answer_list[index]['answer'], int):\n",
    "                \n",
    "#                 f_csv.writerows([{'id':index+1,'answer':answer_list[index]['answer'][0][0]}])\n",
    "                \n",
    "#             else:\n",
    "                \n",
    "#                 f_csv.writerows([{'id':index+1,'answer':answer_list[index]['answer'][0][0].encode(\"utf-8\")}])        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_submit(\"rapid\", DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_submit(\"test\", DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
