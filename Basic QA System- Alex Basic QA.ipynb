{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the python script containing the same code as the load the data notebook\n",
    "%run loadData.py\n",
    "# now we can access train, dev, and test\n",
    "# along with trainSents, devSents testSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning functions\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Follow lemmatize function from guide notebook: WSTA_N1B_preprocessing.ipynb\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "def vectorize_documents(text_documents):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vector_documents = vectorizer.fit_transform(text_documents)\n",
    "    \n",
    "    return [vector_documents, vectorizer]\n",
    "\n",
    "def vectorize_query(vectorizer, text_query):\n",
    "    return vectorizer.transform([text_query])\n",
    "\n",
    "def process_neighbours(vector_documents):\n",
    "    \n",
    "    neighbours = NearestNeighbors(1, algorithm=\"brute\", metric=\"cosine\")\n",
    "    neighbours.fit(vector_documents)\n",
    "    \n",
    "    return neighbours\n",
    "\n",
    "def closest_document(neighbours, vector_query):\n",
    "\n",
    "    result = neighbours.kneighbors(vector_query, 1, return_distance=True)\n",
    "\n",
    "    result_index = result[1][0][0]\n",
    "    result_distance = result[0][0][0]\n",
    "    \n",
    "    return [result_distance, result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets make a list of the questions part A gets right\n",
    "correct = []\n",
    "for i in range (0, len(dev)):\n",
    "    documents = devSents[i]\n",
    "    questions = dev[i]\n",
    "\n",
    "    vector_documents, vectorizer = vectorize_documents(documents)\n",
    "    analyze = vectorizer.build_analyzer()\n",
    "    neighbours = process_neighbours(vector_documents)\n",
    "\n",
    "    for j in range (0, len(questions)):\n",
    "        text_query = questions[j][\"question\"]\n",
    "        vector_query = vectorize_query(vectorizer, text_query)\n",
    "        result_similarity, result_index  = closest_document(neighbours, vector_query)\n",
    "        if result_index == int(questions[j][\"answer_sentence\"]):\n",
    "            correct.append((i,j,result_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partACorrect = correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4864\n"
     ]
    }
   ],
   "source": [
    "print len(partACorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# The required jar files : https://nlp.stanford.edu/software/CRF-NER.shtml#Download\n",
    "# It's 171mb so I've added to the gitignore\n",
    "# If you download it, and rename the folder name \"stanford\" in the main directory\n",
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle # Useful for read / write of list file\n",
    "import os #Needed to check if file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets store the stanford tagger output in a file\n",
    "# This function returns the tagging output of stanford for each dataset\n",
    "# with datasetName - 'train', 'dev', test' \n",
    "\n",
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taggedTrain = getStanfordTagging('train')\n",
    "taggedDev = getStanfordTagging('dev')\n",
    "taggedTest = getStanfordTagging('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second main part of your basic QA system is an NER system. In this initial system you should have at least four\n",
    "answer types: PERSON, LOCATION, NUMBER, and OTHER. You should run the Stanford NER system over your\n",
    "sentences to extract people and location entities (Hint: make use of the \"tag_sents\" method in the NLTK interface to do\n",
    "this efficiently for multiple sentences in a single call, otherwise this will be very slow; you may also want to cache the\n",
    "entity information during development of your system, rather than calling Stanford NER for each run). Note that\n",
    "contiguous words tagged as the same type should be considered part of the same entity. ORGANIZATION entities\n",
    "extracted by the NER system should be considered OTHER. You should also extract and treat as OTHER any other\n",
    "non-sentence initial sequence of capitalized words not tagged by Stanford NER. Finally, you should label all numbers as\n",
    "NUMBER. In this process, you might notice errors related to your preprocessing (e.g. tokenization), errors which can be\n",
    "easily corrected should be addressed at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for subTrain in taggedTrain:\n",
    "    for sent in subTrain:\n",
    "        for (word, tag) in sent:\n",
    "            counter[tag] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({u'LOCATION': 59363,\n",
       "         u'O': 1903787,\n",
       "         u'ORGANIZATION': 46508,\n",
       "         u'PERSON': 47497})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Given a stanford tagged list, refines the list by:\\n\",\n",
    "# Grouping all contiguous words with the same tag\\n\",\n",
    "# Relabels Organisations as Other\\n\",\n",
    "# Labels Number\\n\",\n",
    "def refineWordTags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif isCapitalised(word):\n",
    "                tag = 'OTHERCAP'\n",
    "            elif word in preUnits:\n",
    "                tag = 'PRENUM'\n",
    "            elif word in postUnits:\n",
    "                tag = 'POSTNUM'\n",
    "            elif isStopWord(word):\n",
    "                tag = 'STOPWORD'\n",
    "            elif isPunctuation(word):\n",
    "                tag = 'PUNC'\n",
    "\n",
    "        newWordTags.append((word, tag))\n",
    "    \n",
    "    newWordTags = combineTags (newWordTags)\n",
    "    return newWordTags\n",
    "        \n",
    "def combineTags(wordTags):\n",
    "    newTags = []\n",
    "    prevWord = wordTags[0][0]\n",
    "    prevTag = wordTags[0][1]\n",
    "    for (word, tag) in wordTags[1:]:\n",
    "        if tag == 'NUMBER' and prevTag == 'PRENUM':\n",
    "            prevTag = 'NUMBER'\n",
    "        elif prevTag == 'PRENUM':\n",
    "            prevTag = 'O'\n",
    "        if tag == 'POSTNUM' and prevTag == \"NUMBER\":\n",
    "            tag = \"NUMBER\"\n",
    "        elif tag == \"POSTNUM\":\n",
    "            tag = \"O\"\n",
    "        newTags.append((prevWord, prevTag))\n",
    "        prevWord = word\n",
    "        prevTag = tag\n",
    "    newTags.append((prevWord, prevTag))\n",
    "    \n",
    "    newNewTags = []\n",
    "    prevWord = newTags[0][0]\n",
    "    prevTag = newTags[0][1]\n",
    "    if (prevTag == \"OTHERCAP\"):\n",
    "        prevTag = \"O\"\n",
    "    for (word, tag) in wordTags[1:]:            \n",
    "        if tag == prevTag :\n",
    "            prevWord += ' ' + word\n",
    "        else :\n",
    "            newNewTags.append((prevWord, prevTag))\n",
    "            prevWord = word\n",
    "            prevTag = tag\n",
    "    newNewTags.append((prevWord, prevTag))\n",
    "    return newNewTags\n",
    "\n",
    "# Thanks for this list to save me typing it : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "punctuation = ['.',',',';',':']\n",
    "\n",
    "def isPunctuation(word):\n",
    "    return word in punctuation\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Obtained from training data\n",
    "postUnits = [u'%', u'century', u'years', u'percent', u'years ago', u'days', u'months', u'km', u'hours', u'times', u'inches', u'\\xb0C', u'minutes', u'acres', u'\\xb0F', u'weeks', u'people', u'sq mi', u'mi', u'ft', u'feet', u'metres', u'mm', u'square miles', u'miles', u'pm', u'per cent', u'year', u'copies', u'yuan', u'men', u'square feet', u'third', u'kilometres', u'nm', u'tonnes', u'species', u'decades', u'barrels', u'tons', u'largest', u'centuries', u'km2']\n",
    "preUnits = [u'$', u'around', u'late', u'early', u'nearly', u'since', u'approximately', u'number']\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word.lower() in stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluateNER():\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    for (doc, questID, sentID) in partACorrect:\n",
    "        answer = dev[doc][questID][\"answer\"]\n",
    "        possAnswers = refineWordTags(taggedDev[doc][sentID])        \n",
    "        inThere = False\n",
    "        for possAnswer in possAnswers:\n",
    "            if possAnswer[0] == answer:\n",
    "                inThere = True\n",
    "                break\n",
    "        if inThere:\n",
    "            correct.append((doc,sentID,answer, possAnswers))\n",
    "        else :\n",
    "            wrong.append((doc,sentID, answer, possAnswers))\n",
    "    return (correct, wrong)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.458881578947\n"
     ]
    }
   ],
   "source": [
    "(nerCorrectList, nerWrongList) = evaluateNER()\n",
    "print (len(nerCorrectList) + 0.0) / (len(nerCorrectList) +len(nerWrongList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.462787828947\n",
    "With mistake :D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showWrong(wrongItem):\n",
    "    print \"ANS: \" + wrongItem[2]\n",
    "    print \n",
    "    print wrongItem[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANS: Infrared astronomy\n",
      "\n",
      "[(u'Infrared astronomy uses sensor-equipped telescopes', 'O'), (u'to', 'STOPWORD'), (u'penetrate dusty regions', u'O'), (u'of', 'STOPWORD'), (u'space', u'O'), (u',', 'PUNC'), (u'such as', 'STOPWORD'), (u'molecular clouds', u'O'), (u';', 'PUNC'), (u'detect objects', u'O'), (u'such as', 'STOPWORD'), (u'planets', u'O'), (u',', 'PUNC'), (u'and to', 'STOPWORD'), (u'view highly red-shifted objects', u'O'), (u'from the', 'STOPWORD'), (u'early', 'PRENUM'), (u'days', 'POSTNUM'), (u'of the', 'STOPWORD'), (u'universe', u'O'), (u'.', 'PUNC')]\n"
     ]
    }
   ],
   "source": [
    "showWrong(nerWrongList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
