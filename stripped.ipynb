{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the python script containing the same code as the load the data notebook\n",
    "%run loadData.py\n",
    "# now we can access train, dev, and test\n",
    "# along with trainSents, devSents testSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pickle # Useful for read / write of list file\n",
    "import os #Needed to check if file exists\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from string import punctuation  \n",
    "punct_tokens = set(punctuation)\n",
    "extra_tokens = set([\"what\", \"where\", \"how\", \"when\", \"who\"])\n",
    "\n",
    "filter_tokens = extra_tokens.union(punct_tokens).union(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing tuning functions\n",
    "\n",
    "# Follow lemmatize function from guide notebook: WSTA_N1B_preprocessing.ipynb\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.WordPunctTokenizer() #word_tokenize #tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(line):\n",
    "    tokenized_sentence = word_tokenizer.tokenize(line.lower())\n",
    "    lemmatized_sentence = [lemmatize(token) for token in tokenized_sentence]\n",
    "    filtered_sentence = [token for token in lemmatized_sentence if token not in filter_tokens]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "def vectorize_documents(text_documents):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', tokenizer=pre_process)\n",
    "    vector_documents = vectorizer.fit_transform(text_documents)\n",
    "    \n",
    "    return [vector_documents, vectorizer]\n",
    "\n",
    "def vectorize_query(vectorizer, text_query):\n",
    "    return vectorizer.transform([text_query])\n",
    "\n",
    "def process_neighbours(vector_documents):\n",
    "    \n",
    "    neighbours = NearestNeighbors(1, algorithm=\"brute\", metric=\"cosine\")\n",
    "    neighbours.fit(vector_documents)\n",
    "    \n",
    "    return neighbours\n",
    "\n",
    "def closest_document(neighbours, vector_query):\n",
    "\n",
    "    result = neighbours.kneighbors(vector_query, 1, return_distance=True)\n",
    "\n",
    "    result_index = result[1][0][0]\n",
    "    result_distance = result[0][0][0]\n",
    "    \n",
    "    return [result_distance, result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taggedTrain = getStanfordTagging('train')\n",
    "taggedDev = getStanfordTagging('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a stanford tagged list, refines the list by:\\n\",\n",
    "# Grouping all contiguous words with the same tag\\n\",\n",
    "# Relabels Organisations as Other\\n\",\n",
    "# Labels Number\\n\",\n",
    "def refineWordTags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    prevWord = ''\n",
    "    prevTag = taggedWordList[0][1] # Ie the first tag\\n\",\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif isCapitalised(word):\n",
    "                tag = 'OTHERCAP'\n",
    "            elif isStopWord(word):\n",
    "                tag = 'STOPWORD'\n",
    "        if (tag == prevTag):\n",
    "            prevWord += ' ' + word\n",
    "        else :\n",
    "            newWordTags.append((prevWord, prevTag))\n",
    "            prevWord = word\n",
    "            prevTag = tag\n",
    "    # Need to add the final ones\n",
    "    newWordTags.append((prevWord, prevTag))\n",
    "    return newWordTags\n",
    "        \n",
    "# Thanks : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word.lower() in stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluateNER(questionsList, documentsList, numToEval):\n",
    "    \n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    for i in range (0, numToEval):\n",
    "        \n",
    "        documents = documentsList[i]\n",
    "        questions = questionsList[i]\n",
    "        \n",
    "        for j in range (0, len(questions)):\n",
    "            \n",
    "            answer = questionsList[i][j][\"answer\"]\n",
    "            answerID = questionsList[i][j][\"answer_sentence\"]\n",
    "            possAnswers = refineWordTags(taggedDev[i][answerID])\n",
    "            inThere = False\n",
    "            \n",
    "            for possAnswer in possAnswers:\n",
    "                if possAnswer[0] == answer:\n",
    "                    inThere = True\n",
    "                    break\n",
    "            if inThere:\n",
    "                correct.append((i,j, possAnswers))\n",
    "            else:\n",
    "                wrong.append((i,j, answer, possAnswers))\n",
    "\n",
    "    return (correct, wrong)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Correct : 3408\n",
      "Number incorrect: 5055\n",
      "Average correct : 0.402694080113\n"
     ]
    }
   ],
   "source": [
    "(corNER, wrongNER) = evaluateNER(dev, devSents,len(dev))\n",
    "\n",
    "print(\"Number Correct : \" + str(len(corNER)))\n",
    "print(\"Number incorrect: \" + str(len(wrongNER)))\n",
    "print(\"Average correct : \" + str((len(corNER) + 0.0) / (len(corNER)+len(wrongNER))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Part A gives us a most likely sentence\n",
    "# Part B splits into entities\n",
    "\n",
    "# Given a question, returns a tag for the answer form\n",
    "# From PERSON, LOCATION, NUMBER, OTHER \n",
    "# Assuming question is lowercased\n",
    "def getQuestionType(question):\n",
    "    if 'Who' in question:\n",
    "        return \"PERSON\"\n",
    "    if 'where' in question:\n",
    "        return \"LOCATION\"\n",
    "    if 'How many' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'How much' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'When' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'what year' in question:\n",
    "        return \"NUMBER\"\n",
    "    else:\n",
    "        return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_entitites(entities):\n",
    "    return filter(lambda x: x[0] == \"O\" or x[0] == \"STOPWORD\", entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, answers whose content words all appear in the question should be ranked lowest.\n",
    "\n",
    "def first_filter2(question, answer_entities):\n",
    "   \n",
    "    ranked_list = []\n",
    "    \n",
    "    question = set(pre_process(question))\n",
    "    \n",
    "#     print question\n",
    "#     print\n",
    "    \n",
    "    for entity in answer_entities:\n",
    "\n",
    "        raw_span = entity[0]\n",
    "        span_tag = entity[1]\n",
    "        \n",
    "        set_span = set(pre_process(raw_span))\n",
    "        \n",
    "        if span_tag != \"O\" and span_tag != \"STOPWORD\" and span_tag !=\"O\":\n",
    "            \n",
    "            if set_span.issubset(question):\n",
    "                \n",
    "                ranked_list.append([entity, 1])\n",
    "#                 print \"IN\", raw_span, span_tag, set_span, question\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                ranked_list.append([entity, 2])\n",
    "#                 print \"OUT\", raw_span, span_tag, set_span, question\n",
    "    \n",
    "    return sorted(ranked_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second, answers which match the question type should be ranked higher than those that don't; for this, you\n",
    "# should build a simple rule-based question type classifier based on key words (e.g. questions which contain \"who\" are\n",
    "# people).\n",
    "\n",
    "# First, answers whose content words all appear in the question should be ranked lowest.\n",
    "\n",
    "def second_filter2(question, ranked_list):\n",
    "   \n",
    "    question_type = getQuestionType(question)\n",
    "#     print question_type\n",
    "    \n",
    "    for index, answer in enumerate(ranked_list):\n",
    "        \n",
    "        entity_tag = answer[0][1]\n",
    "        \n",
    "        if entity_tag == question_type:\n",
    "#             print \"MATCH\", answer[0], question_type, question\n",
    "            ranked_list[index].append(2)\n",
    "#             ranked_list[index][1] += 1\n",
    "        else:\n",
    "            ranked_list[index].append(1)\n",
    "#             ranked_list[index][1] -= 1\n",
    "            \n",
    "    return ranked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Follow lemmatize function from guide notebook: WSTA_N1B_preprocessing.ipynb\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.WordPunctTokenizer() #word_tokenize #tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process2(line):\n",
    "    tokenized_sentence = word_tokenizer.tokenize(line.lower())\n",
    "    lemmatized_sentence = [lemmatize(token) for token in tokenized_sentence]\n",
    "    filtered_sentence = [token for token in lemmatized_sentence if token not in filter_tokens]\n",
    "    tagged_sent = nltk.pos_tag(lemmatized_sentence)\n",
    "    final = []\n",
    "    for word, tag in tagged_sent:\n",
    "        if \"V\" in tag or \"NN\" in tag:\n",
    "#             final.append((word,tag))\n",
    "            final.append(word)\n",
    "            \n",
    "#     print \"RESULT: \", final\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Third, among entities of the same type, the prefered entity should be the one which is closer in the sentence to a\n",
    "# closed-class word from the question.\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def third_filter2(question, possAnswers, ranked_list):\n",
    "    \n",
    "    question = pre_process2(question)\n",
    "\n",
    "    answer_sent = \" \".join([x[0] for x in possAnswers])\n",
    "    answer_sent = pre_process(answer_sent)\n",
    "    raw_answer_sent = \" \".join(answer_sent)\n",
    "    \n",
    "#     print \"QUESTION: \"\n",
    "#     pp.pprint(question)\n",
    "#     print \"ANSWER: \"\n",
    "#     pp.pprint(answer_sent)\n",
    "#     pp.pprint(raw_answer_sent)\n",
    "    \n",
    "    for index, answer in enumerate(ranked_list):\n",
    "\n",
    "        span_tag = answer[0][1]\n",
    "        raw_span = answer[0][0]\n",
    "\n",
    "        proc_span = pre_process(raw_span)\n",
    "\n",
    "        raw_proc_span = \" \".join(proc_span)\n",
    "        new_raw_proc_span = \"-\".join(proc_span)\n",
    "\n",
    "        raw_answer_sent = raw_answer_sent.replace(raw_proc_span, new_raw_proc_span)\n",
    "    \n",
    "    answer_sent = raw_answer_sent.split(\" \")\n",
    "    \n",
    "    avg_dict = defaultdict(float)\n",
    "    \n",
    "    for open_class in question:\n",
    "        \n",
    "        if open_class in answer_sent:\n",
    "            \n",
    "            open_class_locations = [i for i, x in enumerate(answer_sent) if x == open_class]\n",
    "            \n",
    "#             print \"OPEN CLASS: \", repr(open_class)\n",
    "\n",
    "            for index, answer in enumerate(ranked_list):\n",
    "\n",
    "                span_tag = answer[0][1]\n",
    "                raw_span = answer[0][0]\n",
    "\n",
    "                proc_span = pre_process(raw_span)\n",
    "                \n",
    "                raw_proc_span = \" \".join(proc_span)\n",
    "                new_raw_proc_span = \"-\".join(proc_span)\n",
    "                \n",
    "                proc_span_locations = [i for i, x in enumerate(answer_sent) if x == new_raw_proc_span]\n",
    "                \n",
    "                min_dist = len(answer_sent)\n",
    "                min_dist_ind = (None, None)\n",
    "                \n",
    "                for loc1 in proc_span_locations:\n",
    "                    \n",
    "                    for loc2 in open_class_locations:\n",
    "                        \n",
    "                        dist = abs(loc1 - loc2)\n",
    "                        \n",
    "                        if dist < min_dist:\n",
    "                            \n",
    "                            min_dist = dist\n",
    "                            min_dist_ind = (loc1, loc2)\n",
    "                \n",
    "#                 print \"PROC: \", proc_span_locations\n",
    "#                 print \"OPEN CLASS: \", open_class_locations                \n",
    "                scale = (len(answer_sent) - min_dist) * 1.0 / len(answer_sent)\n",
    "#                 print \"JOINT: \", min_dist_ind, scale\n",
    "                avg_dict[index] += scale\n",
    "#                 ranked_list[index][1] *= scale\n",
    "    \n",
    "    \n",
    "    for key, value in avg_dict.iteritems():\n",
    "        ranked_list[key].append(value / len(question))\n",
    "\n",
    "    return ranked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_rank(ranking_list):\n",
    "    \n",
    "    new_ranking = []\n",
    "    \n",
    "    for rank in ranking_list:\n",
    "        \n",
    "        new_rank = ( rank[1] + rank[2] )\n",
    "        \n",
    "        if len(rank) == 4:\n",
    "             new_rank *= rank[3]\n",
    "        \n",
    "        new_ranking.append([rank[0], new_rank])\n",
    "        \n",
    "    return sorted(new_ranking, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluateAnswerRanking2(questionsList,documentsList, numToEval):\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    (corNER, wrongNER) = evaluateNER(questionsList,documentsList, numToEval)\n",
    "    \n",
    "    entityListsWithAnswer = corNER\n",
    "    \n",
    "    for index, (i, j, possAnswers) in enumerate(entityListsWithAnswer):\n",
    "        \n",
    "        question = questionsList[i][j][\"question\"]\n",
    "        answer =  questionsList[i][j][\"answer\"]\n",
    "        \n",
    "        first_pass = first_filter2(question, possAnswers)\n",
    "        second_pass = second_filter2(question, first_pass)\n",
    "        third_pass = third_filter2(question, possAnswers, second_pass)\n",
    "        fourth_pass = reduce_rank(third_pass)\n",
    "        \n",
    "        pp.pprint(fourth_pass)\n",
    "        print\n",
    "    \n",
    "        if index > 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   [(u'Leaves', 'OTHERCAP'), 1.3548387096774193],\n",
      "    [(u'IR-filter', 'OTHERCAP'), 1.2096774193548385],\n",
      "    [(u'IR', u'LOCATION'), 0.967741935483871],\n",
      "    [(u'IR-passing', 'OTHERCAP'), 0.629032258064516],\n",
      "    [(u'Wood', 'OTHERCAP'), 0.3870967741935484],\n",
      "    [(u'IR-glowing', 'OTHERCAP'), 0.24193548387096775]]\n",
      "\n",
      "[[(u'The', 'OTHERCAP'), 0.0]]\n",
      "\n",
      "[   [(u'William Herschel', u'PERSON'), 2.888888888888889],\n",
      "    [(u'19th', 'NUMBER'), 1.1666666666666665],\n",
      "    [(u'The', 'OTHERCAP'), 0.0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateAnswerRanking2(dev, devSents, len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluateAnswerRanking(questionsList,documentsList, numToEval):\n",
    "    \n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    (corNER, wrongNER) = evaluateNER(questionsList,documentsList, numToEval)\n",
    "    entityListsWithAnswer = corNER\n",
    "    \n",
    "    for (i,j,possAnswers) in entityListsWithAnswer:\n",
    "        \n",
    "        question = questionsList[i][j][\"question\"]\n",
    "        answer =  questionsList[i][j][\"answer\"]\n",
    "        \n",
    "        first_pass = first_filter2(question, possAnswers)\n",
    "        second_pass = second_filter2(question, first_pass)\n",
    "        third_pass = third_filter2(question, possAnswers, second_pass)\n",
    "        fourth_pass = reduce_rank(third_pass)\n",
    "                \n",
    "#         pp.pprint(third_pass)\n",
    "\n",
    "        top_answer = fourth_pass.pop(0)\n",
    "#         pp.pprint(top_answer)        \n",
    "        answerPredicited = top_answer[0][0]\n",
    "        \n",
    "#         print answerPredicited\n",
    "\n",
    "        if (answerPredicited == answer):\n",
    "            correct.append((i,j))\n",
    "        else :\n",
    "            wrong.append((i,j,answerPredicited))\n",
    "#         break\n",
    "        #print correct\n",
    "    return (correct, wrong)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Correct : 1707\n",
      "Number incorrect: 1701\n",
      "Average correct : 0.50088028169\n"
     ]
    }
   ],
   "source": [
    "(corAns, wrongAns) = evaluateAnswerRanking(dev, devSents,len(dev))\n",
    "print(\"Number Correct : \" + str(len(corAns)))\n",
    "print(\"Number incorrect: \" + str(len(wrongAns)))\n",
    "print (\"Average correct : \" + str((len(corAns) + 0.0) / (len(corAns)+len(wrongAns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#first:  whose anwsers all appear in the questions rank the lowest\n",
    "#assumption: input question in its dictionary value\n",
    "from string import punctuation  \n",
    "non_words = list(punctuation)\n",
    "\n",
    "def first_filter(question, anwser_entities):\n",
    "    ranking_dict_1 = {}\n",
    "    ranking_list = []\n",
    "    merge_list = []\n",
    "    answer_entities_list = []\n",
    "    for entity in anwser_entities:\n",
    "        answer_entities_list.append(entity[0])\n",
    "        if entity[0] in question:\n",
    "            if entity[0].lower() not in stop_words and entity[0]!='' and entity[0].lower() not in non_words:\n",
    "                #print entity[0]\n",
    "                merge_list.append(entity[0])\n",
    "        else:\n",
    "            if entity[0].lower() not in stop_words and entity[0]!='' and entity[0].lower() not in non_words:\n",
    "                ranking_list.append(entity)\n",
    "    ranking_dict_1[\"ranking_list\"] = ranking_list\n",
    "    ranking_dict_1[\"same_word_list\"] = merge_list\n",
    "    ranking_dict_1[\"answer_entities_list\"] = answer_entities_list\n",
    "    return ranking_dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#second: answers which match the question type should be ranked higher than those that dont\n",
    "\n",
    "#assumption: save questions' type in the dictionary format quesiton1 = \n",
    "#{u'answer': u'long playing',u'answer_sentence': 2, u'question':......., 'question_type:'PERSON'}\n",
    "\n",
    "def second_filter(question, ranking_dict_1):\n",
    "    question_with_type ={}\n",
    "    question_with_type['question_type']= getQuestionType(question)\n",
    "    question_with_type['question'] = question\n",
    "    #print question_with_type\n",
    "    ranking_dict_2 = {}\n",
    "    ranking_list =[]\n",
    "    merge_list = []\n",
    "    for entity in ranking_dict_1[\"ranking_list\"]:\n",
    "        if question_with_type['question_type'] == 'O':\n",
    "            ranking_list.append(entity[0])\n",
    "        else:\n",
    "            if entity[1] == question_with_type['question_type']:\n",
    "                ranking_list.append(entity[0])\n",
    "            else:\n",
    "                merge_list.append(entity[0])\n",
    "    ranking_dict_2[\"same_word_list\"] = ranking_dict_1[\"same_word_list\"]\n",
    "    ranking_dict_2[\"ranking_list\"] = ranking_list\n",
    "    ranking_dict_2[\"Other_tags_list\"] = merge_list\n",
    "    ranking_dict_2[\"answer_entities_list\"] = ranking_dict_1[\"answer_entities_list\"]\n",
    "    return ranking_dict_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Thrid: based on second, the prefered entity should be the one which is close in \n",
    "#the sentence to a closed-class word form the question\n",
    "from collections import OrderedDict\n",
    "\n",
    "def cal_distance_words(entity,same_words, anwser_entities):\n",
    "    temp = 0\n",
    "    for same_word in same_words:\n",
    "        temp += abs(anwser_entities.index(entity) - anwser_entities.index(same_word))\n",
    "    return float(temp)/float(len(same_words))\n",
    "\n",
    "def sort_orderedDict(orderdict):\n",
    "    return OrderedDict(sorted(orderdict.items(), key = lambda x:x[1], reverse = False))\n",
    "        \n",
    "\n",
    "def third_filter(question,second_filter,anwser_entities):\n",
    "    dict_ranking ={}\n",
    "    if (len(second_filter[\"same_word_list\"])==0):\n",
    "        if len(second_filter['ranking_list']) != 0:\n",
    "            return second_filter['ranking_list'][0]\n",
    "        else:\n",
    "            return second_filter[\"Other_tags_list\"][0]\n",
    "            #for entity in second_filter[\"Other_tags_list\"]:\n",
    "                #dict_ranking[entity]= cal_distance_words(entity, second_filter[\"same_word_list\"],anwser_entities)\n",
    "            #dict_ranking = sort_orderedDict(dict_ranking)\n",
    "            #if len(dict_ranking.items()) ==0:\n",
    "                #return 0\n",
    "            #else:\n",
    "            #return dict_ranking.items()[0][0]\n",
    "    else:\n",
    "        #if len(second_filter['ranking_list']) != 0:\n",
    "        for entity in second_filter[\"ranking_list\"]:\n",
    "            dict_ranking[entity]= cal_distance_words(entity, second_filter[\"same_word_list\"],anwser_entities)\n",
    "        #else:\n",
    "            #if len(second_filter[\"Other_tags_list\"]) !=0:\n",
    "                #for entity in second_filter[\"Other_tags_list\"]:\n",
    "                    #dict_ranking[entity]= cal_distance_words(entity, second_filter[\"same_word_list\"],anwser_entities)\n",
    "            #print dict_ranking\n",
    "            #else:\n",
    "                #return 0\n",
    "        dict_ranking = sort_orderedDict(dict_ranking)\n",
    "            #print dict_ranking\n",
    "        if len(dict_ranking.items()) ==0:\n",
    "            return 0\n",
    "        else:\n",
    "            return dict_ranking.items()[0][0]#second: answers which match the question type should be ranked higher than those that dont\n",
    "\n",
    "#assumption: save questions' type in the dictionary format quesiton1 = \n",
    "#{u'answer': u'long playing',u'answer_sentence': 2, u'question':......., 'question_type:'PERSON'}\n",
    "\n",
    "def second_filter(question, ranking_dict_1):\n",
    "    question_with_type ={}\n",
    "    question_with_type['question_type']= getQuestionType(question)\n",
    "    question_with_type['question'] = question\n",
    "    #print question_with_type\n",
    "    ranking_dict_2 = {}\n",
    "    ranking_list =[]\n",
    "    merge_list = []\n",
    "    for entity in ranking_dict_1[\"ranking_list\"]:\n",
    "        if question_with_type['question_type'] == 'O':\n",
    "            ranking_list.append(entity[0])\n",
    "        else:\n",
    "            if entity[1] == question_with_type['question_type']:\n",
    "                ranking_list.append(entity[0])\n",
    "            else:\n",
    "                merge_list.append(entity[0])\n",
    "    ranking_dict_2[\"same_word_list\"] = ranking_dict_1[\"same_word_list\"]\n",
    "    ranking_dict_2[\"ranking_list\"] = ranking_list\n",
    "    ranking_dict_2[\"Other_tags_list\"] = merge_list\n",
    "    ranking_dict_2[\"answer_entities_list\"] = ranking_dict_1[\"answer_entities_list\"]\n",
    "    return ranking_dict_2\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
