{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Retival\n",
    "Copy this at the end of the other notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Where numToEval is max 360 for train\n",
    "def evaluateSentenceRetrival(questionsList,documentsList, numToEval):\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    for i in range (0, numToEval):\n",
    "        documents = documentsList[i]\n",
    "        questions = questionsList[i]\n",
    "        \n",
    "        vector_documents, vectorizer = vectorize_documents(documents)\n",
    "        analyze = vectorizer.build_analyzer()\n",
    "        neighbours = process_neighbours(vector_documents)\n",
    "        \n",
    "        for j in range (0, len(questions)):\n",
    "            text_query = questions[j][\"question\"]\n",
    "            vector_query = vectorize_query(vectorizer, text_query)\n",
    "            result_similarity, result_index  = closest_document(neighbours, vector_query)\n",
    "            if result_index == int(questions[j][\"answer_sentence\"]):\n",
    "                correct.append((i,j,result_index))\n",
    "            else :\n",
    "                wrong.append ((i,j,result_index))\n",
    "    return (correct, wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(corSR, wrongSR) = evaluateSentenceRetrival(dev, devSents,len(dev))\n",
    "print(\"Number Correct : \" + str(len(corSR)))\n",
    "print(\"Number incorrect: \" + str(len(wrongSR)))\n",
    "print (\"Average correct : \" + str((len(corSR) + 0.0) / (len(corSR)+len(wrongSR))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ran in the other docco. Getting about 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER\n",
    "Tests to see if the required entity exists within the given taggings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluateNER(questionsList,documentsList, numToEval):\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    for i in range (0, numToEval):\n",
    "        documents = documentsList[i]\n",
    "        questions = questionsList[i]\n",
    "        for j in range (0, len(questions)):\n",
    "            answer = questionsList[i][j][\"answer\"]\n",
    "            answerID = questionsList[i][j][\"answer_sentence\"]\n",
    "            possAnswers = refineWordTags(taggedDev[i][answerID])\n",
    "            inThere = False\n",
    "            for possAnswer in possAnswers:\n",
    "                if possAnswer[0] == answer:\n",
    "                    inThere = True\n",
    "                    break\n",
    "            if inThere:\n",
    "                correct.append((i,j, possAnswers))\n",
    "            else :\n",
    "                wrong.append((i,j, answer, possAnswers))\n",
    "    return (correct, wrong)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(corNER, wrongNER) = evaluateNER(dev, devSents,len(dev))\n",
    "\n",
    "print(\"Number Correct : \" + str(len(corNER)))\n",
    "print(\"Number incorrect: \" + str(len(wrongNER)))\n",
    "print(\"Average correct : \" + str((len(corNER) + 0.0) / (len(corNER)+len(wrongNER))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the wrong taggings\n",
    "for num in range (0, 10):\n",
    "    (i,j, answer, possAnswers) = wrong[ num]\n",
    "    print \"ANSWER : \" + answer\n",
    "    for possAnswer in possAnswers:\n",
    "        print possAnswer[0]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Ranking\n",
    "For each \"correct\" tagging above (ie tags where the answer is a member\n",
    "Test the answer ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluateAnswerRanking(questionsList,documentsList, numToEval):\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    (corNER, wrongNER) = evaluateNER(questionsList,documentsList, numToEval)\n",
    "    entityListsWithAnswer = corNER\n",
    "    for (i,j,possAnswers) in entityListsWithAnswer:\n",
    "        question = questionsList[i][j][\"question\"]\n",
    "        answer =  questionsList[i][j][\"answer\"]\n",
    "        answerPredicited = \"\" # TODO add Dereks part here\n",
    "        if (answerPredicited == answer):\n",
    "            correct.append((i,j))\n",
    "        else :\n",
    "            wrong.append((i,j,answerPredicited))\n",
    "    return (correct, wrong)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(corAns, wrongAns) = evaluateAnswerRanking(dev, devSents,len(dev))\n",
    "print(\"Number Correct : \" + str(len(corAns)))\n",
    "print(\"Number incorrect: \" + str(len(wrongAns)))\n",
    "print (\"Average correct : \" + str((len(corAns) + 0.0) / (len(corAns)+len(wrongAns))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
