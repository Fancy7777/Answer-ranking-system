{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the python script containing the same code as the load the data notebook\n",
    "%run loadData.py\n",
    "# now we can access train, dev, and test\n",
    "# along with trainSents, devSents testSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from string import punctuation  \n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import csv\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)\n",
    "\n",
    "punct_tokens = set(punctuation)\n",
    "extra_tokens = set([\"what\", \"where\", \"how\", \"when\", \"who\"])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filter_tokens = extra_tokens.union(punct_tokens).union(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_answers = []\n",
    "for i in range(0, len(train)):\n",
    "    qs = train[i]\n",
    "    for j in range(0, len(qs)):\n",
    "        q = qs[j]\n",
    "        answer = q[\"answer\"]\n",
    "        train_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    elif (datasetName == 'ans_train'):\n",
    "        theFilePath = './preCompTags/stanfordTaggedTrainAnswers.txt'\n",
    "        theSents = train_answers\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_train_set = getStanfordTagging('train')\n",
    "tagged_dev_set = getStanfordTagging('dev')\n",
    "tagged_test_set = getStanfordTagging('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "# Thanks for this list to save me typing it : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "punctuation = [\"''\",'``','(','.',':', ',',')']\n",
    "\n",
    "\n",
    "months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "\n",
    "def isPunctuation(word):\n",
    "    return word in punctuation\n",
    "\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Obtained from training data\n",
    "postUnits = [u'%', u'century', u'years', u'percent', u'years ago', u'days', u'months', u'km', u'hours', u'times', u'inches', u'\\xb0C', u'minutes', u'acres', u'\\xb0F', u'weeks', u'people', u'sq mi', u'mi', u'ft', u'feet', u'metres', u'mm', u'square miles', u'miles', u'pm', u'per cent', u'year', u'copies', u'yuan', u'men', u'square feet', u'third', u'kilometres', u'nm', u'tonnes', u'species', u'decades', u'barrels', u'tons', u'largest', u'centuries', u'km2']\n",
    "preUnits = [u'$',u'around', u'late', u'early', u'nearly', u'since', u'approximately', u'number']\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    if word in months:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word.lower() in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = \"\"\" ANS: {<JJ>?<N.*>*}\n",
    "                   {<DT>?<N.*>*}\n",
    "                   }<UH|POS|VB|VBG|RP|DT|MD|PRP$|TO|RB|JJS|PDT|IN|PRP|VBP|VBN|RBS|WRB|WP|EX|VBZ|WDT|VBD>{\n",
    "                    \"\"\"\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "\n",
    "def chunk(words):\n",
    "    tokenWS = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    chunks =  cp.parse(tokenWS)\n",
    "    possAnswers = []\n",
    "    for subtree in chunks.subtrees():\n",
    "        if subtree.label() == 'ANS':\n",
    "            possAnswers.append((' '.join(word for word, pos in subtree.leaves()),'O'))\n",
    "    possAnswers.append((\"Nope\", \"CRAP\")) # To ensure nothing has 0 tags\n",
    "    return possAnswers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_non_chunked_words_as_single_tags(words):\n",
    "    chunked_output = chunk(words)\n",
    "    token_words = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    if len(chunked_output) == 0:\n",
    "        chunked_words = [\"DEREKWANG\"]\n",
    "    else:\n",
    "        chunked_words = [nltk.word_tokenize(word_tag_pair[0]) for word_tag_pair in chunked_output ]    \n",
    "    all_word_tags = []\n",
    "    \n",
    "    current_chunk_index = 0\n",
    "    current_chunk_word = 0\n",
    "    current_chunk_list = chunked_words[0]\n",
    "    \n",
    "    for word_tag_pair in token_words:\n",
    "        word = word_tag_pair[0]\n",
    "        if word == current_chunk_list[current_chunk_word]:\n",
    "            # Need to move onto next word\n",
    "            if current_chunk_word == len(current_chunk_list) - 1:\n",
    "                # last word in this current chunk\n",
    "                all_word_tags.append(chunked_output[current_chunk_index])\n",
    "                current_chunk_index += 1\n",
    "                current_chunk_word = 0\n",
    "                if current_chunk_index == len(chunked_words):\n",
    "                    current_chunk_list = [\"NOPE\"]\n",
    "                else:\n",
    "                    current_chunk_list = chunked_words[current_chunk_index]\n",
    "            else :\n",
    "                current_chunk_word += 1\n",
    "        else :\n",
    "            # Need to add word, as it's not in a chunk :(\n",
    "            all_word_tags.append((word,'O'))\n",
    "    return all_word_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def refine_word_tags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif word in preUnits:\n",
    "                tag = 'PRENUM'\n",
    "            elif word in postUnits:\n",
    "                tag = 'POSTNUM'\n",
    "            elif isCapitalised(word):\n",
    "                tag = \"OTHERCAP\"\n",
    "        newWordTags.append((word, tag))\n",
    "    \n",
    "    newWordTags = combineTags (newWordTags)\n",
    "    other_processed_tags = process_others(newWordTags)\n",
    "    return other_processed_tags\n",
    "        \n",
    "def combineTags(wordTags):\n",
    "    \n",
    "    newTags = []\n",
    "    prevWord = wordTags[0][0]\n",
    "    prevTag = wordTags[0][1]\n",
    "    \n",
    "    for (word, tag) in wordTags[1:]:\n",
    "        if tag == 'NUMBER' and prevTag == 'PRENUM':\n",
    "            prevTag = 'NUMBER'\n",
    "        elif prevTag == 'PRENUM':\n",
    "            prevTag = 'O'\n",
    "        if tag == 'POSTNUM' and prevTag == \"NUMBER\":\n",
    "            tag = \"NUMBER\"\n",
    "        elif tag == \"POSTNUM\":\n",
    "            tag = \"O\"\n",
    "        newTags.append((prevWord, prevTag))\n",
    "        prevWord = word\n",
    "        prevTag = tag\n",
    "    newTags.append((prevWord, prevTag))\n",
    "        \n",
    "    newNewTags = []\n",
    "    prevWord = newTags[0][0]\n",
    "    prevTag = newTags[0][1]\n",
    "    if (prevTag == \"OTHERCAP\" and newTags[1][1] != \"OTHERCAP\"):\n",
    "        prevTag = \"O\"\n",
    "        \n",
    "    for (word, tag) in newTags[1:]:\n",
    "#         print tag, prevTag\n",
    "        if tag == prevTag :\n",
    "            if word == '%':\n",
    "                prevWord += word\n",
    "            else :\n",
    "                if prevWord == '$':\n",
    "                    prevWord += word\n",
    "                else :\n",
    "                    prevWord += ' ' + word\n",
    "        else :\n",
    "            newNewTags.append((prevWord, prevTag))\n",
    "            prevWord = word\n",
    "            prevTag = tag\n",
    "            \n",
    "    newNewTags.append((prevWord, prevTag))\n",
    "    \n",
    "    return newNewTags\n",
    "\n",
    "def process_others(words_with_tags):\n",
    "    new_taggings = []\n",
    "    for (words, tag) in words_with_tags:\n",
    "        if tag == 'O':\n",
    "            #chunk_results = add_non_chunked_words_as_single_tags(words)\n",
    "            #for (word,tag) in chunk_results:\n",
    "            #    new_taggings.append((word, tag))\n",
    "            new_taggings.append((words,tag))\n",
    "        else :\n",
    "            new_taggings.append((words, tag))\n",
    "    return new_taggings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluateNERonDev():\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    for i in range(0, len(dev)):\n",
    "        qs = dev[i]\n",
    "        for j in range(0, len(qs)):\n",
    "            q = qs[j]\n",
    "            idSent  = q[\"answer_sentence\"]\n",
    "            sent = devSents[i][idSent]\n",
    "            answer = q[\"answer\"]\n",
    "            possAnswers = refine_word_tags(tagged_dev_set[i][idSent])\n",
    "            inThere = False\n",
    "            for possAnswer in possAnswers:\n",
    "                if possAnswer[0] == answer:\n",
    "                    inThere = True\n",
    "                    break\n",
    "            if inThere:\n",
    "                correct.append((i, j, idSent, possAnswers))\n",
    "            else :\n",
    "                wrong.append((i, j, idSent, possAnswers))\n",
    "    return (correct, wrong)\n",
    "\n",
    "\n",
    "(correct,wrong) = evaluateNERonDev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.376344086022\n"
     ]
    }
   ],
   "source": [
    "print (len(correct) + 0.0)/ (len(correct) + len(wrong))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with others together, and spliting out other cap - 0.376344086022\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get answers with tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def getAnswerDict(qss):\n",
    "    sentDicts = defaultdict(list)\n",
    "    for docID in range(0, len(qss)):\n",
    "        qs = qss[docID]\n",
    "        for q in qs:\n",
    "            answer = q[\"answer\"]\n",
    "            answerSent = (docID, q[\"answer_sentence\"])\n",
    "            sentDicts[answerSent].append(answer)\n",
    "    return sentDicts\n",
    "train_sentence_contained_answers = getAnswerDict(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'long playing', u'12\", 10\", 7\"', u'rpm']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentence_contained_answers[(0,2)] # contains the answers in that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAnswerIndex(sent, answer):\n",
    "    tokenised_answer = nltk.word_tokenize(answer)\n",
    "    len_tokenised_answer = len(tokenised_answer)\n",
    "    tokenised_sent =  nltk.word_tokenize(sent)\n",
    "    highest_index = (len(tokenised_sent) - len_tokenised_answer) + 1\n",
    "    \n",
    "    for i in range (0, highest_index):\n",
    "            sentence_fragment = tokenised_sent[i:i+len_tokenised_answer]\n",
    "            if (sentence_fragment == tokenised_answer):\n",
    "                return (i, i+len_tokenised_answer)\n",
    "    #print \"Problem, cannot find answer index\"\n",
    "    #print sent\n",
    "    #print answer\n",
    "    #print\n",
    "    return (-1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gramophone record (phonograph record in American English) or vinyl record, commonly known as a \"record\", is an analogue sound storage medium in the form of a flat polyvinyl chloride (previously shellac) disc with an inscribed, modulated spiral groove.\n",
      "\n",
      "[u'analogue sound storage medium']\n",
      "\n",
      "(24, 28)\n"
     ]
    }
   ],
   "source": [
    "the_sentence = trainSents[0][0]\n",
    "the_answers_contained = train_sentence_contained_answers[(0,0)]\n",
    "\n",
    "print the_sentence\n",
    "print\n",
    "print the_answers_contained\n",
    "print\n",
    "print getAnswerIndex (the_sentence, the_answers_contained[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_candidates_to_range(candidates):\n",
    "    candidatesIndexRange = []\n",
    "    index = 0\n",
    "    for (words, tag) in candidates:\n",
    "        tokenisedWords = nltk.word_tokenize(words)\n",
    "        nextIndex = index + len(tokenisedWords)\n",
    "        candidatesIndexRange.append((index,nextIndex ))\n",
    "        index = nextIndex\n",
    "    return candidatesIndexRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 7), (7, 9), (9, 49)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_candidates = refine_word_tags(tagged_train_set[0][0])\n",
    "\n",
    "convert_candidates_to_range(example_candidates)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the taggings of the (possibly overlapping) candidates list\n",
    "\n",
    "def get_taggings_in_range((ans_start,ans_end),candidates):\n",
    "    candidate_ranges = convert_candidates_to_range(candidates)\n",
    "    \n",
    "    current_considered_candidate_id = -1\n",
    "    candidates_containing_answer = []\n",
    "    \n",
    "    for (cand_start, cand_end) in candidate_ranges:\n",
    "        current_considered_candidate_id += 1\n",
    "        if ans_start == cand_start and ans_end == cand_end :\n",
    "            #candidates and answer the same\n",
    "            candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            break\n",
    "        if cand_start <= ans_start and cand_end >= ans_end :\n",
    "            #candidate contains the answer, but has extra words\n",
    "            candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            break\n",
    "\n",
    "        if cand_start >= ans_start or cand_end > ans_start:\n",
    "            if (cand_end <= ans_end):\n",
    "                candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            else :\n",
    "                # we have finished, as \n",
    "                if (cand_start < ans_end ):\n",
    "                    candidates_containing_answer.append(current_considered_candidate_id)\n",
    "                break\n",
    "    return candidates_containing_answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The groove usually starts near the periphery and ends near the center of the disc .', 'O')]\n",
      "\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "example_candidates = refine_word_tags(tagged_train_set[0][1])\n",
    "example_answer_range = (3,6)\n",
    "print example_candidates\n",
    "print\n",
    "print get_taggings_in_range(example_answer_range,example_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_NER_tag_analysis() :\n",
    "    other_taggings = []\n",
    "    for i in range(0, len(trainSents)):\n",
    "        sent_set = trainSents[i]\n",
    "        for j in range(0, len(sent_set)):\n",
    "            sent = sent_set[j]\n",
    "            answers_in_sent = train_sentence_contained_answers[(i,j)]\n",
    "            candidates = refine_word_tags(tagged_train_set[i][j])\n",
    "            for answer in answers_in_sent:\n",
    "                (ans_start, ans_end) = getAnswerIndex(sent, answer)\n",
    "                if (ans_start, ans_end) == (-1,-1) :\n",
    "                    continue\n",
    "                   \n",
    "                candidate_ids = get_taggings_in_range((ans_start, ans_end),candidates)\n",
    "                if len(candidate_ids) == 1:\n",
    "                    possible_correct_candidate = candidates[candidate_ids[0]]\n",
    "                    if possible_correct_candidate[1] == \"O\":\n",
    "                        other_taggings.append(answer)\n",
    "    return other_taggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n"
     ]
    }
   ],
   "source": [
    "# Ie answers that were tagged as otherh\n",
    "aswers_tagged_as_other = get_NER_tag_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate rules for Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets generate our data!\n",
    "# For each answer - tag it, and if other:\n",
    "# add to list\n",
    "\n",
    "from collections import Counter\n",
    "other_pos_tag_counter = Counter()\n",
    "\n",
    "for ans in aswers_tagged_as_other:\n",
    "    tokenised_answer = nltk.word_tokenize(ans)\n",
    "    pos_answer =  nltk.pos_tag(tokenised_answer)\n",
    "    \n",
    "    just_tags = [pair[1] for pair in pos_answer]\n",
    "    other_pos_tag_counter[tuple(just_tags)] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('NN',), 4568),\n",
       " (('NNS',), 1510),\n",
       " (('JJ', 'NN'), 1325),\n",
       " (('JJ',), 1196),\n",
       " (('NN', 'NN'), 954)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_pos_tag_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags_by_answer_length = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    tags_by_answer_length.append(Counter())\n",
    "    \n",
    "for tag_tuple in other_pos_tag_counter.keys():\n",
    "    answer_length = len(tag_tuple)\n",
    "    if answer_length == 0:\n",
    "        continue\n",
    "    answer_index = answer_length - 1\n",
    "    if answer_index >= 10:\n",
    "        continue\n",
    "    tags_by_answer_length[answer_index][tag_tuple] += other_pos_tag_counter[tag_tuple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('NN',), 4568),\n",
       " (('NNS',), 1510),\n",
       " (('JJ',), 1196),\n",
       " (('CD',), 233),\n",
       " (('VBG',), 194)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_by_answer_length[0].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('JJ', 'NN'), 1325),\n",
       " (('NN', 'NN'), 954),\n",
       " (('JJ', 'NNS'), 847),\n",
       " (('NN', 'NNS'), 620),\n",
       " (('DT', 'NN'), 448)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_by_answer_length[1].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "tag_terminal_names = defaultdict(lambda:\"TERM0\")\n",
    "\n",
    "    \n",
    "all_tag_types = set()\n",
    "for tags in other_pos_tag_counter:\n",
    "    for tag in tags:\n",
    "        all_tag_types.add(tag)\n",
    "     \n",
    "\n",
    "next_terminal_id = 0\n",
    "for tag in all_tag_types:\n",
    "    tag_terminal_name = tag\n",
    "    if not tag.isalpha():\n",
    "        tag_terminal_name = \"TERM\" + str(next_terminal_id)\n",
    "        next_termindal_id += 1\n",
    "    tag_terminal_names[tag] = tag_terminal_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERM0\n",
      "NN\n",
      "TERM0\n"
     ]
    }
   ],
   "source": [
    "print tag_terminal_names[\"$\"]\n",
    "print tag_terminal_names[\"NN\"]\n",
    "print tag_terminal_names['joshi'] # an unseen pos tag appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def give_grammar_rules(tags,prob):\n",
    "    grammar_rules = set()\n",
    "    \n",
    "    if len(tags)== 1:\n",
    "        first_tag = tags[0]\n",
    "        first_tag_terminal = tag_terminal_names[first_tag]\n",
    "        grammar_rules.add(\"SIZE_ONE -> \" + first_tag_terminal + \" [\" + str(prob) + \"]\")\n",
    "        new_rules = get_sub_grammar_rules(tags)\n",
    "        grammar_rules = grammar_rules.union(new_rules)\n",
    "    if len(tags) == 2:\n",
    "        grammar_rules.add(\"SIZE_TWO -> \" + tag_terminal_names[tags[0]] + \" \" + tag_terminal_names[tags[1]] +\n",
    "                           \" [\" + str(prob) + \"]\")        \n",
    "        grammar_rules = grammar_rules.union(get_sub_grammar_rules(tags))\n",
    "    if len(tags) == 3:\n",
    "        grammar_rules.add(\"SIZE_THREE -> \" + tag_terminal_names[tags[0]] + '_' + tag_terminal_names[tags[1]] + \" \" +tag_terminal_names[tags[2]] +  \" [\" + str(prob) + \"]\")\n",
    "        grammar_rules = grammar_rules.union(get_sub_grammar_rules(tags))\n",
    "    return grammar_rules\n",
    "        \n",
    "def get_sub_grammar_rules(tags):\n",
    "    grammar_rules = set()\n",
    "    if len(tags) == 1:\n",
    "        first_tag = tags[0]\n",
    "\n",
    "        first_tag_terminal = tag_terminal_names[first_tag]\n",
    "        grammar_rules.add(first_tag_terminal + ' -> ' + '\"' + first_tag + '\"' + \" [1]\")\n",
    "\n",
    "    if len(tags) == 2:\n",
    "        grammar_rules.add(tag_terminal_names[tags[0]] + ' -> ' + '\"' + tags[0] + '\"' + \" [1]\")\n",
    "        grammar_rules.add(tag_terminal_names[tags[1]] + ' -> ' + '\"' + tags[1] + '\"' + \" [1]\")\n",
    "        \n",
    "    if len(tags) == 3:\n",
    "        grammar_rules = grammar_rules.union(get_sub_grammar_rules([tags[2]]))\n",
    "        grammar_rules.add(tag_terminal_names[tags[0]] + '_'+tag_terminal_names[tags[1]]+ \" -> \" + tag_terminal_names[tags[0]] + ' ' +  tag_terminal_names[tags[1]] + \" [1]\")\n",
    "        grammar_rules = grammar_rules.union(get_sub_grammar_rules([tags[0]]))\n",
    "        grammar_rules = grammar_rules.union(get_sub_grammar_rules([tags[1]]))\n",
    "    return grammar_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['NNP -> \"NNP\" [1]', 'SIZE_THREE -> NNP_NN NNP [0.5]', 'NN -> \"NN\" [1]', 'NNP_NN -> NNP NN [1]'])\n"
     ]
    }
   ],
   "source": [
    "#print give_grammar_rules(('NNP',),0.5)\n",
    "#print give_grammar_rules(('NNP',\"NN\"), 0.2)\n",
    "print give_grammar_rules(('NNP','NN',\"NNP\"),0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_sizes = [0,0,0,0,0]\n",
    "for i in range(0, 3):\n",
    "      total_sizes[i]= sum (tags_by_answer_length[i].values())\n",
    "\n",
    "total_ans = sum(total_sizes)\n",
    "grammar_rules = set()\n",
    "for ans_length in range(0, 3):\n",
    "    if ans_length == 0 :      \n",
    "        for (tag, val) in tags_by_answer_length[0].most_common(1000000): # Ie all\n",
    "            prob_of_tag = (val + 0.0) / total_sizes[ans_length]\n",
    "            grammar_rules = grammar_rules.union( give_grammar_rules(tag,prob_of_tag))\n",
    "    else :   \n",
    "        for (tags, val) in tags_by_answer_length[ans_length].most_common(5):\n",
    "            prob_of_tag = (val + 0.0) / total_sizes[ans_length]\n",
    "            grammar_rules = grammar_rules.union(give_grammar_rules(tags,prob_of_tag))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grammar_rules.add(\"WORDS -> WORD WORDS [0.5]\")\n",
    "grammar_rules.add(\"WORDS -> WORD [0.5]\")\n",
    "\n",
    "grammar_rules.add(\"WORD -> SIZE_ONE [0.01]\")\n",
    "grammar_rules.add(\"WORD -> SIZE_TWO [0.1]\")\n",
    "grammar_rules.add(\"WORD -> SIZE_THREE [0.89]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Okay, so the above gives some rules to start!\n",
    "rules_in_string = \"\"\n",
    "for rule in grammar_rules:\n",
    "    rules_in_string += rule + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S -> SIZE_THREE [0.89]\n",
      "SIZE_ONE -> VB [0.000971109492595]\n",
      "SIZE_TWO -> JJ NN [0.239515545915]\n",
      "WORD ->\n"
     ]
    }
   ],
   "source": [
    "print rules_in_string[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.grammar import ProbabilisticProduction, PCFG, Nonterminal\n",
    "PCFG.EPSILON = 1000000\n",
    "answer_pcfg = PCFG.fromstring(rules_in_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_pcfg.is_flexible_chomsky_normal_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_CYK(words, grammar):\n",
    "    table = defaultdict(dict)    \n",
    "    back = defaultdict(dict)\n",
    "    \n",
    "    # righter-most index j\n",
    "    for j in range(1, len(words)+1):\n",
    "        # insert token preterminal rewrites, POS -> 'word'\n",
    "        token = words[j-1]\n",
    "        for prod in grammar.productions(rhs=token):\n",
    "            if len(prod.rhs()) == 1:\n",
    "                table[j-1,j][prod.lhs()] = prod.prob()\n",
    "\n",
    "        # deal with pesky unary productions \n",
    "        changes = True\n",
    "        cell = table[j-1,j]\n",
    "        while changes:\n",
    "            # repeat this loop until no rule changes; will infinitely\n",
    "            # loop for grammars with a unary cycle\n",
    "            changes = False\n",
    "            for non_term in list(cell.keys()):\n",
    "                prob = cell[non_term]\n",
    "                for prod in grammar.productions(rhs=non_term):\n",
    "                    if len(prod.rhs()) == 1:\n",
    "                        unary_prob = prod.prob() * prob\n",
    "                        if unary_prob > cell.get(prod.lhs(), 0):\n",
    "                            cell[prod.lhs()] = unary_prob\n",
    "                            back[j-1,j][prod.lhs()] = (None, prod)\n",
    "                            changes = True\n",
    "        \n",
    "        # now look for larger productions that span [i, j]\n",
    "        # allowing i to move leftward over the input\n",
    "        for i in range(j-2, -1, -1):\n",
    "            cell = table[i,j]\n",
    "            # k is the split point, i < k < j\n",
    "            for k in range(i+1, j):\n",
    "                # find chart cells based on the split point\n",
    "                left_cell = table[i,k]\n",
    "                right_cell = table[k,j]\n",
    "                # find binary productions which handle a valid symbol A from left cell, X -> A B\n",
    "                for left_nt, left_prob in left_cell.items():\n",
    "                    for prod in grammar.productions(rhs=left_nt):\n",
    "                        if len(prod.rhs()) == 2:\n",
    "                            # check if the left and right cells have a valid parse\n",
    "                            right_prob = right_cell.get(prod.rhs()[1])\n",
    "                            if left_prob != None and right_prob != None:\n",
    "                                # score the partial parse\n",
    "                                prob = prod.prob() * left_prob * right_prob\n",
    "                                if prob > cell.get(prod.lhs(), 0.0):\n",
    "                                    # if it exceeds the current best analysis, update the cell\n",
    "                                    cell[prod.lhs()] = prob\n",
    "                                    # and store a record of how we got here\n",
    "                                    back[i,j][prod.lhs()] = (k, prod)\n",
    "        # display the table and back pointers\n",
    "    display_CYK_chart(words, table, back)\n",
    "    \n",
    "    # have to build the tree from the back pointers\n",
    "    return build_tree(words, back, grammar.start(), i=0, j=len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_CYK_chart(words, table, back):\n",
    "    length = len(words)\n",
    "    html = ''\n",
    "    html += '<tr>'\n",
    "    for i in range(length):\n",
    "        html += '<td><i>%s<i></td>' % words[i]\n",
    "    html += '</tr>'\n",
    "    for i in range(length):\n",
    "        html += '<tr>'\n",
    "        for j in range(1, length+1):\n",
    "            if j <= i:\n",
    "                html += '<td>'\n",
    "            else:\n",
    "                html += \"<td bgcolor='lightcyan'>\"\n",
    "                html += '[%d,%d]' % (i,j) \n",
    "                for symbol, prob in table.get((i,j), {}).items():\n",
    "                    html += '<br><b>%s</b> [%.5f]' % (str(symbol), prob)\n",
    "                    b = back[i,j].get(symbol)\n",
    "                    if b != None:\n",
    "                        html += '<br>&nbsp; (k=%s, %s)' % (b[0], str(b[1]))\n",
    "            html += '</td>'\n",
    "        html += '</tr>'\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_tree(words, back, symbol, i, j):\n",
    "    backpointer = back[i, j].get(symbol)\n",
    "    if backpointer == None:\n",
    "        # X -> 'word' production\n",
    "        assert j == i+1\n",
    "        return nltk.tree.Tree(symbol, [words[i]])\n",
    "    else:\n",
    "        k, prod = back[i, j][symbol]\n",
    "        if k != None:\n",
    "            # X -> A B binary production\n",
    "            left_subtree = build_tree(words, back, prod.rhs()[0], i=i, j=k)\n",
    "            right_subtree = build_tree(words, back, prod.rhs()[1], i=k, j=j)\n",
    "            return nltk.tree.Tree(symbol, [left_subtree, right_subtree])\n",
    "        else:\n",
    "            # X -> A unary production\n",
    "            subtree = build_tree(words, back, prod.rhs()[0], i=i, j=j)\n",
    "            return nltk.tree.Tree(symbol, [subtree])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<tr><td><i>NN<i></td></tr><tr><td bgcolor='lightcyan'>[0,1]<br><b>S</b> [0.00555]<br>&nbsp; (k=None, S -> SIZE_ONE [0.01])<br><b>WORDS</b> [0.00277]<br>&nbsp; (k=None, WORDS -> WORD [0.5])<br><b>WORD</b> [0.00555]<br>&nbsp; (k=None, WORD -> SIZE_ONE [0.01])<br><b>SIZE_ONE</b> [0.55450]<br>&nbsp; (k=None, SIZE_ONE -> NN [0.554504])<br><b>NN</b> [1.00000]</td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEIAAAB9CAIAAACZGiLHAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAHXRFWHRTb2Z0d2FyZQBHUEwgR2hvc3RzY3JpcHQgOS4xOeMCIOUAAANaSURBVHic7ZvPceMgFIfJzjZASlCOOZoScAlSB5E7MC4BbQeiBKWAHKAEUYKYnJMZ6ZSz90BC2FiWnSzsvnjed5Kx5uHP/JGH3/hqv9+T78+P//0B0oAaZ2CtzVo/cJVpbTjnpJSMsWEYrLXGmBy9vLPPw3a7HccxXGfqJZBrNJRSlNKyLHMUPySXBiHEGGOtHceRMZbdJ/dw7/d7KWXf91m7yLVTCSHCdVEU0zRl6sjzM19pIQRjzDk3jmPTNPk6IlnXBiHEGFMURVEU+brw5NX4Z+CPEUigBiQyarinJ9F1+erH5NR4fv718JCvfgxOKkigBiRQAxKoAQnUgARqQAI1IIEakLgQjYznVNPLi3185Le3merHXOJx2z/LuOIek5xSv47GsYzLGKO1ttauVitCiD9Rjk/LnXP39/f+Timlv40QMk1TVVWc8wWBtm19j9fX10IIpZSUUinFOTfGdF1HCFFKnVXZ5wPLGdeHlvCy6zqt9extWuv4rUPu7u7Cdaiz3W5DkbjCycqvk+rm5iaMwMlD/M1m48eh7/vD79s5Z63lnC8MhTFmvV6Hl2VZaq39dVVVxz7AQuVXjbquKaVN0wgh/CRZwB/0t2272+0+vCWEkFKeM90ppbPtq9VqHMfDCsuV32OaYNk0TVgMx2iapqoqSqlzjryJ+fZpmnxnzrmFZGNBdbfbSSnj4TpZ+UdwDU0nMy6/oXlP55w3CVBKw3Adq8A5D7OIEGKMYYzFFRhj8Q0nK7+PxmzGFXYq7+nb27allPoWv2+Qt9w1/jqW2Ww2vse+733lUKFpmrIsu7fz37Mqx+tdaz0Mw8L2khytddgh/4aMT3Hn3Oy8Wq/XC5vY17jEHyPfF9SABIZmp8DQ7NOgBiRQAxKoAQnUgARqQAI1IIEakEANSGBoBokLmVTJNJRSRVH4rMcYU9d1Xdezjal6jEn2N5S6rodh0FrHaRDn/LAxB4kn1WzktZCDpSKxxmzkdSwHS0j6Je4jr3MaE5JeYzbyOpaDpSKZRhxtlWXpA8HZxhzg4w8SqAEJ1IAEhmanwNDs06AGJFADEqgBCdSABGpAAjUggRqQQA1IYGgGiQuZVBia/QmGZgnA0OwIGJp9HQzNIIGPP0igBiRQAxKoAYnfD/krqs+OOPkAAAAASUVORK5CYII=",
      "text/plain": [
       "Tree(S, [Tree(SIZE_ONE, [Tree(NN, ['NN'])])])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_CYK('NN'.split(), answer_pcfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<tr><td><i>NN<i></td><td><i>NN<i></td></tr><tr><td bgcolor='lightcyan'>[0,1]<br><b>S</b> [0.00555]<br>&nbsp; (k=None, S -> SIZE_ONE [0.01])<br><b>WORDS</b> [0.00277]<br>&nbsp; (k=None, WORDS -> WORD [0.5])<br><b>WORD</b> [0.00555]<br>&nbsp; (k=None, WORD -> SIZE_ONE [0.01])<br><b>SIZE_ONE</b> [0.55450]<br>&nbsp; (k=None, SIZE_ONE -> NN [0.554504])<br><b>NN</b> [1.00000]</td><td bgcolor='lightcyan'>[0,2]<br><b>SIZE_TWO</b> [0.17245]<br>&nbsp; (k=1, SIZE_TWO -> NN NN [0.172451])<br><b>WORDS</b> [0.00001]<br>&nbsp; (k=1, WORDS -> WORD WORDS [0.5])</td></tr><tr><td></td><td bgcolor='lightcyan'>[1,2]<br><b>S</b> [0.00555]<br>&nbsp; (k=None, S -> SIZE_ONE [0.01])<br><b>WORDS</b> [0.00277]<br>&nbsp; (k=None, WORDS -> WORD [0.5])<br><b>WORD</b> [0.00555]<br>&nbsp; (k=None, WORD -> SIZE_ONE [0.01])<br><b>SIZE_ONE</b> [0.55450]<br>&nbsp; (k=None, SIZE_ONE -> NN [0.554504])<br><b>NN</b> [1.00000]</td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-293-f0d2b63ba82f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparse_CYK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_pcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-288-bebda6bc4bdc>\u001b[0m in \u001b[0;36mparse_CYK\u001b[0;34m(words, grammar)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# have to build the tree from the back pointers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-291-fa0d934369b5>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(words, back, symbol, i, j)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackpointer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# X -> 'word' production\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parse_CYK(['NN','NN'], answer_pcfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE_THREE -> NNS_DT NN [0.000643087]\n"
     ]
    }
   ],
   "source": [
    "prods = answer_pcfg.productions()\n",
    "print prods[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print answer_pcfg.productions(lhs=Nonterminal('SIZE_THREE'), rhs = \"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
