{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the python script containing the same code as the load the data notebook\n",
    "%run loadData.py\n",
    "# now we can access train, dev, and test\n",
    "# along with trainSents, devSents testSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from string import punctuation  \n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import csv\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)\n",
    "\n",
    "punct_tokens = set(punctuation)\n",
    "extra_tokens = set([\"what\", \"where\", \"how\", \"when\", \"who\"])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filter_tokens = extra_tokens.union(punct_tokens).union(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_answers = []\n",
    "for i in range(0, len(train)):\n",
    "    qs = train[i]\n",
    "    for j in range(0, len(qs)):\n",
    "        q = qs[j]\n",
    "        answer = q[\"answer\"]\n",
    "        train_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    elif (datasetName == 'ans_train'):\n",
    "        theFilePath = './preCompTags/stanfordTaggedTrainAnswers.txt'\n",
    "        theSents = train_answers\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_train_set = getStanfordTagging('train')\n",
    "tagged_dev_set = getStanfordTagging('dev')\n",
    "tagged_test_set = getStanfordTagging('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "# Thanks for this list to save me typing it : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "punctuation = [\"''\",'``','(','.',':', ',',')']\n",
    "\n",
    "\n",
    "months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "\n",
    "def isPunctuation(word):\n",
    "    return word in punctuation\n",
    "\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Obtained from training data\n",
    "postUnits = [u'%', u'century', u'years', u'percent', u'years ago', u'days', u'months', u'km', u'hours', u'times', u'inches', u'\\xb0C', u'minutes', u'acres', u'\\xb0F', u'weeks', u'people', u'sq mi', u'mi', u'ft', u'feet', u'metres', u'mm', u'square miles', u'miles', u'pm', u'per cent', u'year', u'copies', u'yuan', u'men', u'square feet', u'third', u'kilometres', u'nm', u'tonnes', u'species', u'decades', u'barrels', u'tons', u'largest', u'centuries', u'km2']\n",
    "preUnits = [u'$',u'around', u'late', u'early', u'nearly', u'since', u'approximately', u'number']\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    if word in months:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word.lower() in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = \"\"\" ANS: {<JJ>?<N.*>*}\n",
    "                   {<DT>?<N.*>*}\n",
    "                   }<UH|POS|VB|VBG|RP|DT|MD|PRP$|TO|RB|JJS|PDT|IN|PRP|VBP|VBN|RBS|WRB|WP|EX|VBZ|WDT|VBD>{\n",
    "                    \"\"\"\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "\n",
    "def chunk(words):\n",
    "    tokenWS = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    chunks =  cp.parse(tokenWS)\n",
    "    possAnswers = []\n",
    "    for subtree in chunks.subtrees():\n",
    "        if subtree.label() == 'ANS':\n",
    "            possAnswers.append((' '.join(word for word, pos in subtree.leaves()),'O'))\n",
    "    possAnswers.append((\"Nope\", \"CRAP\")) # To ensure nothing has 0 tags\n",
    "    return possAnswers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_non_chunked_words_as_single_tags(words):\n",
    "    chunked_output = chunk(words)\n",
    "    token_words = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    if len(chunked_output) == 0:\n",
    "        chunked_words = [\"DEREKWANG\"]\n",
    "    else:\n",
    "        chunked_words = [nltk.word_tokenize(word_tag_pair[0]) for word_tag_pair in chunked_output ]    \n",
    "    all_word_tags = []\n",
    "    \n",
    "    current_chunk_index = 0\n",
    "    current_chunk_word = 0\n",
    "    current_chunk_list = chunked_words[0]\n",
    "    \n",
    "    for word_tag_pair in token_words:\n",
    "        word = word_tag_pair[0]\n",
    "        if word == current_chunk_list[current_chunk_word]:\n",
    "            # Need to move onto next word\n",
    "            if current_chunk_word == len(current_chunk_list) - 1:\n",
    "                # last word in this current chunk\n",
    "                all_word_tags.append(chunked_output[current_chunk_index])\n",
    "                current_chunk_index += 1\n",
    "                current_chunk_word = 0\n",
    "                if current_chunk_index == len(chunked_words):\n",
    "                    current_chunk_list = [\"NOPE\"]\n",
    "                else:\n",
    "                    current_chunk_list = chunked_words[current_chunk_index]\n",
    "            else :\n",
    "                current_chunk_word += 1\n",
    "        else :\n",
    "            # Need to add word, as it's not in a chunk :(\n",
    "            all_word_tags.append((word,'O'))\n",
    "    return all_word_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def refine_word_tags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif word in preUnits:\n",
    "                tag = 'PRENUM'\n",
    "            elif word in postUnits:\n",
    "                tag = 'POSTNUM'\n",
    "            elif isCapitalised(word):\n",
    "                tag = \"OTHERCAP\"\n",
    "        newWordTags.append((word, tag))\n",
    "    \n",
    "    newWordTags = combineTags (newWordTags)\n",
    "    other_processed_tags = process_others(newWordTags)\n",
    "    return other_processed_tags\n",
    "        \n",
    "def combineTags(wordTags):\n",
    "    \n",
    "    newTags = []\n",
    "    prevWord = wordTags[0][0]\n",
    "    prevTag = wordTags[0][1]\n",
    "    \n",
    "    for (word, tag) in wordTags[1:]:\n",
    "        if tag == 'NUMBER' and prevTag == 'PRENUM':\n",
    "            prevTag = 'NUMBER'\n",
    "        elif prevTag == 'PRENUM':\n",
    "            prevTag = 'O'\n",
    "        if tag == 'POSTNUM' and prevTag == \"NUMBER\":\n",
    "            tag = \"NUMBER\"\n",
    "        elif tag == \"POSTNUM\":\n",
    "            tag = \"O\"\n",
    "        newTags.append((prevWord, prevTag))\n",
    "        prevWord = word\n",
    "        prevTag = tag\n",
    "    newTags.append((prevWord, prevTag))\n",
    "        \n",
    "    newNewTags = []\n",
    "    prevWord = newTags[0][0]\n",
    "    prevTag = newTags[0][1]\n",
    "    if (prevTag == \"OTHERCAP\" and newTags[1][1] != \"OTHERCAP\"):\n",
    "        prevTag = \"O\"\n",
    "        \n",
    "    for (word, tag) in newTags[1:]:\n",
    "#         print tag, prevTag\n",
    "        if tag == prevTag :\n",
    "            if word == '%':\n",
    "                prevWord += word\n",
    "            else :\n",
    "                if prevWord == '$':\n",
    "                    prevWord += word\n",
    "                else :\n",
    "                    prevWord += ' ' + word\n",
    "        else :\n",
    "            newNewTags.append((prevWord, prevTag))\n",
    "            prevWord = word\n",
    "            prevTag = tag\n",
    "            \n",
    "    newNewTags.append((prevWord, prevTag))\n",
    "    \n",
    "    return newNewTags\n",
    "\n",
    "def process_others(words_with_tags):\n",
    "    new_taggings = []\n",
    "    for (words, tag) in words_with_tags:\n",
    "        if tag == 'O':\n",
    "            #chunk_results = add_non_chunked_words_as_single_tags(words)\n",
    "            #for (word,tag) in chunk_results:\n",
    "            #    new_taggings.append((word, tag))\n",
    "            new_taggings.append((words,tag))\n",
    "        else :\n",
    "            new_taggings.append((words, tag))\n",
    "    return new_taggings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluateNERonDev():\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    for i in range(0, len(dev)):\n",
    "        qs = dev[i]\n",
    "        for j in range(0, len(qs)):\n",
    "            q = qs[j]\n",
    "            idSent  = q[\"answer_sentence\"]\n",
    "            sent = devSents[i][idSent]\n",
    "            answer = q[\"answer\"]\n",
    "            possAnswers = refine_word_tags(tagged_dev_set[i][idSent])\n",
    "            inThere = False\n",
    "            for possAnswer in possAnswers:\n",
    "                if possAnswer[0] == answer:\n",
    "                    inThere = True\n",
    "                    break\n",
    "            if inThere:\n",
    "                correct.append((i, j, idSent, possAnswers))\n",
    "            else :\n",
    "                wrong.append((i, j, idSent, possAnswers))\n",
    "    return (correct, wrong)\n",
    "\n",
    "\n",
    "(correct,wrong) = evaluateNERonDev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.376344086022\n"
     ]
    }
   ],
   "source": [
    "print (len(correct) + 0.0)/ (len(correct) + len(wrong))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with others together, and spliting out other cap - 0.376344086022\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get answers with tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def getAnswerDict(qss):\n",
    "    sentDicts = defaultdict(list)\n",
    "    for docID in range(0, len(qss)):\n",
    "        qs = qss[docID]\n",
    "        for q in qs:\n",
    "            answer = q[\"answer\"]\n",
    "            answerSent = (docID, q[\"answer_sentence\"])\n",
    "            sentDicts[answerSent].append(answer)\n",
    "    return sentDicts\n",
    "train_sentence_contained_answers = getAnswerDict(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'long playing', u'12\", 10\", 7\"', u'rpm']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentence_contained_answers[(0,2)] # contains the answers in that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAnswerIndex(sent, answer):\n",
    "    tokenised_answer = nltk.word_tokenize(answer)\n",
    "    len_tokenised_answer = len(tokenised_answer)\n",
    "    tokenised_sent =  nltk.word_tokenize(sent)\n",
    "    highest_index = (len(tokenised_sent) - len_tokenised_answer) + 1\n",
    "    \n",
    "    for i in range (0, highest_index):\n",
    "            sentence_fragment = tokenised_sent[i:i+len_tokenised_answer]\n",
    "            if (sentence_fragment == tokenised_answer):\n",
    "                return (i, i+len_tokenised_answer)\n",
    "    #print \"Problem, cannot find answer index\"\n",
    "    #print sent\n",
    "    #print answer\n",
    "    #print\n",
    "    return (-1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gramophone record (phonograph record in American English) or vinyl record, commonly known as a \"record\", is an analogue sound storage medium in the form of a flat polyvinyl chloride (previously shellac) disc with an inscribed, modulated spiral groove.\n",
      "\n",
      "[u'analogue sound storage medium']\n",
      "\n",
      "(24, 28)\n"
     ]
    }
   ],
   "source": [
    "the_sentence = trainSents[0][0]\n",
    "the_answers_contained = train_sentence_contained_answers[(0,0)]\n",
    "\n",
    "print the_sentence\n",
    "print\n",
    "print the_answers_contained\n",
    "print\n",
    "print getAnswerIndex (the_sentence, the_answers_contained[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_candidates_to_range(candidates):\n",
    "    candidatesIndexRange = []\n",
    "    index = 0\n",
    "    for (words, tag) in candidates:\n",
    "        tokenisedWords = nltk.word_tokenize(words)\n",
    "        nextIndex = index + len(tokenisedWords)\n",
    "        candidatesIndexRange.append((index,nextIndex ))\n",
    "        index = nextIndex\n",
    "    return candidatesIndexRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 7), (7, 9), (9, 49)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_candidates = refine_word_tags(tagged_train_set[0][0])\n",
    "\n",
    "convert_candidates_to_range(example_candidates)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the taggings of the (possibly overlapping) candidates list\n",
    "\n",
    "def get_taggings_in_range((ans_start,ans_end),candidates):\n",
    "    candidate_ranges = convert_candidates_to_range(candidates)\n",
    "    \n",
    "    current_considered_candidate_id = -1\n",
    "    candidates_containing_answer = []\n",
    "    \n",
    "    for (cand_start, cand_end) in candidate_ranges:\n",
    "        current_considered_candidate_id += 1\n",
    "        if ans_start == cand_start and ans_end == cand_end :\n",
    "            #candidates and answer the same\n",
    "            candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            break\n",
    "        if cand_start <= ans_start and cand_end >= ans_end :\n",
    "            #candidate contains the answer, but has extra words\n",
    "            candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            break\n",
    "\n",
    "        if cand_start >= ans_start or cand_end > ans_start:\n",
    "            if (cand_end <= ans_end):\n",
    "                candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            else :\n",
    "                # we have finished, as \n",
    "                if (cand_start < ans_end ):\n",
    "                    candidates_containing_answer.append(current_considered_candidate_id)\n",
    "                break\n",
    "    return candidates_containing_answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The groove usually starts near the periphery and ends near the center of the disc .', 'O')]\n",
      "\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "example_candidates = refine_word_tags(tagged_train_set[0][1])\n",
    "example_answer_range = (3,6)\n",
    "print example_candidates\n",
    "print\n",
    "print get_taggings_in_range(example_answer_range,example_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_NER_tag_analysis() :\n",
    "    other_taggings = []\n",
    "    for i in range(0, len(trainSents)):\n",
    "        sent_set = trainSents[i]\n",
    "        for j in range(0, len(sent_set)):\n",
    "            sent = sent_set[j]\n",
    "            answers_in_sent = train_sentence_contained_answers[(i,j)]\n",
    "            candidates = refine_word_tags(tagged_train_set[i][j])\n",
    "            for answer in answers_in_sent:\n",
    "                (ans_start, ans_end) = getAnswerIndex(sent, answer)\n",
    "                if (ans_start, ans_end) == (-1,-1) :\n",
    "                    continue\n",
    "                   \n",
    "                candidate_ids = get_taggings_in_range((ans_start, ans_end),candidates)\n",
    "                if len(candidate_ids) == 1:\n",
    "                    possible_correct_candidate = candidates[candidate_ids[0]]\n",
    "                    if possible_correct_candidate[1] == \"O\":\n",
    "                        other_taggings.append(answer)\n",
    "    return other_taggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n",
      "Correct - should be not many\n"
     ]
    }
   ],
   "source": [
    "# Ie answers that were tagged as otherh\n",
    "aswers_tagged_as_other = get_NER_tag_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate rules for Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets generate our data!\n",
    "# For each answer - tag it, and if other:\n",
    "# add to list\n",
    "\n",
    "from collections import Counter\n",
    "other_pos_tag_counter = Counter()\n",
    "\n",
    "for ans in aswers_tagged_as_other:\n",
    "    tokenised_answer = nltk.word_tokenize(ans)\n",
    "    pos_answer =  nltk.pos_tag(tokenised_answer)\n",
    "    \n",
    "    just_tags = [pair[1] for pair in pos_answer]\n",
    "    other_pos_tag_counter[tuple(just_tags)] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('NN',), 4568),\n",
       " (('NNS',), 1510),\n",
       " (('JJ', 'NN'), 1325),\n",
       " (('JJ',), 1196),\n",
       " (('NN', 'NN'), 954)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_pos_tag_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags_by_answer_length = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    tags_by_answer_length.append(Counter())\n",
    "    \n",
    "for tag_tuple in other_pos_tag_counter.keys():\n",
    "    answer_length = len(tag_tuple)\n",
    "    if answer_length == 0:\n",
    "        continue\n",
    "    answer_index = answer_length - 1\n",
    "    if answer_index >= 10:\n",
    "        continue\n",
    "    tags_by_answer_length[answer_index][tag_tuple] += other_pos_tag_counter[tag_tuple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('NN',), 4568),\n",
       " (('NNS',), 1510),\n",
       " (('JJ',), 1196),\n",
       " (('CD',), 233),\n",
       " (('VBG',), 194)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_by_answer_length[0].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('JJ', 'NN'), 1325),\n",
       " (('NN', 'NN'), 954),\n",
       " (('JJ', 'NNS'), 847),\n",
       " (('NN', 'NNS'), 620),\n",
       " (('DT', 'NN'), 448)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_by_answer_length[1].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "tag_terminal_names = defaultdict(lambda:\"TERM0\")\n",
    "\n",
    "    \n",
    "all_tag_types = set()\n",
    "for tags in other_pos_tag_counter:\n",
    "    for tag in tags:\n",
    "        all_tag_types.add(tag)\n",
    "     \n",
    "\n",
    "next_terminal_id = 0\n",
    "for tag in all_tag_types:\n",
    "    tag_terminal_name = tag\n",
    "    if not tag.isalpha():\n",
    "        tag_terminal_name = \"TERM\" + str(next_terminal_id)\n",
    "        next_termindal_id += 1\n",
    "    tag_terminal_names[tag] = tag_terminal_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERM0\n",
      "NN\n",
      "TERM0\n"
     ]
    }
   ],
   "source": [
    "print tag_terminal_names[\"$\"]\n",
    "print tag_terminal_names[\"NN\"]\n",
    "print tag_terminal_names['joshi'] # an unseen pos tag appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([1, 2])\n",
      "set([1, 2])\n"
     ]
    }
   ],
   "source": [
    "test_set = set()\n",
    "test_set.add(1)\n",
    "test_set.add(2)\n",
    "print test_set\n",
    "new_set = set()\n",
    "new_set.add(5)\n",
    "test_set.union(new_set)\n",
    "print test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def give_grammar_rules(tags,prob):\n",
    "    grammar_rules = set()\n",
    "    \n",
    "    if len(tags)== 1:\n",
    "        first_tag = tags[0]\n",
    "        first_tag_terminal = tag_terminal_names[first_tag]\n",
    "        grammar_rules.add(\"SIZE_ONE -> \" + first_tag_terminal + \" [\" + str(prob) + \"]\")\n",
    "        new_rules = get_sub_grammar_rules(tags)\n",
    "        grammar_rules = grammar_rules.union(new_rules)\n",
    "    if len(tags) == 2:\n",
    "        grammar_rules.add(\"SIZE_TWO -> \" + tag_terminal_names[tags[0]] + \" \" + tag_terminal_names[tags[1]] +\n",
    "                           \" [\" + str(prob) + \"]\")        \n",
    "        grammar_rules = grammar_rules.union(get_sub_grammar_rules(tags))\n",
    "    if len(tags) == 3:\n",
    "        grammar_rule = \"SIZE_THREE -> \" + tag_terminal_names[tags[0]] + '_'+tag_terminal_names[tags[1]] + \" \" +tag_terminal_names[tags[2]] + \" [1]\"\n",
    "        grammar_rules = grammar_rules.union(get_sub_grammar_rules(tags))\n",
    "    return grammar_rules\n",
    "        \n",
    "def get_sub_grammar_rules(tags):\n",
    "    grammar_rules = set()\n",
    "    if len(tags) == 1:\n",
    "        first_tag = tags[0]\n",
    "        first_tag_terminal = tag_terminal_names[first_tag]\n",
    "        grammar_rules.add(first_tag_terminal + ' -> ' + '\"' + first_tag + '\"' + \" [1]\")\n",
    "        #print grammar_rules\n",
    "    if len(tags) == 2:\n",
    "        grammar_rules.add(tag_terminal_names[tags[0]] + ' -> ' + '\"' + tags[0] + '\"' + \" [1]\")\n",
    "        grammar_rules.add(tag_terminal_names[tags[1]] + ' -> ' + '\"' + tags[1] + '\"' + \" [1]\")\n",
    "    if len(tags) == 3:\n",
    "        grammar_rules = grammar_rules.union(get_sub_grammar_rules(tags[2]))\n",
    "        grammar_rules.add(tag_terminal_names[tags[0]] + '_'+tag_terminal_names[tags[1]]+ \" -> \" + tag_terminal_names[tags[0]] + ' ' +  tag_terminal_names[tags[1]] + \" [1]\")\n",
    "        grammar_rules = grammar_rules.union(get_sub_grammar_rules(tags[0]))\n",
    "        grammar_rules = grammar_rules.union(get_sub_grammar_rules(tags[1]))\n",
    "    return grammar_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['SIZE_ONE -> NNP [0.5]', 'NNP -> \"NNP\" [1]'])\n",
      "set(['NNP -> \"NNP\" [1]', 'NN -> \"NN\" [1]', 'SIZE_TWO -> NNP NN [0.2]'])\n",
      "set(['TERM0 -> \"N\" [1]', 'NNP_NN -> NNP NN [1]', 'TERM0 -> \"P\" [1]', 'TERM0_TERM0 -> TERM0 TERM0 [1]'])\n"
     ]
    }
   ],
   "source": [
    "print give_grammar_rules(('NNP',),0.5)\n",
    "print give_grammar_rules(('NNP',\"NN\"), 0.2)\n",
    "print give_grammar_rules(('NNP','NN',\"NNP\"),0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-ceb3af3fbd5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags_by_answer_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mans_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprob_of_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mans_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mgrammar_rules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgive_grammar_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprob_of_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-173-2b5b60e5b3ad>\u001b[0m in \u001b[0;36mgive_grammar_rules\u001b[0;34m(tags, prob)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         grammar_rules.add(\"SIZE_TWO -> \" + tag_terminal_names[tag[0]] + \" \" + tag_terminal_names[tag[1]] +\n\u001b[0;32m---> 12\u001b[0;31m                            \" [\" + str(prob) + \"]\")        \n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mgrammar_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrammar_rules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sub_grammar_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "total_sizes = [0,0,0,0,0]\n",
    "for i in range(0, 3):\n",
    "      total_sizes[i]= sum (tags_by_answer_length[i].values())\n",
    "\n",
    "total_ans = sum(total_sizes)\n",
    "grammar_rules = []\n",
    "for ans_length in range(0, 3):\n",
    "    if ans_length == 0 :      \n",
    "        for (tag, val) in tags_by_answer_length[0].most_common(1000000): # Ie all\n",
    "            prob_of_tag = (val + 0.0) / total_sizes[ans_length]\n",
    "            grammar_rules.append( give_grammar_rules(tag,prob_of_tag))\n",
    "    else :   \n",
    "        for (tags, val) in tags_by_answer_length[ans_length].most_common(10):\n",
    "            prob_of_tag = (val + 0.0) / total_sizes[ans_length]\n",
    "            grammar_rules.append(give_grammar_rules(tags,prob_of_tag))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SIZE_ONE -> NN [0.554503520272]', 'NN -> \"NN\" [1]'], ['SIZE_ONE -> NNS [0.183296916727]', 'NNS -> \"NNS\" [1]'], ['SIZE_ONE -> JJ [0.145180869143]', 'JJ -> \"JJ\" [1]'], ['SIZE_ONE -> CD [0.0282835639718]', 'CD -> \"CD\" [1]']]\n"
     ]
    }
   ],
   "source": [
    "print grammar_rules[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Okay, so the above gives some rules to start!\n",
    "rules_in_string = \"S -> SIZE_ONE [0.6]\\nS -> SIZE_TWO [0.2]\\nS -> SIZE_THREE [0.2]\\n\"\n",
    "for rules in grammar_rules:\n",
    "    for rule in rules:\n",
    "        rules_in_string += rule + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Productions for SIZE_TWO do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-e235c9acdf48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manswer_pcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules_in_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/alex/anaconda2/lib/python2.7/site-packages/nltk/grammar.pyc\u001b[0m in \u001b[0;36mfromstring\u001b[0;34m(cls, input, encoding)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         start, productions = read_grammar(input, standard_nonterm_parser,\n\u001b[1;32m   1097\u001b[0m                                           probabilistic=True, encoding=encoding)\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPCFG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproductions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alex/anaconda2/lib/python2.7/site-packages/nltk/grammar.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, start, productions, calculate_leftcorners)\u001b[0m\n\u001b[1;32m   1082\u001b[0m             if not ((1-PCFG.EPSILON) < p <\n\u001b[1;32m   1083\u001b[0m                     (1+PCFG.EPSILON)):\n\u001b[0;32m-> 1084\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Productions for %r do not sum to 1\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Productions for SIZE_TWO do not sum to 1"
     ]
    }
   ],
   "source": [
    "answer_pcfg = nltk.grammar.PCFG.fromstring(rules_in_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
