{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the python script containing the same code as the load the data notebook\n",
    "%run loadData.py\n",
    "# now we can access train, dev, and test\n",
    "# along with trainSents, devSents testSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from string import punctuation  \n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import csv\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)\n",
    "\n",
    "punct_tokens = set(punctuation)\n",
    "extra_tokens = set([\"what\", \"where\", \"how\", \"when\", \"who\"])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filter_tokens = extra_tokens.union(punct_tokens).union(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_train_set = getStanfordTagging('train')\n",
    "tagged_dev_set = getStanfordTagging('dev')\n",
    "tagged_test_set = getStanfordTagging('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "# Thanks for this list to save me typing it : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "punctuation = [\"''\",'``','(','.',':', ',',')']\n",
    "\n",
    "\n",
    "months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "\n",
    "def isPunctuation(word):\n",
    "    return word in punctuation\n",
    "\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Obtained from training data\n",
    "postUnits = [u'%', u'century', u'years', u'percent', u'years ago', u'days', u'months', u'km', u'hours', u'times', u'inches', u'\\xb0C', u'minutes', u'acres', u'\\xb0F', u'weeks', u'people', u'sq mi', u'mi', u'ft', u'feet', u'metres', u'mm', u'square miles', u'miles', u'pm', u'per cent', u'year', u'copies', u'yuan', u'men', u'square feet', u'third', u'kilometres', u'nm', u'tonnes', u'species', u'decades', u'barrels', u'tons', u'largest', u'centuries', u'km2']\n",
    "preUnits = [u'$',u'around', u'late', u'early', u'nearly', u'since', u'approximately', u'number']\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    if word in months:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word.lower() in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grammar = \"\"\" ANS: {<JJ>?<N.*>*}\n",
    "                   {<DT>?<N.*>*}\n",
    "                   }<UH|POS|VB|VBG|RP|DT|MD|PRP$|TO|RB|JJS|PDT|IN|PRP|VBP|VBN|RBS|WRB|WP|EX|VBZ|WDT|VBD>{\n",
    "                    \"\"\"\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "\n",
    "def chunk(words):\n",
    "    tokenWS = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    chunks =  cp.parse(tokenWS)\n",
    "    possAnswers = []\n",
    "    for subtree in chunks.subtrees():\n",
    "        if subtree.label() == 'ANS':\n",
    "            possAnswers.append((' '.join(word for word, pos in subtree.leaves()),'O'))\n",
    "    possAnswers.append((\"Nope\", \"CRAP\")) # To ensure nothing has 0 tags\n",
    "    return possAnswers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('it', 'PRP')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_non_chunked_words_as_single_tags(words):\n",
    "    chunked_output = chunk(words)\n",
    "    token_words = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    if len(chunked_output) == 0:\n",
    "        chunked_words = [\"DEREKWANG\"]\n",
    "    else:\n",
    "        chunked_words = [nltk.word_tokenize(word_tag_pair[0]) for word_tag_pair in chunked_output ]    \n",
    "    all_word_tags = []\n",
    "    \n",
    "    current_chunk_index = 0\n",
    "    current_chunk_word = 0\n",
    "    current_chunk_list = chunked_words[0]\n",
    "    \n",
    "    for word_tag_pair in token_words:\n",
    "        word = word_tag_pair[0]\n",
    "        if word == current_chunk_list[current_chunk_word]:\n",
    "            # Need to move onto next word\n",
    "            if current_chunk_word == len(current_chunk_list) - 1:\n",
    "                # last word in this current chunk\n",
    "                all_word_tags.append(chunked_output[current_chunk_index])\n",
    "                current_chunk_index += 1\n",
    "                current_chunk_word = 0\n",
    "                if current_chunk_index == len(chunked_words):\n",
    "                    current_chunk_list = [\"NOPE\"]\n",
    "                else:\n",
    "                    current_chunk_list = chunked_words[current_chunk_index]\n",
    "            else :\n",
    "                current_chunk_word += 1\n",
    "        else :\n",
    "            # Need to add word, as it's not in a chunk :(\n",
    "            all_word_tags.append((word,'O'))\n",
    "    return all_word_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def refine_word_tags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif word in preUnits:\n",
    "                tag = 'PRENUM'\n",
    "            elif isPunctuation(word):\n",
    "                tag = 'PUNC'\n",
    "            elif word in postUnits:\n",
    "                tag = 'POSTNUM'\n",
    "            elif isCapitalised(word):\n",
    "                tag = \"OTHERCAP\"\n",
    "        newWordTags.append((word, tag))\n",
    "    \n",
    "    newWordTags = combineTags (newWordTags)\n",
    "    other_processed_tags = process_others(newWordTags)\n",
    "    return other_processed_tags\n",
    "        \n",
    "def combineTags(wordTags):\n",
    "    \n",
    "    newTags = []\n",
    "    prevWord = wordTags[0][0]\n",
    "    prevTag = wordTags[0][1]\n",
    "    \n",
    "    for (word, tag) in wordTags[1:]:\n",
    "        if tag == 'NUMBER' and prevTag == 'PRENUM':\n",
    "            prevTag = 'NUMBER'\n",
    "        elif prevTag == 'PRENUM':\n",
    "            prevTag = 'O'\n",
    "        if tag == 'POSTNUM' and prevTag == \"NUMBER\":\n",
    "            tag = \"NUMBER\"\n",
    "        elif tag == \"POSTNUM\":\n",
    "            tag = \"O\"\n",
    "        newTags.append((prevWord, prevTag))\n",
    "        prevWord = word\n",
    "        prevTag = tag\n",
    "    newTags.append((prevWord, prevTag))\n",
    "        \n",
    "    newNewTags = []\n",
    "    prevWord = newTags[0][0]\n",
    "    prevTag = newTags[0][1]\n",
    "    if (prevTag == \"OTHERCAP\" and newTags[1][1] != \"OTHERCAP\"):\n",
    "        prevTag = \"O\"\n",
    "        \n",
    "    for (word, tag) in newTags[1:]:\n",
    "#         print tag, prevTag\n",
    "        if tag == prevTag :\n",
    "            if word == '%':\n",
    "                prevWord += word\n",
    "            else :\n",
    "                if prevWord == '$':\n",
    "                    prevWord += word\n",
    "                else :\n",
    "                    prevWord += ' ' + word\n",
    "        else :\n",
    "            newNewTags.append((prevWord, prevTag))\n",
    "            prevWord = word\n",
    "            prevTag = tag\n",
    "            \n",
    "    newNewTags.append((prevWord, prevTag))\n",
    "    \n",
    "    return newNewTags\n",
    "\n",
    "def process_others(words_with_tags):\n",
    "    new_taggings = []\n",
    "    for (words, tag) in words_with_tags:\n",
    "        if tag == 'O':\n",
    "            chunk_results = add_non_chunked_words_as_single_tags(words)\n",
    "            for (word,tag) in chunk_results:\n",
    "                new_taggings.append((word, tag))\n",
    "            #new_taggings.append((words,tag))\n",
    "        else :\n",
    "            new_taggings.append((words, tag))\n",
    "    return new_taggings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def getAnswerDict(qss):\n",
    "    sentDicts = defaultdict(list)\n",
    "    for docID in range(0, len(qss)):\n",
    "        qs = qss[docID]\n",
    "        for q in qs:\n",
    "            answer = q[\"answer\"]\n",
    "            answerSent = (docID, q[\"answer_sentence\"])\n",
    "            sentDicts[answerSent].append(answer)\n",
    "    return sentDicts\n",
    "train_sentence_contained_answers = getAnswerDict(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'long playing', u'12\", 10\", 7\"', u'rpm']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentence_contained_answers[(0,2)] # contains the answers in that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAnswerIndex(sent, answer):\n",
    "    tokenised_answer = nltk.word_tokenize(answer)\n",
    "    len_tokenised_answer = len(tokenised_answer)\n",
    "    tokenised_sent =  nltk.word_tokenize(sent)\n",
    "    highest_index = (len(tokenised_sent) - len_tokenised_answer) + 1\n",
    "    \n",
    "    for i in range (0, highest_index):\n",
    "            sentence_fragment = tokenised_sent[i:i+len_tokenised_answer]\n",
    "            if (sentence_fragment == tokenised_answer):\n",
    "                return (i, i+len_tokenised_answer)\n",
    "    #print \"Problem, cannot find answer index\"\n",
    "    #print sent\n",
    "    #print answer\n",
    "    #print\n",
    "    return (-1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gramophone record (phonograph record in American English) or vinyl record, commonly known as a \"record\", is an analogue sound storage medium in the form of a flat polyvinyl chloride (previously shellac) disc with an inscribed, modulated spiral groove.\n",
      "\n",
      "[u'analogue sound storage medium']\n",
      "\n",
      "(24, 28)\n"
     ]
    }
   ],
   "source": [
    "the_sentence = trainSents[0][0]\n",
    "the_answers_contained = train_sentence_contained_answers[(0,0)]\n",
    "\n",
    "print the_sentence\n",
    "print\n",
    "print the_answers_contained\n",
    "print\n",
    "print getAnswerIndex (the_sentence, the_answers_contained[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_candidates_to_range(candidates):\n",
    "    candidatesIndexRange = []\n",
    "    index = 0\n",
    "    for (words, tag) in candidates:\n",
    "        tokenisedWords = nltk.word_tokenize(words)\n",
    "        nextIndex = index + len(tokenisedWords)\n",
    "        candidatesIndexRange.append((index,nextIndex ))\n",
    "        index = nextIndex\n",
    "    return candidatesIndexRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 3), (3, 4), (4, 6), (6, 7)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_candidates = refine_word_tags(tagged_train_set[0][0])\n",
    "\n",
    "convert_candidates_to_range(example_candidates)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the taggings of the (possibly overlapping) candidates list\n",
    "\n",
    "def get_taggings_in_range((ans_start,ans_end),candidates):\n",
    "    candidate_ranges = convert_candidates_to_range(candidates)\n",
    "    \n",
    "    current_considered_candidate_id = -1\n",
    "    candidates_containing_answer = []\n",
    "    \n",
    "    for (cand_start, cand_end) in candidate_ranges:\n",
    "        current_considered_candidate_id += 1\n",
    "        if ans_start == cand_start and ans_end == cand_end :\n",
    "            #candidates and answer the same\n",
    "            candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            break\n",
    "        if cand_start <= ans_start and cand_end >= ans_end :\n",
    "            #candidate contains the answer, but has extra words\n",
    "            candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            break\n",
    "\n",
    "        if cand_start >= ans_start or cand_end > ans_start:\n",
    "            if (cand_end <= ans_end):\n",
    "                candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            else :\n",
    "                # we have finished, as \n",
    "                if (cand_start < ans_end ):\n",
    "                    candidates_containing_answer.append(current_considered_candidate_id)\n",
    "                break\n",
    "    return candidates_containing_answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', 'O'), (u'groove', 'O'), (u'usually', 'O'), (u'starts', 'O'), (u'near', 'O'), (u'the', 'O'), (u'periphery', 'O'), (u'and', 'O'), (u'ends', 'O'), (u'near', 'O'), (u'the', 'O'), (u'center', 'O'), (u'of', 'O'), (u'the', 'O'), (u'disc', 'O'), (u'.', 'PUNC')]\n",
      "\n",
      "[3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "example_candidates = refine_word_tags(tagged_train_set[0][1])\n",
    "example_answer_range = (3,6)\n",
    "print example_candidates\n",
    "print\n",
    "print get_taggings_in_range(example_answer_range,example_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_NER_tag_analysis() :\n",
    "    correct_taggings = []\n",
    "    incorrect_taggings = []\n",
    "    evil_ones = []\n",
    "    for i in range(0, len(trainSents)):\n",
    "        sent_set = trainSents[i]\n",
    "        for j in range(0, len(sent_set)):\n",
    "            sent = sent_set[j]\n",
    "            answers_in_sent = train_sentence_contained_answers[(i,j)]\n",
    "            candidates = refine_word_tags(tagged_train_set[i][j])\n",
    "            for answer in answers_in_sent:\n",
    "                (ans_start, ans_end) = getAnswerIndex(sent, answer)\n",
    "                if (ans_start, ans_end) == (-1,-1) :\n",
    "                    #question = train[i][j][\"question\"]\n",
    "                    evil_ones.append((i,j, answer))\n",
    "                    continue\n",
    "                   \n",
    "                candidate_ids = get_taggings_in_range((ans_start, ans_end),candidates)\n",
    "                if len(candidate_ids) == 1:\n",
    "                    possible_correct_candidate = candidates[candidate_ids[0]]\n",
    "                    if possible_correct_candidate[0] == answer :\n",
    "                        # Correct tagging!\n",
    "                        correct_taggings.append((possible_correct_candidate, i, j))\n",
    "                    else :\n",
    "                        incorrect_taggings.append(([possible_correct_candidate], answer, i, j))\n",
    "                else :\n",
    "                    incorrect_candidates = ([candidates[index] for index in candidate_ids],answer, i, j)\n",
    "                    incorrect_taggings.append(incorrect_candidates)\n",
    "\n",
    "    return (correct_taggings, incorrect_taggings, evil_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    }
   ],
   "source": [
    "(correct_taggings, incorrect_taggings, evil_ones) = get_NER_tag_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1051\n"
     ]
    }
   ],
   "source": [
    "print len(evil_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33534\n",
      "35574\n"
     ]
    }
   ],
   "source": [
    "print len(correct_taggings)\n",
    "print len(incorrect_taggings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "correctCounter = Counter()\n",
    "for correct_tags in correct_taggings:\n",
    "    correctCounter[correct_tags[0][1]] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({u'LOCATION': 3503,\n",
       "         'NUMBER': 9459,\n",
       "         'O': 9554,\n",
       "         'OTHERCAP': 6460,\n",
       "         u'PERSON': 4558})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "wrongCounter = Counter()\n",
    "wrongList = defaultdict(list)\n",
    "\n",
    "for wrong_tags in incorrect_taggings:\n",
    "    tagList = []\n",
    "    for (words, tag) in wrong_tags[0]:\n",
    "        tagList.append(tag)\n",
    "    wrongCounter[tuple(tagList)] += 1\n",
    "    wrongList[tuple(tagList)].append(wrong_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('O',), 2875),\n",
       " (('O', 'O'), 2805),\n",
       " (('O', 'O', 'O'), 2406),\n",
       " (('NUMBER',), 1298),\n",
       " (('O', 'O', 'O', 'O'), 1223),\n",
       " (('OTHERCAP', 'O', 'OTHERCAP'), 1090),\n",
       " (('OTHERCAP', 'O'), 1074),\n",
       " (('O', 'OTHERCAP'), 939),\n",
       " (('O', 'O', 'O', 'O', 'O'), 770),\n",
       " (('OTHERCAP',), 768)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongCounter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(u'mirror-like finish', 'O')], u'mirror-like', 0, 285)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongList[('O',)][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "('NN',)\n",
      "('JJ',)\n",
      "('NNP', 'NNP')\n",
      "('NNP',)\n",
      "('CD',)\n",
      "('NN', 'NN')\n",
      "('NNS',)\n",
      "('VBN',)\n",
      "('NNP', 'NNP', 'NNP')\n",
      "('NNP', 'NNS')\n",
      "('NNP', 'NN')\n",
      "('VB',)\n",
      "('NNP', 'NNP', 'NN')\n",
      "('JJ', 'NN', 'NN')\n",
      "('NN', 'NN', 'NNS')\n",
      "('NNP', 'NNP', 'NNS')\n",
      "('JJ', 'NN')\n",
      "('VBG',)\n",
      "('NNS', 'VBP')\n",
      "75\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "other_answer_pos_tags = Counter()\n",
    "for item in wrongList[('O',)]:\n",
    "    answer = item[1]\n",
    "    tokenisedAnswer = nltk.word_tokenize(answer)\n",
    "    posTagedWord = nltk.pos_tag(tokenisedAnswer)\n",
    "    just_the_tags = [pair[1] for pair in posTagedWord]\n",
    "    other_answer_pos_tags[tuple(just_the_tags)] += 1\n",
    "print len(wrongList[('O',)])\n",
    "\n",
    "total_count = 0\n",
    "for (tag_tuple, count) in other_answer_pos_tags.most_common(20):\n",
    "    total_count += count\n",
    "    print tag_tuple\n",
    "print total_count\n",
    "\n",
    "print (total_count + 0.0 )/ len(wrongList[('O',)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u\"''\", \"''\"), (u')', ')'), (u',', ','), (u'the', 'DT'), (u'rotational', 'JJ'), (u'speed', 'NN'), (u'in', 'IN'), (u'rpm', 'NN'), (u'at', 'IN'), (u'which', 'WDT'), (u'they', 'PRP'), (u'are', 'VBP'), (u'played', 'VBN'), (u'(', '(')]\n",
      "(S\n",
      "  ''/''\n",
      "  )/)\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (ANS rotational/JJ speed/NN)\n",
      "  in/IN\n",
      "  (ANS rpm/NN)\n",
      "  at/IN\n",
      "  which/WDT\n",
      "  they/PRP\n",
      "  are/VBP\n",
      "  played/VBN\n",
      "  (/()\n",
      "\n",
      "[(u'rotational speed', 'O'), (u'rpm', 'O'), ('Nope', 'CRAP')]\n"
     ]
    }
   ],
   "source": [
    "test_sent = wrongList[('O',)][3][0][0][0]\n",
    "test_sent_tokenised = nltk.word_tokenize(test_sent)\n",
    "test_sent_pos_tagged = nltk.pos_tag(test_sent_tokenised)\n",
    "\n",
    "print test_sent_pos_tagged\n",
    "\n",
    "\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "result = cp.parse(test_sent_pos_tagged)\n",
    "print result\n",
    "print\n",
    "print chunk(test_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Dev sent for all:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n"
     ]
    }
   ],
   "source": [
    "def evaluateNERonDev():\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    for i in range(0, len(dev)):\n",
    "        qs = dev[i]\n",
    "        for j in range(0, len(qs)):\n",
    "            q = qs[j]\n",
    "            idSent  = q[\"answer_sentence\"]\n",
    "            sent = devSents[i][idSent]\n",
    "            answer = q[\"answer\"]\n",
    "            possAnswers = refine_word_tags(tagged_dev_set[i][idSent])\n",
    "            inThere = False\n",
    "            for possAnswer in possAnswers:\n",
    "                if possAnswer[0] == answer:\n",
    "                    inThere = True\n",
    "                    break\n",
    "            if inThere:\n",
    "                correct.append((i, j, idSent, possAnswers))\n",
    "            else :\n",
    "                wrong.append((i, j, idSent, possAnswers))\n",
    "    return (correct, wrong)\n",
    "\n",
    "\n",
    "(correct,wrong) = evaluateNERonDev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.524045846626\n"
     ]
    }
   ],
   "source": [
    "print (len(correct) + 0.0)/ (len(correct) + len(wrong))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base! 0.261491196975 ... With others in one clump, ie all grouped\n",
    "\n",
    "# 0.412146992792     With others seperated into single units\n",
    "\n",
    "# Add months: 0.416046319272 Every little helps\n",
    "\n",
    "# POS with chunker made from top 10 rules :D\n",
    "# 0.458584426326\n",
    "\n",
    "# including others as singles:\n",
    "# 0.488597424081\n",
    "\n",
    "# slightly longer and more complecated grammar \n",
    "\n",
    "\n",
    "# POS tag on others\n",
    "# 900 tags.\n",
    "\n",
    "# 0.524045846626 -> Chunking with capital tag\n",
    "\n",
    "# adding stopwords tag reduces to 0.517074323526 UNDO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = \"\"\" ANS: {<NNP>*}\n",
    "                   {<JJ>?<NN>*}\n",
    "                   {<NN><NNS>}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "\n",
    "def chunk_dev(words):\n",
    "    tokenWS = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    chunks =  cp.parse(tokenWS)\n",
    "    possAnswers = []\n",
    "    for subtree in chunks.subtrees():\n",
    "        if subtree.label() == 'ANS':\n",
    "            possAnswers.append((' '.join(word for word, pos in subtree.leaves()),'O'))\n",
    "    return possAnswers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New plan, get chunk to return all the words :D\n",
    "\n",
    "\n",
    "words = \"The cat sat on the mat\"\n",
    "\n",
    "chunked_output =  chunk_mark_2(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each answer in train\n",
    "# Any tags that aren't in answers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer_tag_counter = Counter()\n",
    "tag_counter_sents = Counter()\n",
    "ans_length_counter = Counter()\n",
    "\n",
    "for i in range(0, len(train)):\n",
    "    qs = train[i]\n",
    "    for j in range(0, len(qs)):\n",
    "        q = qs[j]\n",
    "        idSent  = q[\"answer_sentence\"]\n",
    "        sent = trainSents[i][idSent]\n",
    "        answer = q[\"answer\"]\n",
    "        \n",
    "        pos_tags_answer = nltk.pos_tag(nltk.word_tokenize(answer))\n",
    "        pos_tags_sent = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "        ans_length_counter[len(pos_tags_answer)] += 1\n",
    "        \n",
    "        for (word, tag) in pos_tags_answer:\n",
    "            answer_tag_counter[tag] += 1\n",
    "            \n",
    "        for (word, tag) in pos_tags_sent:\n",
    "            tag_counter_sents[tag] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_proportion_counter = Counter()\n",
    "for tag in tag_counter.keys():\n",
    "    percentage = (answer_tag_counter[tag]  + 0.0)/ (answer_tag_counter[tag] + tag_counter_sents[tag])\n",
    "    tag_proportion_counter[tag] = percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_rare_tags = []\n",
    "for (tag, percentage) in  tag_proportion_counter.most_common(100000)[-30:]:\n",
    "    list_of_rare_tags.append(tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UH',\n",
       " 'POS',\n",
       " 'VB',\n",
       " \"''\",\n",
       " '``',\n",
       " 'VBG',\n",
       " 'RP',\n",
       " 'DT',\n",
       " 'MD',\n",
       " 'PRP$',\n",
       " 'TO',\n",
       " 'RB',\n",
       " 'JJS',\n",
       " 'PDT',\n",
       " 'IN',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'VBN',\n",
       " 'RBS',\n",
       " 'WRB',\n",
       " 'WP',\n",
       " '(',\n",
       " 'EX',\n",
       " ')',\n",
       " ',',\n",
       " 'VBZ',\n",
       " 'WDT',\n",
       " 'VBD',\n",
       " '.',\n",
       " ':']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_rare_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66104"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([val for (key,val) in ans_length_counter.most_common(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "}<,|EX|WP|WRB|VBZ|WDT|RBS|VDB|.|:>+{"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
