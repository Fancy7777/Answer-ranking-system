{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the python script containing the same code as the load the data notebook\n",
    "%run loadData.py\n",
    "# now we can access train, dev, and test\n",
    "# along with trainSents, devSents testSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from string import punctuation  \n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import csv\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)\n",
    "\n",
    "punct_tokens = set(punctuation)\n",
    "extra_tokens = set([\"what\", \"where\", \"how\", \"when\", \"who\"])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filter_tokens = extra_tokens.union(punct_tokens).union(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_train_set = getStanfordTagging('train')\n",
    "tagged_dev_set = getStanfordTagging('dev')\n",
    "tagged_test_set = getStanfordTagging('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "# Thanks for this list to save me typing it : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "punctuation = [\"''\",'``','(','.',':', ',',')']\n",
    "\n",
    "\n",
    "months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "\n",
    "def isPunctuation(word):\n",
    "    return word in punctuation\n",
    "\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Obtained from training data\n",
    "postUnits = [u'%', u'century', u'years', u'percent', u'years ago', u'days', u'months', u'km', u'hours', u'times', u'inches', u'\\xb0C', u'minutes', u'acres', u'\\xb0F', u'weeks', u'people', u'sq mi', u'mi', u'ft', u'feet', u'metres', u'mm', u'square miles', u'miles', u'pm', u'per cent', u'year', u'copies', u'yuan', u'men', u'square feet', u'third', u'kilometres', u'nm', u'tonnes', u'species', u'decades', u'barrels', u'tons', u'largest', u'centuries', u'km2']\n",
    "preUnits = [u'$',u'around', u'late', u'early', u'nearly', u'since', u'approximately', u'number']\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    if word in months:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word.lower() in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grammar = \"\"\" ANS: {<NNP>*}\n",
    "                   {<JJ>?<NN>*}\n",
    "                   {<NN><NNS>}\n",
    "\"\"\"\n",
    "chink = \"}<,|EX|WP|WRB|VBZ|WDT|RBS|VDB|.|:>+{\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grammar = \"\"\" ANS: {<JJ>?<N.*>*}\n",
    "                   {<DT>?<N.*>*}\n",
    "                   }<UH|POS|VB|VBG|RP|DT|MD|PRP$|TO|RB|JJS|PDT|IN|PRP|VBP|VBN|RBS|WRB|WP|EX|VBZ|WDT|VBD>{\n",
    "                    \"\"\"\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "\n",
    "def chunk(words):\n",
    "    tokenWS = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    chunks =  cp.parse(tokenWS)\n",
    "    possAnswers = []\n",
    "    for subtree in chunks.subtrees():\n",
    "        if subtree.label() == 'ANS':\n",
    "            possAnswers.append((' '.join(word for word, pos in subtree.leaves()),'O'))\n",
    "    possAnswers.append((\"Nope\", \"CRAP\")) # To ensure nothing has 0 tags\n",
    "    return possAnswers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_non_chunked_words_as_single_tags(words):\n",
    "    chunked_output = chunk(words)\n",
    "    token_words = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    if len(chunked_output) == 0:\n",
    "        chunked_words = [\"DEREKWANG\"]\n",
    "    else:\n",
    "        chunked_words = [nltk.word_tokenize(word_tag_pair[0]) for word_tag_pair in chunked_output ]    \n",
    "    all_word_tags = []\n",
    "    \n",
    "    current_chunk_index = 0\n",
    "    current_chunk_word = 0\n",
    "    current_chunk_list = chunked_words[0]\n",
    "    \n",
    "    for word_tag_pair in token_words:\n",
    "        word = word_tag_pair[0]\n",
    "        if word == current_chunk_list[current_chunk_word]:\n",
    "            # Need to move onto next word\n",
    "            if current_chunk_word == len(current_chunk_list) - 1:\n",
    "                # last word in this current chunk\n",
    "                all_word_tags.append(chunked_output[current_chunk_index])\n",
    "                current_chunk_index += 1\n",
    "                current_chunk_word = 0\n",
    "                if current_chunk_index == len(chunked_words):\n",
    "                    current_chunk_list = [\"NOPE\"]\n",
    "                else:\n",
    "                    current_chunk_list = chunked_words[current_chunk_index]\n",
    "            else :\n",
    "                current_chunk_word += 1\n",
    "        else :\n",
    "            # Need to add word, as it's not in a chunk :(\n",
    "            all_word_tags.append((word,'O'))\n",
    "    return all_word_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def refine_word_tags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif word in preUnits:\n",
    "                tag = 'PRENUM'\n",
    "            elif isPunctuation(word):\n",
    "                tag = 'PUNC'\n",
    "            elif word in postUnits:\n",
    "                tag = 'POSTNUM'\n",
    "\n",
    "        newWordTags.append((word, tag))\n",
    "    \n",
    "    newWordTags = combineTags (newWordTags)\n",
    "    other_processed_tags = process_others(newWordTags)\n",
    "    return other_processed_tags\n",
    "        \n",
    "def combineTags(wordTags):\n",
    "    \n",
    "    newTags = []\n",
    "    prevWord = wordTags[0][0]\n",
    "    prevTag = wordTags[0][1]\n",
    "    \n",
    "    for (word, tag) in wordTags[1:]:\n",
    "        if tag == 'NUMBER' and prevTag == 'PRENUM':\n",
    "            prevTag = 'NUMBER'\n",
    "        elif prevTag == 'PRENUM':\n",
    "            prevTag = 'O'\n",
    "        if tag == 'POSTNUM' and prevTag == \"NUMBER\":\n",
    "            tag = \"NUMBER\"\n",
    "        elif tag == \"POSTNUM\":\n",
    "            tag = \"O\"\n",
    "        newTags.append((prevWord, prevTag))\n",
    "        prevWord = word\n",
    "        prevTag = tag\n",
    "    newTags.append((prevWord, prevTag))\n",
    "        \n",
    "    newNewTags = []\n",
    "    prevWord = newTags[0][0]\n",
    "    prevTag = newTags[0][1]\n",
    "    if (prevTag == \"OTHERCAP\"):\n",
    "        prevTag = \"O\"\n",
    "        \n",
    "    for (word, tag) in newTags[1:]:\n",
    "#         print tag, prevTag\n",
    "        if tag == prevTag :\n",
    "            if word == '%':\n",
    "                prevWord += word\n",
    "            else :\n",
    "                if prevWord == '$':\n",
    "                    prevWord += word\n",
    "                else :\n",
    "                    prevWord += ' ' + word\n",
    "        else :\n",
    "            newNewTags.append((prevWord, prevTag))\n",
    "            prevWord = word\n",
    "            prevTag = tag\n",
    "            \n",
    "    newNewTags.append((prevWord, prevTag))\n",
    "    \n",
    "    return newNewTags\n",
    "\n",
    "def process_others(words_with_tags):\n",
    "    new_taggings = []\n",
    "    for (words, tag) in words_with_tags:\n",
    "        if tag == 'O':\n",
    "            chunk_results = add_non_chunked_words_as_single_tags(words)\n",
    "            for (word,tag) in chunk_results:\n",
    "                new_taggings.append((word, tag))\n",
    "            #new_taggings.append((words,tag))\n",
    "        else :\n",
    "            new_taggings.append((words, tag))\n",
    "    return new_taggings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def getAnswerDict(qss):\n",
    "    sentDicts = defaultdict(list)\n",
    "    for docID in range(0, len(qss)):\n",
    "        qs = qss[docID]\n",
    "        for q in qs:\n",
    "            answer = q[\"answer\"]\n",
    "            answerSent = (docID, q[\"answer_sentence\"])\n",
    "            sentDicts[answerSent].append(answer)\n",
    "    return sentDicts\n",
    "train_sentence_contained_answers = getAnswerDict(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'long playing', u'12\", 10\", 7\"', u'rpm']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentence_contained_answers[(0,2)] # contains the answers in that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAnswerIndex(sent, answer):\n",
    "    tokenised_answer = nltk.word_tokenize(answer)\n",
    "    len_tokenised_answer = len(tokenised_answer)\n",
    "    tokenised_sent =  nltk.word_tokenize(sent)\n",
    "    highest_index = (len(tokenised_sent) - len_tokenised_answer) + 1\n",
    "    \n",
    "    for i in range (0, highest_index):\n",
    "            sentence_fragment = tokenised_sent[i:i+len_tokenised_answer]\n",
    "            if (sentence_fragment == tokenised_answer):\n",
    "                return (i, i+len_tokenised_answer)\n",
    "    #print \"Problem, cannot find answer index\"\n",
    "    #print sent\n",
    "    #print answer\n",
    "    #print\n",
    "    return (-1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gramophone record (phonograph record in American English) or vinyl record, commonly known as a \"record\", is an analogue sound storage medium in the form of a flat polyvinyl chloride (previously shellac) disc with an inscribed, modulated spiral groove.\n",
      "\n",
      "[u'analogue sound storage medium']\n",
      "\n",
      "(24, 28)\n"
     ]
    }
   ],
   "source": [
    "the_sentence = trainSents[0][0]\n",
    "the_answers_contained = train_sentence_contained_answers[(0,0)]\n",
    "\n",
    "print the_sentence\n",
    "print\n",
    "print the_answers_contained\n",
    "print\n",
    "print getAnswerIndex (the_sentence, the_answers_contained[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_candidates_to_range(candidates):\n",
    "    candidatesIndexRange = []\n",
    "    index = 0\n",
    "    for (words, tag) in candidates:\n",
    "        tokenisedWords = nltk.word_tokenize(words)\n",
    "        nextIndex = index + len(tokenisedWords)\n",
    "        candidatesIndexRange.append((index,nextIndex ))\n",
    "        index = nextIndex\n",
    "    return candidatesIndexRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6), (6, 7), (7, 9), (9, 10), (10, 11)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_candidates = refine_word_tags(tagged_train_set[0][0])\n",
    "\n",
    "convert_candidates_to_range(example_candidates)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the taggings of the (possibly overlapping) candidates list\n",
    "\n",
    "def get_taggings_in_range((ans_start,ans_end),candidates):\n",
    "    candidate_ranges = convert_candidates_to_range(candidates)\n",
    "    \n",
    "    current_considered_candidate_id = -1\n",
    "    candidates_containing_answer = []\n",
    "    \n",
    "    for (cand_start, cand_end) in candidate_ranges:\n",
    "        current_considered_candidate_id += 1\n",
    "        if ans_start == cand_start and ans_end == cand_end :\n",
    "            #candidates and answer the same\n",
    "            candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            break\n",
    "        if cand_start <= ans_start and cand_end >= ans_end :\n",
    "            #candidate contains the answer, but has extra words\n",
    "            candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            break\n",
    "\n",
    "        if cand_start >= ans_start or cand_end > ans_start:\n",
    "            if (cand_end <= ans_end):\n",
    "                candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            else :\n",
    "                # we have finished, as \n",
    "                if (cand_start < ans_end ):\n",
    "                    candidates_containing_answer.append(current_considered_candidate_id)\n",
    "                break\n",
    "    return candidates_containing_answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The groove usually starts near', 'O'), (u'the', 'STOPWORD'), (u'periphery', u'O'), (u'and', 'STOPWORD'), (u'ends near', u'O'), (u'the', 'STOPWORD'), (u'center', u'O'), (u'of the', 'STOPWORD'), (u'disc', u'O'), (u'.', 'PUNC')]\n",
      "\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "example_candidates = refine_word_tags(tagged_train_set[0][1])\n",
    "example_answer_range = (3,6)\n",
    "print example_candidates\n",
    "print\n",
    "print get_taggings_in_range(example_answer_range,example_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_NER_tag_analysis() :\n",
    "    correct_taggings = []\n",
    "    incorrect_taggings = []\n",
    "    evil_ones = []\n",
    "    for i in range(0, len(trainSents)):\n",
    "        sent_set = trainSents[i]\n",
    "        for j in range(0, len(sent_set)):\n",
    "            sent = sent_set[j]\n",
    "            answers_in_sent = train_sentence_contained_answers[(i,j)]\n",
    "            candidates = refine_word_tags(tagged_train_set[i][j])\n",
    "            for answer in answers_in_sent:\n",
    "                (ans_start, ans_end) = getAnswerIndex(sent, answer)\n",
    "                if (ans_start, ans_end) == (-1,-1) :\n",
    "                    #question = train[i][j][\"question\"]\n",
    "                    evil_ones.append((i,j, answer))\n",
    "                    continue\n",
    "                   \n",
    "                candidate_ids = get_taggings_in_range((ans_start, ans_end),candidates)\n",
    "                if len(candidate_ids) == 1:\n",
    "                    possible_correct_candidate = candidates[candidate_ids[0]]\n",
    "                    if possible_correct_candidate[0] == answer :\n",
    "                        # Correct tagging!\n",
    "                        correct_taggings.append((possible_correct_candidate, i, j))\n",
    "                    else :\n",
    "                        incorrect_taggings.append(([possible_correct_candidate], answer, i, j))\n",
    "                else :\n",
    "                    incorrect_candidates = ([candidates[index] for index in candidate_ids],answer, i, j)\n",
    "                    incorrect_taggings.append(incorrect_candidates)\n",
    "\n",
    "    return (correct_taggings, incorrect_taggings, evil_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-257-2d068b9f8e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mcorrect_taggings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincorrect_taggings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevil_ones\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_NER_tag_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-256-744a03afc19a>\u001b[0m in \u001b[0;36mget_NER_tag_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mcandidate_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_taggings_in_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mpossible_correct_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcandidate_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-e9999d5b4b59>\u001b[0m in \u001b[0;36mget_taggings_in_range\u001b[0;34m((ans_start, ans_end), candidates)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_taggings_in_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mans_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcandidate_ranges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_candidates_to_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcurrent_considered_candidate_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-411d5bee0bc7>\u001b[0m in \u001b[0;36mconvert_candidates_to_range\u001b[0;34m(candidates)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtokenisedWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mnextIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenisedWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcandidatesIndexRange\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnextIndex\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alex/anaconda2/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[1;32m    109\u001b[0m     return [token for sent in sent_tokenize(text, language)\n\u001b[0;32m--> 110\u001b[0;31m             for token in _treebank_word_tokenize(sent)]\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alex/anaconda2/lib/python2.7/site-packages/nltk/tokenize/treebank.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPARENS_BRACKETS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(correct_taggings, incorrect_taggings, evil_ones) = get_NER_tag_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print len(evil_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n",
      "776\n"
     ]
    }
   ],
   "source": [
    "print len(correct_taggings)\n",
    "print len(incorrect_taggings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "correctCounter = Counter()\n",
    "for correct_tags in correct_taggings:\n",
    "    correctCounter[correct_tags[0][1]] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({u'LOCATION': 86, 'NUMBER': 329, 'O': 409, u'PERSON': 141})"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "wrongCounter = Counter()\n",
    "wrongList = defaultdict(list)\n",
    "\n",
    "for wrong_tags in incorrect_taggings:\n",
    "    tagList = []\n",
    "    for (words, tag) in wrong_tags[0]:\n",
    "        tagList.append(tag)\n",
    "    wrongCounter[tuple(tagList)] += 1\n",
    "    wrongList[tuple(tagList)].append(wrong_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('O', 'O', 'O'), 128),\n",
       " (('O', 'O'), 110),\n",
       " (('O',), 75),\n",
       " (('O', 'O', 'O', 'O'), 61),\n",
       " (('NUMBER',), 42),\n",
       " (('O', 'O', 'O', 'O', 'O'), 38),\n",
       " (('NUMBER', 'O', 'NUMBER'), 37),\n",
       " (('NUMBER', 'O'), 29),\n",
       " (('O', 'O', 'O', 'O', 'O', 'O'), 25),\n",
       " (('O', 'NUMBER'), 24)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongCounter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(u'head', 'O'), (u'of', 'O'), (u'state', 'O')], u'head of state', 6, 69)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongList[('O', 'O', 'O')][101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "('NN',)\n",
      "('JJ',)\n",
      "('NNP', 'NNP')\n",
      "('NNP',)\n",
      "('CD',)\n",
      "('NN', 'NN')\n",
      "('NNS',)\n",
      "('VBN',)\n",
      "('NNP', 'NNP', 'NNP')\n",
      "('NNP', 'NNS')\n",
      "('NNP', 'NN')\n",
      "('VB',)\n",
      "('NNP', 'NNP', 'NN')\n",
      "('JJ', 'NN', 'NN')\n",
      "('NN', 'NN', 'NNS')\n",
      "('NNP', 'NNP', 'NNS')\n",
      "('JJ', 'NN')\n",
      "('VBG',)\n",
      "('NNS', 'VBP')\n",
      "75\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "other_answer_pos_tags = Counter()\n",
    "for item in wrongList[('O',)]:\n",
    "    answer = item[1]\n",
    "    tokenisedAnswer = nltk.word_tokenize(answer)\n",
    "    posTagedWord = nltk.pos_tag(tokenisedAnswer)\n",
    "    just_the_tags = [pair[1] for pair in posTagedWord]\n",
    "    other_answer_pos_tags[tuple(just_the_tags)] += 1\n",
    "print len(wrongList[('O',)])\n",
    "\n",
    "total_count = 0\n",
    "for (tag_tuple, count) in other_answer_pos_tags.most_common(20):\n",
    "    total_count += count\n",
    "    print tag_tuple\n",
    "print total_count\n",
    "\n",
    "print (total_count + 0.0 )/ len(wrongList[('O',)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u\"''\", \"''\"), (u')', ')'), (u',', ','), (u'the', 'DT'), (u'rotational', 'JJ'), (u'speed', 'NN'), (u'in', 'IN'), (u'rpm', 'NN'), (u'at', 'IN'), (u'which', 'WDT'), (u'they', 'PRP'), (u'are', 'VBP'), (u'played', 'VBN'), (u'(', '(')]\n",
      "(S\n",
      "  ''/''\n",
      "  )/)\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (ANS rotational/JJ speed/NN)\n",
      "  in/IN\n",
      "  (ANS rpm/NN)\n",
      "  at/IN\n",
      "  which/WDT\n",
      "  they/PRP\n",
      "  are/VBP\n",
      "  played/VBN\n",
      "  (/()\n",
      "\n",
      "[(u'rotational speed', 'O'), (u'rpm', 'O'), ('Nope', 'CRAP')]\n"
     ]
    }
   ],
   "source": [
    "test_sent = wrongList[('O',)][3][0][0][0]\n",
    "test_sent_tokenised = nltk.word_tokenize(test_sent)\n",
    "test_sent_pos_tagged = nltk.pos_tag(test_sent_tokenised)\n",
    "\n",
    "print test_sent_pos_tagged\n",
    "\n",
    "\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "result = cp.parse(test_sent_pos_tagged)\n",
    "print result\n",
    "print\n",
    "print chunk(test_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Dev sent for all:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluateNERonDev():\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    for i in range(0, len(dev)):\n",
    "        qs = dev[i]\n",
    "        for j in range(0, len(qs)):\n",
    "            q = qs[j]\n",
    "            idSent  = q[\"answer_sentence\"]\n",
    "            sent = devSents[i][idSent]\n",
    "            answer = q[\"answer\"]\n",
    "            possAnswers = refine_word_tags(tagged_dev_set[i][idSent])\n",
    "            inThere = False\n",
    "            for possAnswer in possAnswers:\n",
    "                if possAnswer[0] == answer:\n",
    "                    inThere = True\n",
    "                    break\n",
    "            if inThere:\n",
    "                correct.append((i, j, idSent, possAnswers))\n",
    "            else :\n",
    "                wrong.append((i, j, idSent, possAnswers))\n",
    "    return (correct, wrong)\n",
    "\n",
    "\n",
    "(correct,wrong) = evaluateNERonDev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.506203473945\n"
     ]
    }
   ],
   "source": [
    "print (len(correct) + 0.0)/ (len(correct) + len(wrong))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base! 0.261491196975 ... With others in one clump, ie all grouped\n",
    "\n",
    "# 0.412146992792     With others seperated into single units\n",
    "\n",
    "# Add months: 0.416046319272 Every little helps\n",
    "\n",
    "# POS with chunker made from top 10 rules :D\n",
    "# 0.458584426326\n",
    "\n",
    "# including others as singles:\n",
    "# 0.488597424081\n",
    "\n",
    "# slightly longer and more complecated grammar \n",
    "\n",
    "\n",
    "# POS tag on others\n",
    "# 900 tags.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# POS tag on others!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = \"\"\" ANS: {<NNP>*}\n",
    "                   {<JJ>?<NN>*}\n",
    "                   {<NN><NNS>}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "\n",
    "def chunk_dev(words):\n",
    "    tokenWS = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    chunks =  cp.parse(tokenWS)\n",
    "    possAnswers = []\n",
    "    for subtree in chunks.subtrees():\n",
    "        if subtree.label() == 'ANS':\n",
    "            possAnswers.append((' '.join(word for word, pos in subtree.leaves()),'O'))\n",
    "    return possAnswers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New plan, get chunk to return all the words :D\n",
    "\n",
    "\n",
    "words = \"The cat sat on the mat\"\n",
    "\n",
    "chunked_output =  chunk_mark_2(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each answer in train\n",
    "# Any tags that aren't in answers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer_tag_counter = Counter()\n",
    "tag_counter_sents = Counter()\n",
    "ans_length_counter = Counter()\n",
    "\n",
    "for i in range(0, len(train)):\n",
    "    qs = train[i]\n",
    "    for j in range(0, len(qs)):\n",
    "        q = qs[j]\n",
    "        idSent  = q[\"answer_sentence\"]\n",
    "        sent = trainSents[i][idSent]\n",
    "        answer = q[\"answer\"]\n",
    "        \n",
    "        pos_tags_answer = nltk.pos_tag(nltk.word_tokenize(answer))\n",
    "        pos_tags_sent = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "        ans_length_counter[len(pos_tags_answer)] += 1\n",
    "        \n",
    "        for (word, tag) in pos_tags_answer:\n",
    "            answer_tag_counter[tag] += 1\n",
    "            \n",
    "        for (word, tag) in pos_tags_sent:\n",
    "            tag_counter_sents[tag] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_proportion_counter = Counter()\n",
    "for tag in tag_counter.keys():\n",
    "    percentage = (answer_tag_counter[tag]  + 0.0)/ (answer_tag_counter[tag] + tag_counter_sents[tag])\n",
    "    tag_proportion_counter[tag] = percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_rare_tags = []\n",
    "for (tag, percentage) in  tag_proportion_counter.most_common(100000)[-30:]:\n",
    "    list_of_rare_tags.append(tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UH',\n",
       " 'POS',\n",
       " 'VB',\n",
       " \"''\",\n",
       " '``',\n",
       " 'VBG',\n",
       " 'RP',\n",
       " 'DT',\n",
       " 'MD',\n",
       " 'PRP$',\n",
       " 'TO',\n",
       " 'RB',\n",
       " 'JJS',\n",
       " 'PDT',\n",
       " 'IN',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'VBN',\n",
       " 'RBS',\n",
       " 'WRB',\n",
       " 'WP',\n",
       " '(',\n",
       " 'EX',\n",
       " ')',\n",
       " ',',\n",
       " 'VBZ',\n",
       " 'WDT',\n",
       " 'VBD',\n",
       " '.',\n",
       " ':']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_rare_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66104"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([val for (key,val) in ans_length_counter.most_common(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "}<,|EX|WP|WRB|VBZ|WDT|RBS|VDB|.|:>+{"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
