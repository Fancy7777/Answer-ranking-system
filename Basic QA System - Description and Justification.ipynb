{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the python script containing the same code as the load the data notebook\n",
    "%run loadData.py\n",
    "# now we can access train, dev, and test\n",
    "# along with trainSents, devSents testSents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Phonograph records are generally described by their diameter in inches (12\", 10\", 7\"), the rotational speed in rpm at which they are played (16 2\\u20443, 33 1\\u20443, 45, 78), and their time capacity resulting from a combination of those parameters (LP \\u2013 long playing 33 1\\u20443 rpm, SP \\u2013 78 rpm single, EP \\u2013 12-inch single or extended play, 33 or 45 rpm); their reproductive quality or level of fidelity (high-fidelity, orthophonic, full-range, etc.), and the number of audio channels provided (mono, stereo, quad, etc.).'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSents[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'answer': u'long playing',\n",
       " u'answer_sentence': 2,\n",
       " u'question': u'What does LP stand for when it comes to time capacity?'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = testSents[0]\n",
    "questions = test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287\n"
     ]
    }
   ],
   "source": [
    "print len(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Workflow Thoughts (dealing with .ipynb notebooks)\n",
    "\n",
    "Think with each feature we do below, create generalized functions that can be easily composed with easy names, split by type.\n",
    "\n",
    "Then create/use small demo template below using the locked document/questions above to get intuition, check sanity, iterate quickly, to help keep us all on the same page.\n",
    "\n",
    "This way we keep everything well contained/documented/explainable, will help with report writing.\n",
    "\n",
    "Then in separate notebook we do statistical valid/testing for error exploration/analysis, using generalized functions above - easily changeable/copyable.\n",
    "\n",
    "Finally put it all in a python file that will do full run. Write TODOs to illustrate next steps/improvements, that way can stay on top/track/improve upon easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Retreival\n",
    "\n",
    "The first part of your basic QA system will use a bag-of-words (BOW) vector space model to identify the sentence in the Wikipedia article which is most likely to contain the answer to a question, using standard information retrieval techniques. Here the \"query\" is the question, the \"documents\" are actually sentences, and each Wikipedia article should be viewed as separate \"document collection\". You should apply various preprocessing steps appropriate to this situation, including term weighting; if you are at all uncertain about what choices to make, you should evaluate them using the dev data, and use the results to justify your choice in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO\n",
    "\n",
    "* Improving tuning of preprocessing/lemmatize functions for use QA case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning functions\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Follow lemmatize function from guide notebook: WSTA_N1B_preprocessing.ipynb\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "def vectorize_documents(text_documents):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vector_documents = vectorizer.fit_transform(text_documents)\n",
    "    \n",
    "    return [vector_documents, vectorizer]\n",
    "\n",
    "def vectorize_query(vectorizer, text_query):\n",
    "    return vectorizer.transform([text_query])\n",
    "\n",
    "def process_neighbours(vector_documents):\n",
    "    \n",
    "    neighbours = NearestNeighbors(1, algorithm=\"brute\", metric=\"cosine\")\n",
    "    neighbours.fit(vector_documents)\n",
    "    \n",
    "    return neighbours\n",
    "\n",
    "def closest_document(neighbours, vector_query):\n",
    "\n",
    "    result = neighbours.kneighbors(vector_query, 1, return_distance=True)\n",
    "\n",
    "    result_index = result[1][0][0]\n",
    "    result_distance = result[0][0][0]\n",
    "    \n",
    "    return [result_distance, result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demonstration function\n",
    "\n",
    "def demo_process_set(questions, documents):\n",
    "    \n",
    "    vector_documents, vectorizer = vectorize_documents(documents)\n",
    "    analyze = vectorizer.build_analyzer()\n",
    "    neighbours = process_neighbours(vector_documents)\n",
    "\n",
    "    print \"=\" * 20\n",
    "    print \"Vector documents shape: {0}\".format(vector_documents.shape)\n",
    "    print \"Actual documents length: {0}\".format(len(documents))\n",
    "    print \"=\" * 20, \"\\n\"\n",
    "    \n",
    "    for question in questions[10:10+3]:\n",
    "        \n",
    "        text_query = question[\"question\"]\n",
    "\n",
    "        print \"Text query:\\n\\n\\t{0}\\n\".format(text_query)\n",
    "\n",
    "        vector_query = vectorize_query(vectorizer, text_query)\n",
    "\n",
    "        print \"Vector query shape:\\n\\n\\t{0}\".format(vector_query.shape)\n",
    "\n",
    "        result_distance, result_index  = closest_document(neighbours, vector_query)\n",
    "        \n",
    "        print\n",
    "\n",
    "        print \"Result:\\n\\n\\tDistance ({0}), Index ({1})\\n\".format(result_distance, result_index)\n",
    "\n",
    "        print\n",
    "\n",
    "        print \"Query (text):\\n\\n\\t{0}\\n\".format(text_query)\n",
    "        print \"Document (text):\\n\\n\\t{0}\".format(documents[result_index].encode(\"utf-8\"))\n",
    "\n",
    "        print\n",
    "\n",
    "        print \"Query (vector text):\\n\"\n",
    "        pp.pprint(analyze(text_query))\n",
    "        print\n",
    "        \n",
    "        print \"Document (vector text): \\n\\n\"\n",
    "        pp.pprint(analyze(documents[result_index]))\n",
    "        \n",
    "        print \"\\n\", \"=\" * 20, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Vector documents shape: (381, 1886)\n",
      "Actual documents length: 381\n",
      "==================== \n",
      "\n",
      "Text query:\n",
      "\n",
      "\tWhere did the war begin?\n",
      "\n",
      "Vector query shape:\n",
      "\n",
      "\t(1, 1886)\n",
      "  (0, 1835)\t0.336166251799\n",
      "  (0, 602)\t0.569600436833\n",
      "  (0, 309)\t0.750031728336\n",
      "\n",
      "Result:\n",
      "\n",
      "\tDistance (0.675636845783), Index (313)\n",
      "\n",
      "\n",
      "Query (text):\n",
      "\n",
      "\tWhere did the war begin?\n",
      "\n",
      "Document (text):\n",
      "\n",
      "\tHowever, Greece did not coordinate its plans with Russia, did not declare war, and received no outside military or financial support.\n",
      "\n",
      "Query (vector text):\n",
      "\n",
      "[u'did', u'war', u'begin']\n",
      "\n",
      "Document (vector text): \n",
      "\n",
      "\n",
      "[   u'greece',\n",
      "    u'did',\n",
      "    u'coordinate',\n",
      "    u'plans',\n",
      "    u'russia',\n",
      "    u'did',\n",
      "    u'declare',\n",
      "    u'war',\n",
      "    u'received',\n",
      "    u'outside',\n",
      "    u'military',\n",
      "    u'financial',\n",
      "    u'support']\n",
      "\n",
      "==================== \n",
      "\n",
      "Text query:\n",
      "\n",
      "\tRussian troops took over which provinces first?\n",
      "\n",
      "Vector query shape:\n",
      "\n",
      "\t(1, 1886)\n",
      "  (0, 1767)\t0.464837369809\n",
      "  (0, 1740)\t0.565353995174\n",
      "  (0, 1519)\t0.303980942175\n",
      "  (0, 1372)\t0.609833310475\n",
      "\n",
      "Result:\n",
      "\n",
      "\tDistance (0.644976635632), Index (10)\n",
      "\n",
      "\n",
      "Query (text):\n",
      "\n",
      "\tRussian troops took over which provinces first?\n",
      "\n",
      "Document (text):\n",
      "\n",
      "\tThe war opened in the Balkans when Russian troops occupied provinces in modern Romania and began to cross the Danube.\n",
      "\n",
      "Query (vector text):\n",
      "\n",
      "[u'russian', u'troops', u'took', u'provinces']\n",
      "\n",
      "Document (vector text): \n",
      "\n",
      "\n",
      "[   u'war',\n",
      "    u'opened',\n",
      "    u'balkans',\n",
      "    u'russian',\n",
      "    u'troops',\n",
      "    u'occupied',\n",
      "    u'provinces',\n",
      "    u'modern',\n",
      "    u'romania',\n",
      "    u'began',\n",
      "    u'cross',\n",
      "    u'danube']\n",
      "\n",
      "==================== \n",
      "\n",
      "Text query:\n",
      "\n",
      "\tWho were the Ottomans led by?\n",
      "\n",
      "Vector query shape:\n",
      "\n",
      "\t(1, 1886)\n",
      "  (0, 1231)\t0.699129560466\n",
      "  (0, 1042)\t0.714995005355\n",
      "\n",
      "Result:\n",
      "\n",
      "\tDistance (0.611682742126), Index (11)\n",
      "\n",
      "\n",
      "Query (text):\n",
      "\n",
      "\tWho were the Ottomans led by?\n",
      "\n",
      "Document (text):\n",
      "\n",
      "\tLed by Omar Pasha, the Ottomans fought a strong defensive battle and stopped the advance at Silistra.\n",
      "\n",
      "Query (vector text):\n",
      "\n",
      "[u'ottomans', u'led']\n",
      "\n",
      "Document (vector text): \n",
      "\n",
      "\n",
      "[   u'led',\n",
      "    u'omar',\n",
      "    u'pasha',\n",
      "    u'ottomans',\n",
      "    u'fought',\n",
      "    u'strong',\n",
      "    u'defensive',\n",
      "    u'battle',\n",
      "    u'stopped',\n",
      "    u'advance',\n",
      "    u'silistra']\n",
      "\n",
      "==================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_process_set(questions, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second main part of your basic QA system is an NER system. In this initial system you should have at least four\n",
    "answer types: PERSON, LOCATION, NUMBER, and OTHER. You should run the Stanford NER system over your\n",
    "sentences to extract people and location entities (Hint: make use of the \"tag_sents\" method in the NLTK interface to do\n",
    "this efficiently for multiple sentences in a single call, otherwise this will be very slow; you may also want to cache the\n",
    "entity information during development of your system, rather than calling Stanford NER for each run). Note that\n",
    "contiguous words tagged as the same type should be considered part of the same entity. ORGANIZATION entities\n",
    "extracted by the NER system should be considered OTHER. You should also extract and treat as OTHER any other\n",
    "non-sentence initial sequence of capitalized words not tagged by Stanford NER. Finally, you should label all numbers as\n",
    "NUMBER. In this process, you might notice errors related to your preprocessing (e.g. tokenization), errors which can be\n",
    "easily corrected should be addressed at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO\n",
    "\n",
    "* Finish NER Tagging \n",
    "* Test run stanford function on another machine (+ record times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# The required jar files : https://nlp.stanford.edu/software/CRF-NER.shtml#Download\n",
    "# It's 171mb so I've added to the gitignore\n",
    "# If you download it, and rename the folder name \"stanford\" in the main directory\n",
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(u'There', u'O'), (u'were', u'O'), (u'three', u'O'), (u'geese', u'O'), (u',', u'O'), (u'Derek', u'PERSON'), (u',', u'O'), (u'Joshi', u'PERSON'), (u',', u'O'), (u'and', u'O'), (u'Alex', u'PERSON'), (u',', u'O'), (u'who', u'O'), (u'lived', u'O'), (u'in', u'O'), (u'Melbourne', u'LOCATION'), (u',', u'O'), (u'and', u'O'), (u'worked', u'O'), (u'at', u'O'), (u'Microsoft', u'ORGANIZATION')]]\n"
     ]
    }
   ],
   "source": [
    "#Quick test of Tagger\n",
    "text = \"There were three geese, Derek, Joshi, and Alex, who lived in Melbourne, and worked at Microsoft\"\n",
    "tokenizedText = word_tokenize(text)\n",
    "classifiedText = sTagger.tag_sents([tokenizedText])\n",
    "\n",
    "# Warning, it takes a little while ~ 5 seconds on my comp\n",
    "print(classifiedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle # Useful for read / write of list file\n",
    "import os #Needed to check if file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets store the stanford tagger output in a file\n",
    "# This function returns the tagging output of stanford for each dataset\n",
    "# with datasetName - 'train', 'dev', test' \n",
    "\n",
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taggedTrain = getStanfordTagging('train')\n",
    "taggedDev = getStanfordTagging('dev')\n",
    "taggedTest = getStanfordTagging('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'O'), (u'French', u'O'), (u'promoted', u'O'), (u'the', u'O'), (u'rights', u'O'), (u'of', u'O'), (u'Catholics', u'O'), (u',', u'O'), (u'while', u'O'), (u'Russia', u'LOCATION'), (u'promoted', u'O'), (u'those', u'O'), (u'of', u'O'), (u'the', u'O'), (u'Eastern', u'O'), (u'Orthodox', u'O'), (u'Christians', u'O'), (u'.', u'O')]\n"
     ]
    }
   ],
   "source": [
    "print taggedTest[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Given a stanford tagged list, refines the list by:\\n\",\n",
    "# Grouping all contiguous words with the same tag\\n\",\n",
    "# Relabels Organisations as Other\\n\",\n",
    "# Labels Number\\n\",\n",
    "def refineWordTags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    prevWord = ''\n",
    "    prevTag = taggedWordList[0][1] # Ie the first tag\\n\",\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif isCapitalised(word):\n",
    "                tag = 'OTHERCAP'\n",
    "            elif isStopWord(word):\n",
    "                tag = 'STOPWORD'\n",
    "        if (tag == prevTag):\n",
    "            prevWord += ' ' + word\n",
    "        else :\n",
    "            newWordTags.append((prevWord, prevTag))\n",
    "            prevWord = word\n",
    "            prevTag = tag\n",
    "    # Need to add the final ones\n",
    "    newWordTags.append((prevWord, prevTag))\n",
    "    return newWordTags\n",
    "        \n",
    "# Thanks : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word.lower() in stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Raleigh', u'O'), (u'(', u'O'), (u'\\u02c8r\\u0251\\u02d0li', u'O'), (u';', u'O'), (u'RAH-lee', u'O'), (u')', u'O'), (u'is', u'O'), (u'the', u'O'), (u'capital', u'O'), (u'of', u'O'), (u'the', u'O'), (u'state', u'O'), (u'of', u'O'), (u'North', u'LOCATION'), (u'Carolina', u'LOCATION'), (u'as', u'O'), (u'well', u'O'), (u'as', u'O'), (u'the', u'O'), (u'seat', u'O'), (u'of', u'O'), (u'Wake', u'LOCATION'), (u'County', u'LOCATION'), (u'in', u'O'), (u'the', u'O'), (u'United', u'LOCATION'), (u'States', u'LOCATION'), (u'.', u'O')]\n",
      "\n",
      "[('', u'O'), (u'Raleigh', 'OTHERCAP'), (u'( \\u02c8r\\u0251\\u02d0li ;', u'O'), (u'RAH-lee', 'OTHERCAP'), (u')', u'O'), (u'is the', 'STOPWORD'), (u'capital', u'O'), (u'of the', 'STOPWORD'), (u'state', u'O'), (u'of', 'STOPWORD'), (u'North Carolina', u'LOCATION'), (u'as', 'STOPWORD'), (u'well', u'O'), (u'as the', 'STOPWORD'), (u'seat', u'O'), (u'of', 'STOPWORD'), (u'Wake County', u'LOCATION'), (u'in the', 'STOPWORD'), (u'United States', u'LOCATION'), (u'.', u'O')]\n"
     ]
    }
   ],
   "source": [
    "# Small test:\n",
    "print (taggedTrain[1][0])\n",
    "print \n",
    "print refineWordTags(taggedTrain[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Quick regex test:\n",
    "print isNumber('.25')\n",
    "print isNumber('123.123')\n",
    "print isNumber('163.2342.234  ')\n",
    "print isNumber('123,123,123.2  ')\n",
    "print isNumber('joshi')\n",
    "print isNumber('$123')\n",
    "print isNumber('four') \n",
    "print isNumber('h123') # Should this be a number?\n",
    "print isNumber('123m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluateNER(questionsList,documentsList, numToEval):\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    for i in range (0, numToEval):\n",
    "        documents = documentsList[i]\n",
    "        questions = questionsList[i]\n",
    "        for j in range (0, len(questions)):\n",
    "            answer = questionsList[i][j][\"answer\"]\n",
    "            answerID = questionsList[i][j][\"answer_sentence\"]\n",
    "            possAnswers = refineWordTags(taggedDev[i][answerID])\n",
    "            inThere = False\n",
    "            for possAnswer in possAnswers:\n",
    "                if possAnswer[0] == answer:\n",
    "                    inThere = True\n",
    "                    break\n",
    "            if inThere:\n",
    "                correct.append((i,j, possAnswers))\n",
    "            else :\n",
    "                wrong.append((i,j, answer, possAnswers))\n",
    "    return (correct, wrong)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Correct : 3408\n",
      "Number incorrect: 5055\n",
      "Average correct : 0.402694080113\n"
     ]
    }
   ],
   "source": [
    "(corNER, wrongNER) = evaluateNER(dev, devSents,len(dev))\n",
    "\n",
    "print(\"Number Correct : \" + str(len(corNER)))\n",
    "print(\"Number incorrect: \" + str(len(wrongNER)))\n",
    "print(\"Average correct : \" + str((len(corNER) + 0.0) / (len(corNER)+len(wrongNER))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER : active near-infrared illumination\n",
      "\n",
      "Night-vision\n",
      "devices using active near-infrared illumination allow people\n",
      "or\n",
      "animals\n",
      "to be\n",
      "observed without\n",
      "the\n",
      "observer\n",
      "being\n",
      "detected .\n",
      "\n",
      "ANSWER : Infrared astronomy\n",
      "\n",
      "Infrared\n",
      "astronomy uses sensor-equipped telescopes\n",
      "to\n",
      "penetrate dusty regions\n",
      "of\n",
      "space ,\n",
      "such as\n",
      "molecular clouds ; detect objects\n",
      "such as\n",
      "planets ,\n",
      "and to\n",
      "view highly red-shifted objects\n",
      "from the\n",
      "early days\n",
      "of the\n",
      "universe .\n",
      "\n",
      "ANSWER : red\n",
      "\n",
      "Infrared\n",
      "astronomy uses sensor-equipped telescopes\n",
      "to\n",
      "penetrate dusty regions\n",
      "of\n",
      "space ,\n",
      "such as\n",
      "molecular clouds ; detect objects\n",
      "such as\n",
      "planets ,\n",
      "and to\n",
      "view highly red-shifted objects\n",
      "from the\n",
      "early days\n",
      "of the\n",
      "universe .\n",
      "\n",
      "ANSWER : Infrared thermal-imaging cameras\n",
      "\n",
      "Infrared\n",
      "thermal-imaging cameras\n",
      "are\n",
      "used\n",
      "to\n",
      "detect heat loss\n",
      "in\n",
      "insulated systems ,\n",
      "to\n",
      "observe changing blood flow\n",
      "in the\n",
      "skin ,\n",
      "and to\n",
      "detect overheating\n",
      "of\n",
      "electrical apparatus .\n",
      "\n",
      "ANSWER : scientific\n",
      "\n",
      "Infrared\n",
      "radiation\n",
      "is\n",
      "used\n",
      "in\n",
      "industrial , scientific ,\n",
      "and\n",
      "medical applications .\n",
      "\n",
      "ANSWER : 700 nm\n",
      "\n",
      "The\n",
      "human eye\n",
      "is\n",
      "markedly less sensitive\n",
      "to\n",
      "light\n",
      "above\n",
      "700\n",
      "nm wavelength ,\n",
      "so\n",
      "longer wavelengths make insignificant contributions\n",
      "to\n",
      "scenes illuminated\n",
      "by\n",
      "common light sources .\n",
      "\n",
      "ANSWER : near-IR\n",
      "\n",
      "However\n",
      ", particularly intense near-IR light ( e.g. ,\n",
      "from\n",
      "IR\n",
      "lasers ,\n",
      "IR LED\n",
      "sources ,\n",
      "or from\n",
      "bright daylight\n",
      "with the\n",
      "visible light removed\n",
      "by\n",
      "colored gels )\n",
      "can be\n",
      "detected\n",
      "up to\n",
      "approximately\n",
      "780\n",
      "nm ,\n",
      "and will be\n",
      "perceived\n",
      "as\n",
      "red light .\n",
      "\n",
      "ANSWER : 780 nm\n",
      "\n",
      "However\n",
      ", particularly intense near-IR light ( e.g. ,\n",
      "from\n",
      "IR\n",
      "lasers ,\n",
      "IR LED\n",
      "sources ,\n",
      "or from\n",
      "bright daylight\n",
      "with the\n",
      "visible light removed\n",
      "by\n",
      "colored gels )\n",
      "can be\n",
      "detected\n",
      "up to\n",
      "approximately\n",
      "780\n",
      "nm ,\n",
      "and will be\n",
      "perceived\n",
      "as\n",
      "red light .\n",
      "\n",
      "ANSWER : 1050 nm\n",
      "\n",
      "Sources\n",
      "providing wavelengths\n",
      "as\n",
      "long\n",
      "as\n",
      "1050\n",
      "nm\n",
      "can be\n",
      "seen\n",
      "as a\n",
      "dull red glow\n",
      "in\n",
      "intense sources , causing\n",
      "some\n",
      "difficulty\n",
      "in\n",
      "near-IR illumination\n",
      "of\n",
      "scenes\n",
      "in the\n",
      "dark ( usually\n",
      "this\n",
      "practical problem\n",
      "is\n",
      "solved\n",
      "by\n",
      "indirect illumination ) .\n",
      "\n",
      "ANSWER : hotter\n",
      "\n",
      "For\n",
      "example ,\n",
      "for any\n",
      "pre-set emissivity value , objects\n",
      "with\n",
      "higher emissivity\n",
      "will\n",
      "appear hotter ,\n",
      "and those with a\n",
      "lower emissivity\n",
      "will\n",
      "appear cooler .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the wrong taggings\n",
    "for num in range (0, 10):\n",
    "    (i,j, answer, possAnswers) = wrongNER[ num]\n",
    "    print \"ANSWER : \" + answer\n",
    "    for possAnswer in possAnswers:\n",
    "        print possAnswer[0]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After you have extracted a set of entities from your sentence, you will rank them to choose the best answer. The ranking\n",
    "should be based on three factors. First, answers whose content words all appear in the question should be ranked\n",
    "lowest. Second, answers which match the question type should be ranked higher than those that don't; for this, you\n",
    "should build a simple rule-based question type classifier based on key words (e.g. questions which contain \"who\" are\n",
    "people). Third, among entities of the same type, the prefered entity should be the one which is closer in the sentence to a\n",
    "closed-class word from the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO\n",
    "\n",
    "* TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Part A gives us a most likely sentence\n",
    "# Part B splits into entities\n",
    "\n",
    "\n",
    "# Given a question, returns a tag for the answer form\n",
    "# From PERSON, LOCATION, NUMBER, OTHER \n",
    "# Assuming question is lowercased\n",
    "def getQuestionType(question):\n",
    "    if 'Who' in question:\n",
    "        return \"PERSON\"\n",
    "    if 'where' in question:\n",
    "        return \"LOCATION\"\n",
    "    if 'How many' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'How much' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'When' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'what year' in question:\n",
    "        return \"NUMBER\"\n",
    "    else:\n",
    "        return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "What did Herschel call the infrared spectrum?\n",
    "[('', u'O'), (u'He', 'OTHERCAP'), (u'was', 'STOPWORD'), (u'surprised', u'O'), (u'at the', 'STOPWORD'), (u'result', u'O'), (u'and', 'STOPWORD'), (u'called', u'O'), (u'them', 'STOPWORD'), (u'``', u'O'), (u'Calorific Rays', 'OTHERCAP'), (u\"'' .\", u'O')]\n",
    "Calorific Rays\n",
    "was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question = \"What did Herschel call the infrared spectrum?\"\n",
    "a = [('', u'O'), (u'He', 'OTHERCAP'), (u'was', 'STOPWORD'), (u'surprised', u'O'), (u'at the', 'STOPWORD'), (u'result', u'O'), (u'and', 'STOPWORD'), (u'called', u'O'), (u'them', 'STOPWORD'), (u'``', u'O'), (u'Calorific Rays', 'OTHERCAP'), (u\"'' .\", u'O')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ranking_list': [(u'surprised', u'O'), (u'at the', 'STOPWORD'), (u'result', u'O'), (u'called', u'O'), (u'``', u'O'), (u'Calorific Rays', 'OTHERCAP'), (u\"'' .\", u'O')], 'answer_entities_list': ['', u'He', u'was', u'surprised', u'at the', u'result', u'and', u'called', u'them', u'``', u'Calorific Rays', u\"'' .\"], 'same_word_list': [u'He']}\n"
     ]
    }
   ],
   "source": [
    "test = first_filter(question,a)\n",
    "print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ranking_list': [u'surprised', u'at the', u'result', u'called', u'``', u'Calorific Rays', u\"'' .\"], 'Other_tags_list': [], 'answer_entities_list': ['', u'He', u'was', u'surprised', u'at the', u'result', u'and', u'called', u'them', u'``', u'Calorific Rays', u\"'' .\"], 'same_word_list': [u'He']}\n"
     ]
    }
   ],
   "source": [
    "test2 = second_filter(question, test)\n",
    "print test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'third_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-0559f21a3726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print cal_distance_words('Melbourne',test2[\"same_word_list\"],test2[\"answer_entities_list\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthird_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer_entities_list\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtest3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'third_filter' is not defined"
     ]
    }
   ],
   "source": [
    "#print cal_distance_words('Melbourne',test2[\"same_word_list\"],test2[\"answer_entities_list\"])\n",
    "test3 = third_filter(question, test2,test2[\"answer_entities_list\"])\n",
    "print test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first:  whose anwsers all appear in the questions rank the lowest\n",
    "#assumption: input question in its dictionary value\n",
    "\n",
    "def first_filter(question, anwser_entities):\n",
    "    ranking_dict_1 = {}\n",
    "    ranking_list = []\n",
    "    merge_list = []\n",
    "    answer_entities_list = []\n",
    "    for entity in anwser_entities:\n",
    "        answer_entities_list.append(entity[0])\n",
    "        if entity[0] in question:\n",
    "            if entity[0] not in stop_words and entity[0]!='':\n",
    "                #print entity[0]\n",
    "                merge_list.append(entity[0])\n",
    "        else:\n",
    "            if entity[0] not in stop_words and entity[0]!='':\n",
    "                ranking_list.append(entity)\n",
    "    ranking_dict_1[\"ranking_list\"] = ranking_list\n",
    "    ranking_dict_1[\"same_word_list\"] = merge_list\n",
    "    ranking_dict_1[\"answer_entities_list\"] = answer_entities_list\n",
    "    return ranking_dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#second: answers which match the question type should be ranked higher than those that dont\n",
    "\n",
    "#assumption: save questions' type in the dictionary format quesiton1 = \n",
    "#{u'answer': u'long playing',u'answer_sentence': 2, u'question':......., 'question_type:'PERSON'}\n",
    "\n",
    "def second_filter(question, ranking_dict_1):\n",
    "    question_with_type ={}\n",
    "    question_with_type['question_type']= getQuestionType(question)\n",
    "    question_with_type['question'] = question\n",
    "    #print question_with_type\n",
    "    ranking_dict_2 = {}\n",
    "    ranking_list =[]\n",
    "    merge_list = []\n",
    "    for entity in ranking_dict_1[\"ranking_list\"]:\n",
    "        if question_with_type['question_type'] == 'O':\n",
    "            ranking_list.append(entity[0])\n",
    "        else:\n",
    "            if entity[1] == question_with_type['question_type']:\n",
    "                ranking_list.append(entity[0])\n",
    "            else:\n",
    "                merge_list.append(entity[0])\n",
    "    ranking_dict_2[\"same_word_list\"] = ranking_dict_1[\"same_word_list\"]\n",
    "    ranking_dict_2[\"ranking_list\"] = ranking_list\n",
    "    ranking_dict_2[\"Other_tags_list\"] = merge_list\n",
    "    ranking_dict_2[\"answer_entities_list\"] = ranking_dict_1[\"answer_entities_list\"]\n",
    "    return ranking_dict_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Thrid: based on second, the prefered entity should be the one which is close in \n",
    "#the sentence to a closed-class word form the question\n",
    "from collections import OrderedDict\n",
    "\n",
    "def cal_distance_words(entity,same_words, anwser_entities):\n",
    "    temp = 0\n",
    "    for same_word in same_words:\n",
    "        temp += abs(anwser_entities.index(entity) - anwser_entities.index(same_word))\n",
    "    return float(temp)/float(len(same_words))\n",
    "\n",
    "def sort_orderedDict(orderdict):\n",
    "    return OrderedDict(sorted(orderdict.items(), key = lambda x:x[1], reverse = False))\n",
    "        \n",
    "\n",
    "def third_filter(question,second_filter,anwser_entities):\n",
    "    dict_ranking ={}\n",
    "    if (len(second_filter[\"same_word_list\"])==0):\n",
    "        if len(second_filter['ranking_list']) != 0:\n",
    "            return second_filter['ranking_list'][0]\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        for entity in second_filter[\"ranking_list\"]:\n",
    "            dict_ranking[entity]= cal_distance_words(entity, second_filter[\"same_word_list\"],anwser_entities)\n",
    "        #print dict_ranking\n",
    "        dict_ranking = sort_orderedDict(dict_ranking)\n",
    "        #print dict_ranking\n",
    "        if len(dict_ranking.items()) ==0:\n",
    "            return 0\n",
    "        else:\n",
    "            return dict_ranking.items()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "question = \"where does Jack live?\"\n",
    "possAnswers = [(u'Jack', u'PERSON'),(\"live\",'O'),(\"in\",'O'),(\"Melbourne\",'LOCATION')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ranking_list': [(u'Jack', u'PERSON'), ('live', 'O'), ('in', 'O'), ('Footscray', 'LOCATION'), ('work', 'O'), ('Carlton', 'LOCATION')], 'answer_entities_list': [u'Jack', 'live', 'in', 'Footscray', 'work', 'Carlton'], 'same_word_list': []}\n",
      "{'ranking_list': [u'Jack', 'Footscray', 'Carlton'], 'Other_tags_list': ['live', 'in', 'work'], 'answer_entities_list': [u'Jack', 'live', 'in', 'Footscray', 'work', 'Carlton'], 'same_word_list': []}\n",
      "Jack\n"
     ]
    }
   ],
   "source": [
    "print first_filter(question, a)\n",
    "print second_filter(question,first_filter(question, a))\n",
    "print third_filter(question,second_filter(question,first_filter(question, a)),second_filter(question,first_filter(question, a))['answer_entities_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "I've added the evaluation here to save exporting the above functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluateNER(questionsList,documentsList, numToEval):\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    for i in range (0, numToEval):\n",
    "        documents = documentsList[i]\n",
    "        questions = questionsList[i]\n",
    "        for j in range (0, len(questions)):\n",
    "            answer = questionsList[i][j][\"answer\"]\n",
    "            answerID = questionsList[i][j][\"answer_sentence\"]\n",
    "            possAnswers = refineWordTags(taggedDev[i][answerID])\n",
    "            inThere = False\n",
    "            for possAnswer in possAnswers:\n",
    "                if possAnswer[0] == answer:\n",
    "                    inThere = True\n",
    "                    break\n",
    "            if inThere:\n",
    "                correct.append((i,j, possAnswers))\n",
    "            else :\n",
    "                wrong.append((i,j, answer, possAnswers))\n",
    "    return (correct, wrong)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluateAnswerRanking(questionsList,documentsList, numToEval):\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    (corNER, wrongNER) = evaluateNER(questionsList,documentsList, numToEval)\n",
    "    entityListsWithAnswer = corNER\n",
    "    for (i,j,possAnswers) in entityListsWithAnswer:\n",
    "        question = questionsList[i][j][\"question\"]\n",
    "        answer =  questionsList[i][j][\"answer\"]\n",
    "        answerPredicited = third_filter(question,second_filter(question,first_filter(question, possAnswers)),second_filter(question,first_filter(question, possAnswers))['answer_entities_list']) \n",
    "        #print question\n",
    "        #print possAnswers\n",
    "        #print answer\n",
    "        #print answerPredicited\n",
    "        #print '%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%'\n",
    "        # TODO add Dereks part here\n",
    "        if (answerPredicited == answer):\n",
    "            correct.append((i,j))\n",
    "        else :\n",
    "            wrong.append((i,j,answerPredicited))\n",
    "        #print correct\n",
    "    return (correct, wrong)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Correct : 1000\n",
      "Number incorrect: 2408\n",
      "Average correct : 0.293427230047\n"
     ]
    }
   ],
   "source": [
    "(corAns, wrongAns) = evaluateAnswerRanking(dev, devSents,len(dev))\n",
    "print(\"Number Correct : \" + str(len(corAns)))\n",
    "print(\"Number incorrect: \" + str(len(wrongAns)))\n",
    "print (\"Average correct : \" + str((len(corAns) + 0.0) / (len(corAns)+len(wrongAns))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission funtions\n",
    "run it here.\n",
    "Start with Joshi's closest sentences in each doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demonstration function\n",
    "\n",
    "def process_set(questions, documents):\n",
    "    vector_documents, vectorizer = vectorize_documents(documents)\n",
    "    analyze = vectorizer.build_analyzer()\n",
    "    neighbours = process_neighbours(vector_documents)\n",
    "    \n",
    "    joshi_list = []\n",
    "    for question in questions:\n",
    "        joshi_dict = {}\n",
    "        text_query = question[\"question\"]\n",
    "        vector_query = vectorize_query(vectorizer, text_query)\n",
    "        result_distance, result_index  = closest_document(neighbours, vector_query)\n",
    "        \n",
    "        joshi_dict[\"query\"] = text_query\n",
    "        joshi_dict[\"answer_sentence\"] =documents[result_index].encode(\"utf-8\")\n",
    "        joshi_dict[\"index\"]=result_index\n",
    "        joshi_list.append(joshi_dict)\n",
    "    return joshi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joshi_list =[]\n",
    "for index in range(len(testSents)):\n",
    "    documents = testSents[index]\n",
    "    questions = test[index]\n",
    "    joshi_list.append(process_set(questions, documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append Alex's tagged part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': u'What year did the Crimean War begin?', 'answer_sentence': 'The Russians did nothing and he evacuated to Batum in February of the following year.', 'index': 281}\n"
     ]
    }
   ],
   "source": [
    "## alex function\n",
    "print joshi_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc_id in range(len(joshi_list)):\n",
    "    for each_one_index in range(len(joshi_list[doc_id])):\n",
    "        index = joshi_list[doc_id][each_one_index]['index']\n",
    "        joshi_list[doc_id][each_one_index][\"possAnswer\"] = taggedTest[doc_id][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in joshi_list:\n",
    "    for ele in doc:\n",
    "        ele['possAnswer'] = refineWordTags(ele['possAnswer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use Derek's answer ranking and output the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "refine_list =[]\n",
    "for doc in joshi_list:\n",
    "    for ele in doc:\n",
    "        refine_list.append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer_list = []\n",
    "for ele in refine_list:\n",
    "    answer_dict = {}\n",
    "    question = ele['query']\n",
    "    possAnswers = ele['possAnswer']\n",
    "    answer = third_filter(question,second_filter(question,first_filter(question, possAnswers)),second_filter(question,first_filter(question, possAnswers))['answer_entities_list'])\n",
    "    answer_dict['sentence'] = possAnswers\n",
    "    answer_dict['question'] = question\n",
    "    answer_dict['answer'] = answer\n",
    "    answer_list.append(answer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "headers = ['id', 'answer']\n",
    "\n",
    "with open('submit.csv','w') as f:\n",
    "    f_csv = csv.DictWriter(f, headers)\n",
    "    f_csv.writeheader()\n",
    "    for index in range(len(answer_list)):\n",
    "        if isinstance( answer_list[index]['answer'], int):\n",
    "            f_csv.writerows([{'id':index+1,'answer':answer_list[index]['answer']}])\n",
    "        else:\n",
    "            f_csv.writerows([{'id':index+1,'answer':answer_list[index]['answer'].encode(\"utf-8\")}])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
