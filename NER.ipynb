{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run loadData.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# The required jar files : https://nlp.stanford.edu/software/CRF-NER.shtml#Download\n",
    "# It's 171mb so I've added to the gitignore\n",
    "# If you download it, and rename the folder name \"stanford\" in the main directory\n",
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle # Useful for read / write of list file\n",
    "import os #Needed to check if file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets store the stanford tagger output in a file\n",
    "# This function returns the tagging output of stanford for each dataset\n",
    "# with datasetName - 'train', 'dev', test' \n",
    "\n",
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    fnameDevAnswers = './preCompTags/stanfordTaggedDevAnswers.txt'\n",
    "    fnameTrainAnswers = './preCompTags/stanfordTaggedTrainAnswers.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    elif (datasetName == 'ansDev' ):\n",
    "        theFilePath = fnameDevAnswers \n",
    "        devAnswers = []\n",
    "        for sents in dev:\n",
    "            sentsAnswers = []\n",
    "            for question in sents:\n",
    "                sentsAnswers.append(question[\"answer\"])\n",
    "            devAnswers.append(sentsAnswers)\n",
    "        theSents = devAnswers\n",
    "    elif (datasetName == 'ansTrain' ):\n",
    "            theFilePath = fnameTrainAnswers \n",
    "            trainAnswers = []\n",
    "            for sents in train:\n",
    "                sentsAnswers = []\n",
    "                for question in sents:\n",
    "                    sentsAnswers.append(question[\"answer\"])\n",
    "                trainAnswers.append(sentsAnswers)\n",
    "            theSents = trainAnswers\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taggedTrain = getStanfordTagging('train')\n",
    "taggedDev = getStanfordTagging('dev')\n",
    "taggedTest = getStanfordTagging('test')\n",
    "\n",
    "taggedTrainAnswers = getStanfordTagging('ansTrain')\n",
    "taggedDevAnswers = getStanfordTagging('ansDev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Given a stanford tagged list, refines the list by:\\n\",\n",
    "# Grouping all contiguous words with the same tag\\n\",\n",
    "# Relabels Organisations as Other\\n\",\n",
    "# Labels Number\\n\",\n",
    "def refineWordTags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif isCapitalised(word):\n",
    "                tag = 'OTHERCAP'\n",
    "        newWordTags.append((word, tag))\n",
    "    return groupSameTags(newWordTags)       \n",
    "\n",
    "def groupSameTags(taggedSent):\n",
    "    #taggedSent = addCommasToNums(taggedSent)\n",
    "    newWordTags = []\n",
    "    prevWord = taggedSent[0][0]\n",
    "    prevTag = taggedSent[0][1]\n",
    "    numInWord = 1\n",
    "    for (word,tag) in taggedSent[1:] :\n",
    "        if prevTag != tag and numInWord == 1 and prevTag == 'OTHERCAP':\n",
    "            prevTag = 'O'\n",
    "        if tag == prevTag:\n",
    "            prevWord += ' ' + word\n",
    "            numInWord += 1\n",
    "        else :\n",
    "            #Save prevWord, make new\n",
    "            newWordTags.append((prevWord, prevTag))\n",
    "            numInWord = 1\n",
    "            prevWord = word\n",
    "            prevTag = tag\n",
    "    newWordTags.append((prevWord, prevTag))\n",
    "    newWordTags = groupOthers(newWordTags)\n",
    "    return newWordTags\n",
    "        \n",
    "def groupOthers(wordTags):\n",
    "    newWordTags = []\n",
    "    for (word,tag) in wordTags:\n",
    "        if tag != 'O':\n",
    "            newWordTags.append((word,tag))\n",
    "        else :\n",
    "            newChunked = chunk(word)\n",
    "            if newChunked != []:\n",
    "                newWordTags.append(newChunked)\n",
    "    return newWordTags\n",
    "# Thanks : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "months = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    if word.lower() in months:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "chunkParser = nltk.RegexpParser(chunkRules)\n",
    "\n",
    "def chunk(words):\n",
    "    tokenWS = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    chunks =  chunkParser.parse(tokenWS)\n",
    "    possAnswers = []\n",
    "    for subtree in chunks.subtrees():\n",
    "        if subtree.label() == 'ANS':\n",
    "            possAnswers.append((' '.join(word for word, pos in subtree.leaves()),'O'))\n",
    "    #possAnswers.append((\"Nope\", \"CRAP\"))\n",
    "    return possAnswers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "def evaluateNER(questionsList,documentsList, numToEval):\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    correctTagCounter = Counter()\n",
    "    incorrectTagCounter = Counter()\n",
    "    for i in range (0, numToEval):\n",
    "        documents = documentsList[i]\n",
    "        questions = questionsList[i]\n",
    "        for j in range (0, len(questions)):\n",
    "            answer = questionsList[i][j][\"answer\"]\n",
    "            answerID = questionsList[i][j][\"answer_sentence\"]\n",
    "            possAnswers = refineWordTags(taggedDev[i][answerID])\n",
    "            inThere = False\n",
    "            for possAnswer in possAnswers:\n",
    "                if possAnswer[0] == answer:\n",
    "                    inThere = True\n",
    "                    correctTagCounter[possAnswer[1]] += 1\n",
    "                    break\n",
    "            if inThere:\n",
    "                correct.append((i,j, possAnswers))\n",
    "            else :\n",
    "                wrong.append((i,j, answer, possAnswers))\n",
    "                # Tag answer\n",
    "    return (correct, wrong, correctTagCounter, incorrectTagCounter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Correct : 2760\n",
      "Number incorrect: 5703\n",
      "Average correct : 0.326125487416\n"
     ]
    }
   ],
   "source": [
    "(corNER, wrongNER, correctTagCounter, incorrectTagCounter) = evaluateNER(dev, devSents,len(dev))\n",
    "\n",
    "print(\"Number Correct : \" + str(len(corNER)))\n",
    "print(\"Number incorrect: \" + str(len(wrongNER)))\n",
    "print(\"Average correct : \" + str((len(corNER) + 0.0) / (len(corNER)+len(wrongNER))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.326125487416 with no others!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n",
      "1073\n"
     ]
    }
   ],
   "source": [
    "# Visual Evaluation:\n",
    "\n",
    "def visualEvaluateNER():\n",
    "    numTrue = 0\n",
    "    numTrials = 0\n",
    "    for i in range(5):\n",
    "        trainy = train[i]\n",
    "        numTrials += len(trainy)\n",
    "        for qa in trainy:\n",
    "            answer = qa[\"answer\"]\n",
    "            answerID = qa[\"answer_sentence\"]\n",
    "            sentence = trainSents[i][answerID]\n",
    "            tags = refineWordTags(taggedTrain[i][answerID])\n",
    "            inThere = False\n",
    "            for possAnswer in tags:\n",
    "                if possAnswer[0] == answer:\n",
    "                    inThere = True\n",
    "                    break\n",
    "            if inThere:\n",
    "                numTrue += 1\n",
    "    print numTrue\n",
    "    print numTrials\n",
    "    \n",
    "        #print (\"Answer: \" + answer)\n",
    "        #print\n",
    "        #print (\"Sentence: \" + sentence)\n",
    "        #print\n",
    "        #print (tags)\n",
    "        #print\n",
    "        #print inThere\n",
    "        #print \n",
    "        #print\n",
    "visualEvaluateNER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35228331780055916"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "378 / 1073.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Buckingham Palace', 'O'), ('London', 'O'), ('capital of England', 'O'), ('the queen', 'O'), ('lives', 'O'), ('Nope', 'CRAP')]\n"
     ]
    }
   ],
   "source": [
    "words = \"Buckingham Palace, in London, is the capital of England, and where the queen lives\"\n",
    "print chunk(words)\n",
    "#print refineWordTags(taggedDev[1][1])\n",
    "#print devSents[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    tokenWS = nltk.pos_tag(nltk.word_tokenize(words))\n",
    "    chunks =  chunkParser.parse(tokenWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Buckingham/NNP\n",
      "  Palace/NNP\n",
      "  ,/,\n",
      "  in/IN\n",
      "  London/NNP\n",
      "  ,/,\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  capital/NN\n",
      "  of/IN\n",
      "  England/NNP\n",
      "  ,/,\n",
      "  and/CC\n",
      "  where/WRB\n",
      "  the/DT\n",
      "  queen/NN\n",
      "  lives/NNS)\n"
     ]
    }
   ],
   "source": [
    "#for subtree in chunks.subtrees():\n",
    "    #print subtree.label()\n",
    "#    if subtree.label() == 'S':\n",
    "#        for subtree2 in subtree.subtrees():\n",
    "            #print subtree2.label()\n",
    "            #print (' '.join(word for word, pos in t.leaves()),'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('NN',), 8929)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mostCommon = [(('NN',), 8929),\n",
    " (('CD',), 7556),\n",
    " (('NNP', 'NNP'), 5135),\n",
    " (('NNS',), 2316),\n",
    " (('JJ',), 2056),\n",
    " (('JJ', 'NN'), 1819),\n",
    " (('NNP', 'NNP', 'NNP'), 1528),\n",
    " (('NNP',), 1471),\n",
    " (('CD', 'NN'), 1124),\n",
    " (('JJ', 'NNS'), 1069),\n",
    " (('NN', 'NN'), 1063),\n",
    " (('NN', 'NNS'), 727),\n",
    " (('NNP', 'CD'), 625),\n",
    " (('CD', 'NNS'), 610),\n",
    " (('NNP', 'NN'), 551),\n",
    " (('DT', 'NNP', 'NNP'), 543),\n",
    " (('DT', 'NN'), 527),\n",
    " (('DT', 'JJ', 'NN'), 453),\n",
    " (('NNP', 'CD', ',', 'CD'), 418),\n",
    " (('JJ', 'NNP'), 394),\n",
    " (('CD', 'NNP', 'CD'), 387),\n",
    " (('CD', 'NNP'), 379),\n",
    " (('NNP', 'NNS'), 367),\n",
    " (('NNP', 'NNP', 'NNP', 'NNP'), 347),\n",
    " (('NNP', 'CC', 'NNP'), 341),\n",
    " (('DT', 'NNP'), 336),\n",
    " (('CD', 'CD'), 330),\n",
    " (('NNP', 'IN', 'NNP'), 320),\n",
    " (('NN', 'CC', 'NN'), 296),\n",
    " (('VBG',), 251),\n",
    " (('JJ', 'NN', 'NN'), 235),\n",
    " (('IN', 'CD'), 201),\n",
    " (('JJ', 'NN', 'NNS'), 200),\n",
    " (('DT', 'NN', 'NN'), 195),\n",
    " (('$', 'CD', 'CD'), 194),\n",
    " (('DT', 'NNP', 'NNP', 'NNP'), 188),\n",
    " (('RB',), 185),\n",
    " (('DT', 'NNP', 'NN'), 185),\n",
    " (('JJ', 'NNP', 'NNP'), 181),\n",
    " (('VBN',), 176),\n",
    " (('DT', 'NNP', 'IN', 'NNP'), 160),\n",
    " (('NN', 'IN', 'NN'), 156),\n",
    " (('NN', 'NNP'), 143),\n",
    " (('NNS', 'CC', 'NNS'), 138),\n",
    " (('JJ', 'JJ', 'NN'), 137),\n",
    " (('NNP', 'NNP', 'CC', 'NNP', 'NNP'), 135),\n",
    " (('JJ', 'CC', 'JJ'), 122),\n",
    " (('$', 'CD'), 119),\n",
    " (('NN', 'IN', 'NNP'), 117),\n",
    " (('NNP', 'NNP', 'IN', 'NNP'), 116)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NNP', 'NNP', 'CC', 'NNP', 'NNP'],\n",
       " ['NNP', 'NNP', 'IN', 'NNP'],\n",
       " ['DT', 'NNP', 'IN', 'NNP'],\n",
       " ['DT', 'NNP', 'NNP', 'NNP'],\n",
       " ['NNP', 'NNP', 'NNP', 'NNP']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mostCommonToList(mostCommon):\n",
    "    newList = []\n",
    "    for (tagTuple, val) in mostCommon[0:50]:\n",
    "        tagList = []\n",
    "        for tag in tagTuple:\n",
    "            tagList.append(tag)\n",
    "        newList.append(tagList)\n",
    "    return newList\n",
    "mostCommonList = mostCommonToList(mostCommon)\n",
    "mostCommonList = sorted(mostCommonList, key=len)\n",
    "mostCommonList = mostCommonList[::-1]\n",
    "mostCommonList[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANS: {<NNP><NNP><CC><NNP><NNP>}\n",
      "{<NNP><NNP><IN><NNP>}\n",
      "{<DT><NNP><IN><NNP>}\n",
      "{<DT><NNP><NNP><NNP>}\n",
      "{<NNP><NNP><NNP><NNP>}\n",
      "{<NNP><CD><,><CD>}\n",
      "{<NN><IN><NNP>}\n",
      "{<JJ><CC><JJ>}\n",
      "{<JJ><JJ><NN>}\n",
      "{<NNS><CC><NNS>}\n",
      "{<NN><IN><NN>}\n",
      "{<JJ><NNP><NNP>}\n",
      "{<DT><NNP><NN>}\n",
      "{<$><CD><CD>}\n",
      "{<DT><NN><NN>}\n",
      "{<JJ><NN><NNS>}\n",
      "{<JJ><NN><NN>}\n",
      "{<NN><CC><NN>}\n",
      "{<NNP><IN><NNP>}\n",
      "{<NNP><CC><NNP>}\n",
      "{<CD><NNP><CD>}\n",
      "{<DT><JJ><NN>}\n",
      "{<DT><NNP><NNP>}\n",
      "{<NNP><NNP><NNP>}\n",
      "{<$><CD>}\n",
      "{<NN><NNP>}\n",
      "{<IN><CD>}\n",
      "{<CD><CD>}\n",
      "{<DT><NNP>}\n",
      "{<NNP><NNS>}\n",
      "{<CD><NNP>}\n",
      "{<JJ><NNP>}\n",
      "{<DT><NN>}\n",
      "{<NNP><NN>}\n",
      "{<CD><NNS>}\n",
      "{<NNP><CD>}\n",
      "{<NN><NNS>}\n",
      "{<NN><NN>}\n",
      "{<JJ><NNS>}\n",
      "{<CD><NN>}\n",
      "{<JJ><NN>}\n",
      "{<NNP><NNP>}\n",
      "{<VBN>}\n",
      "{<RB>}\n",
      "{<VBG>}\n",
      "{<NNP>}\n",
      "{<JJ>}\n",
      "{<NNS>}\n",
      "{<CD>}\n",
      "{<NN>}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunkRules = \"ANS: \"\n",
    "for posTags in mostCommonList:\n",
    "    ruleInStr = \"{\"\n",
    "    for tag in posTags:\n",
    "        ruleInStr = ruleInStr + \"<\"+ tag + \">\"\n",
    "    chunkRules += ruleInStr + '}\\n'\n",
    "print chunkRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunkRules = \"\"\"\n",
    "ANS:    {<NNP><NNP><CC><NNP><NNP>}\n",
    "        {<NNP><NNP><IN><NNP>}\n",
    "        {<DT><NNP><IN><NNP>}\n",
    "        {<DT><NNP><NNP><NNP>}\n",
    "        {<NNP><NNP><NNP><NNP>}\n",
    "        {<NNP><CD><,><CD>}\n",
    "        {<NN><IN><NNP>}\n",
    "        {<JJ><CC><JJ>}\n",
    "        {<JJ><JJ><NN>}\n",
    "        {<NNS><CC><NNS>}\n",
    "        {<NN><IN><NN>}\n",
    "        {<JJ><NNP><NNP>}\n",
    "        {<DT><NNP><NN>}\n",
    "        {<$><CD><CD>}\n",
    "        {<DT><NN><NN>}\n",
    "        {<JJ><NN><NNS>}\n",
    "        {<JJ><NN><NN>}\n",
    "        {<NN><CC><NN>}\n",
    "        {<NNP><IN><NNP>}\n",
    "        {<NNP><CC><NNP>}\n",
    "        {<CD><NNP><CD>}\n",
    "        {<DT><JJ><NN>}\n",
    "        {<DT><NNP><NNP>}\n",
    "        {<NNP><NNP><NNP>}\n",
    "        {<$><CD>}\n",
    "        {<NN><NNP>}\n",
    "        {<IN><CD>}\n",
    "        {<CD><CD>}\n",
    "        {<DT><NNP>}\n",
    "        {<NNP><NNS>}\n",
    "        {<CD><NNP>}\n",
    "        {<JJ><NNP>}\n",
    "        {<DT><NN>}\n",
    "        {<NNP><NN>}\n",
    "        {<CD><NNS>}\n",
    "        {<NNP><CD>}\n",
    "        {<NN><NNS>}\n",
    "        {<NN><NN>}\n",
    "        {<JJ><NNS>}\n",
    "        {<CD><NN>}\n",
    "        {<JJ><NN>}\n",
    "        {<NNP><NNP>}\n",
    "        {<VBN>}\n",
    "        {<RB>}\n",
    "        {<VBG>}\n",
    "        {<NNP>}\n",
    "        {<JJ>}\n",
    "        {<NNS>}\n",
    "        {<CD>}\n",
    "        {<NN>}\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  A/DT\n",
      "  (ANS prison/NN riot/NN)\n",
      "  left/VBD\n",
      "  six/CD\n",
      "  (ANS members/NNS)\n",
      "  of/IN\n",
      "  staff/NN\n",
      "  needing/VBG\n",
      "  (ANS hospital/NN treatment/NN)\n",
      "  earlier/RBR\n",
      "  this/DT\n",
      "  month/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (ANS BBC/NNP)\n",
      "  (ANS learns/NNS))\n",
      "prison riot\n",
      "members\n",
      "hospital treatment\n",
      "BBC\n",
      "learns\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def prepareForNLP(text):\n",
    "\tsentences = nltk.sent_tokenize(text)\n",
    "\tsentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\tsentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "\treturn sentences\n",
    "\n",
    "def chunk(sentence):\n",
    "    chunkToExtract = \"\"\"\n",
    "    ANS: {<NNP>*}\n",
    "        {<DT>?<JJ>?<NNS>}\n",
    "        {<NN><NN>}\"\"\"\n",
    "    parser = nltk.RegexpParser(chunkToExtract)\n",
    "    result = parser.parse(sentence)\n",
    "    print result\n",
    "    for subtree in result.subtrees():\n",
    "\t\tif subtree.label() == 'ANS':\n",
    "\t\t\tt = subtree\n",
    "\t\t\tt = ' '.join(word for word, pos in t.leaves())\n",
    "\t\t\tprint(t)\n",
    "\n",
    "\n",
    "\n",
    "sentences = prepareForNLP(\"A prison riot left six members of staff needing hospital treatment earlier this month, the BBC learns\")\n",
    "for sentence in sentences:\n",
    "\tchunk(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
