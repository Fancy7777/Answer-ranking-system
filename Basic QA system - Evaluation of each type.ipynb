{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the python script containing the same code as the load the data notebook\n",
    "%run loadData.py\n",
    "# now we can access train, dev, and test\n",
    "# along with trainSents, devSents testSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from string import punctuation  \n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import csv\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)\n",
    "\n",
    "punct_tokens = set(punctuation)\n",
    "extra_tokens = set([\"what\", \"where\", \"how\", \"when\", \"who\"])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filter_tokens = extra_tokens.union(punct_tokens).union(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_train_set = getStanfordTagging('train')\n",
    "tagged_dev_set = getStanfordTagging('dev')\n",
    "tagged_test_set = getStanfordTagging('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_rapid_set = tagged_train_set[:rapid_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "# Thanks for this list to save me typing it : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "punctuation = ['.',',',';',':','`', \"'\"]\n",
    "\n",
    "def isPunctuation(word):\n",
    "    return word in punctuation\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Obtained from training data\n",
    "postUnits = [u'%', u'century', u'years', u'percent', u'years ago', u'days', u'months', u'km', u'hours', u'times', u'inches', u'\\xb0C', u'minutes', u'acres', u'\\xb0F', u'weeks', u'people', u'sq mi', u'mi', u'ft', u'feet', u'metres', u'mm', u'square miles', u'miles', u'pm', u'per cent', u'year', u'copies', u'yuan', u'men', u'square feet', u'third', u'kilometres', u'nm', u'tonnes', u'species', u'decades', u'barrels', u'tons', u'largest', u'centuries', u'km2']\n",
    "preUnits = [u'$',u'around', u'late', u'early', u'nearly', u'since', u'approximately', u'number']\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word.lower() in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def refine_word_tags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif isCapitalised(word):\n",
    "                tag = 'OTHERCAP'\n",
    "            elif word in preUnits:\n",
    "                tag = 'PRENUM'\n",
    "            elif word in postUnits:\n",
    "                tag = 'POSTNUM'\n",
    "            elif isStopWord(word):\n",
    "                tag = 'STOPWORD'\n",
    "            elif isPunctuation(word):\n",
    "                tag = 'PUNC'\n",
    "\n",
    "        newWordTags.append((word, tag))\n",
    "    \n",
    "    newWordTags = combineTags (newWordTags)\n",
    "    return newWordTags\n",
    "        \n",
    "def combineTags(wordTags):\n",
    "    \n",
    "    newTags = []\n",
    "    prevWord = wordTags[0][0]\n",
    "    prevTag = wordTags[0][1]\n",
    "    \n",
    "    for (word, tag) in wordTags[1:]:\n",
    "        if tag == 'NUMBER' and prevTag == 'PRENUM':\n",
    "            prevTag = 'NUMBER'\n",
    "        elif prevTag == 'PRENUM':\n",
    "            prevTag = 'O'\n",
    "        if tag == 'POSTNUM' and prevTag == \"NUMBER\":\n",
    "            tag = \"NUMBER\"\n",
    "        elif tag == \"POSTNUM\":\n",
    "            tag = \"O\"\n",
    "        newTags.append((prevWord, prevTag))\n",
    "        prevWord = word\n",
    "        prevTag = tag\n",
    "    newTags.append((prevWord, prevTag))\n",
    "        \n",
    "    newNewTags = []\n",
    "    prevWord = newTags[0][0]\n",
    "    prevTag = newTags[0][1]\n",
    "    if (prevTag == \"OTHERCAP\"):\n",
    "        prevTag = \"O\"\n",
    "        \n",
    "    for (word, tag) in newTags[1:]:\n",
    "#         print tag, prevTag\n",
    "        if tag == prevTag :\n",
    "            if word == '%':\n",
    "                prevWord += word\n",
    "            else :\n",
    "                if prevWord == '$':\n",
    "                    prevWord += word\n",
    "                else :\n",
    "                    prevWord += ' ' + word\n",
    "        else :\n",
    "            newNewTags.append((prevWord, prevTag))\n",
    "            prevWord = word\n",
    "            prevTag = tag\n",
    "            \n",
    "    newNewTags.append((prevWord, prevTag))\n",
    "    \n",
    "    return newNewTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def getAnswerDict(qss):\n",
    "    sentDicts = defaultdict(list)\n",
    "    for docID in range(0, len(qss)):\n",
    "        qs = qss[docID]\n",
    "        for q in qs:\n",
    "            answer = q[\"answer\"]\n",
    "            answerSent = (docID, q[\"answer_sentence\"])\n",
    "            sentDicts[answerSent].append(answer)\n",
    "    return sentDicts\n",
    "train_sentence_contained_answers = getAnswerDict(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'long playing', u'12\", 10\", 7\"', u'rpm']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentence_contained_answers[(0,2)] # contains the answers in that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAnswerIndex(sent, answer):\n",
    "    tokenised_answer = nltk.word_tokenize(answer)\n",
    "    len_tokenised_answer = len(tokenised_answer)\n",
    "    tokenised_sent =  nltk.word_tokenize(sent)\n",
    "    highest_index = (len(tokenised_sent) - len_tokenised_answer) + 1\n",
    "    \n",
    "    for i in range (0, highest_index):\n",
    "            sentence_fragment = tokenised_sent[i:i+len_tokenised_answer]\n",
    "            if (sentence_fragment == tokenised_answer):\n",
    "                return (i, i+len_tokenised_answer)\n",
    "    #print \"Problem, cannot find answer index\"\n",
    "    #print sent\n",
    "    #print answer\n",
    "    #print\n",
    "    return (-1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gramophone record (phonograph record in American English) or vinyl record, commonly known as a \"record\", is an analogue sound storage medium in the form of a flat polyvinyl chloride (previously shellac) disc with an inscribed, modulated spiral groove.\n",
      "\n",
      "[u'analogue sound storage medium']\n",
      "\n",
      "(24, 28)\n"
     ]
    }
   ],
   "source": [
    "the_sentence = trainSents[0][0]\n",
    "the_answers_contained = train_sentence_contained_answers[(0,0)]\n",
    "\n",
    "print the_sentence\n",
    "print\n",
    "print the_answers_contained\n",
    "print\n",
    "print getAnswerIndex (the_sentence, the_answers_contained[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_candidates_to_range(candidates):\n",
    "    candidatesIndexRange = []\n",
    "    index = 0\n",
    "    for (words, tag) in candidates:\n",
    "        tokenisedWords = nltk.word_tokenize(words)\n",
    "        nextIndex = index + len(tokenisedWords)\n",
    "        candidatesIndexRange.append((index,nextIndex ))\n",
    "        index = nextIndex\n",
    "    return candidatesIndexRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6), (6, 7), (7, 9), (9, 10), (10, 11)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_candidates = refine_word_tags(tagged_train_set[0][0])\n",
    "\n",
    "convert_candidates_to_range(example_candidates)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the taggings of the (possibly overlapping) candidates list\n",
    "\n",
    "def get_taggings_in_range((ans_start,ans_end),candidates):\n",
    "    candidate_ranges = convert_candidates_to_range(candidates)\n",
    "    \n",
    "    current_considered_candidate_id = -1\n",
    "    candidates_containing_answer = []\n",
    "    \n",
    "    for (cand_start, cand_end) in candidate_ranges:\n",
    "        current_considered_candidate_id += 1\n",
    "        if ans_start == cand_start and ans_end == cand_end :\n",
    "            #candidates and answer the same\n",
    "            candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            break\n",
    "        if cand_start <= ans_start and cand_end >= ans_end :\n",
    "            #candidate contains the answer, but has extra words\n",
    "            candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            break\n",
    "\n",
    "        if cand_start >= ans_start or cand_end > ans_start:\n",
    "            if (cand_end <= ans_end):\n",
    "                candidates_containing_answer.append(current_considered_candidate_id)\n",
    "            else :\n",
    "                # we have finished, as \n",
    "                if (cand_start < ans_end ):\n",
    "                    candidates_containing_answer.append(current_considered_candidate_id)\n",
    "                break\n",
    "    return candidates_containing_answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The groove usually starts near', 'O'), (u'the', 'STOPWORD'), (u'periphery', u'O'), (u'and', 'STOPWORD'), (u'ends near', u'O'), (u'the', 'STOPWORD'), (u'center', u'O'), (u'of the', 'STOPWORD'), (u'disc', u'O'), (u'.', 'PUNC')]\n",
      "\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "example_candidates = refine_word_tags(tagged_train_set[0][1])\n",
    "example_answer_range = (3,6)\n",
    "print example_candidates\n",
    "print\n",
    "print get_taggings_in_range(example_answer_range,example_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_NER_tag_analysis() :\n",
    "    correct_taggings = []\n",
    "    incorrect_taggings = []\n",
    "    evil_ones = []\n",
    "    for i in range(0, len(trainSents)):\n",
    "        sent_set = trainSents[i]\n",
    "        for j in range(0, len(sent_set)):\n",
    "            sent = sent_set[j]\n",
    "            answers_in_sent = train_sentence_contained_answers[(i,j)]\n",
    "            candidates = refine_word_tags(tagged_train_set[i][j])\n",
    "            for answer in answers_in_sent:\n",
    "                (ans_start, ans_end) = getAnswerIndex(sent, answer)\n",
    "                if (ans_start, ans_end) == (-1,-1) :\n",
    "                    #question = train[i][j][\"question\"]\n",
    "                    evil_ones.append((i,j, answer))\n",
    "                    continue\n",
    "                   \n",
    "                candidate_ids = get_taggings_in_range((ans_start, ans_end),candidates)\n",
    "                if len(candidate_ids) == 1:\n",
    "                    possible_correct_candidate = candidates[candidate_ids[0]]\n",
    "                    if possible_correct_candidate[0] == answer :\n",
    "                        # Correct tagging!\n",
    "                        correct_taggings.append((possible_correct_candidate, i, j))\n",
    "                    else :\n",
    "                        incorrect_taggings.append(([possible_correct_candidate], answer, i, j))\n",
    "                else :\n",
    "                    incorrect_candidates = ([candidates[index] for index in candidate_ids],answer, i, j)\n",
    "                    incorrect_taggings.append(incorrect_candidates)\n",
    "        if (i == 10):\n",
    "            break\n",
    "    return (correct_taggings, incorrect_taggings, evil_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(correct_taggings, incorrect_taggings, evil_ones) = get_NER_tag_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print len(evil_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "correctCounter = Counter()\n",
    "for correct_tags in correct_taggings:\n",
    "    correctCounter[correct_tags[0][1]] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({u'LOCATION': 86,\n",
       "         'NUMBER': 317,\n",
       "         u'O': 154,\n",
       "         'OTHERCAP': 200,\n",
       "         u'PERSON': 141,\n",
       "         'STOPWORD': 1})"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "wrongCounter = Counter()\n",
    "wrongList = defaultdict(list)\n",
    "\n",
    "for wrong_tags in incorrect_taggings:\n",
    "    tagList = []\n",
    "    for (words, tag) in wrong_tags[0]:\n",
    "        tagList.append(tag)\n",
    "    wrongCounter[tuple(tagList)] += 1\n",
    "    wrongList[tuple(tagList)].append(wrong_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'O',), 148),\n",
       " (('O', 'STOPWORD', u'O'), 89),\n",
       " (('OTHERCAP', 'STOPWORD', 'OTHERCAP'), 49),\n",
       " (('NUMBER',), 33),\n",
       " (('STOPWORD', u'O'), 33),\n",
       " (('O', 'OTHERCAP'), 29),\n",
       " (('NUMBER', u'O'), 26),\n",
       " (('OTHERCAP', 'NUMBER', 'PUNC', 'NUMBER'), 26),\n",
       " (('OTHERCAP', u'O'), 23),\n",
       " ((u'O', 'STOPWORD', u'O', 'STOPWORD', u'O'), 23)]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongCounter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'the'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongList[('O', 'STOPWORD', u'O')][0][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testCounter = Counter()\n",
    "for tuplesy in wrongList[('O', 'STOPWORD', u'O')]:\n",
    "    testCounter[tuplesy[0][1][0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 26), (u'of', 11), (u'the', 10), (u'of the', 5), (u'or', 5)]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testCounter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Dev sent for all:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluateNERonDev():\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    for i in range(0, len(dev)):\n",
    "        qs = dev[i]\n",
    "        for j in range(0, len(qs)):\n",
    "            q = qs[j]\n",
    "            idSent  = q[\"answer_sentence\"]\n",
    "            sent = devSents[i][idSent]\n",
    "            answer = q[\"answer\"]\n",
    "            possAnswers = refine_word_tags(tagged_dev_set[i][idSent])\n",
    "            inThere = False\n",
    "            for possAnswer in possAnswers:\n",
    "                if possAnswer[0] == answer:\n",
    "                    inThere = True\n",
    "                    break\n",
    "            if inThere:\n",
    "                correct.append((i, j, idSent, possAnswers))\n",
    "            else :\n",
    "                wrong.append((i, j, idSent, possAnswers))\n",
    "    return (correct, wrong)\n",
    "\n",
    "\n",
    "(correct,wrong) = evaluateNERonDev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.469927921541\n"
     ]
    }
   ],
   "source": [
    "print (len(correct) + 0.0)/ (len(correct) + len(wrong))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy - 0.456693843791 BASE\n",
    "\n",
    "# Fix dollar space + % space! - 0.470873212809\n",
    "# Add ` and '  - 0.470991374217 loool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# POS tag on others!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def evaluate_ner(name, data):\n",
    "    \n",
    "    question_set = data[name][\"question_set\"]\n",
    "    b_output_answer_set = data[name][\"b_output_answer_set\"]\n",
    "    \n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    for result_b in b_output_answer_set:\n",
    "        \n",
    "        answer = question_set[result_b[\"set_index\"]][result_b[\"question_index\"]][\"answer\"]\n",
    "        \n",
    "        possible_candidates = result_b[\"candidates\"]\n",
    "        \n",
    "        answer_exists_in_candidates = False\n",
    "        \n",
    "        for candidate in possible_candidates:\n",
    "            \n",
    "            candidate_string = candidate[0]\n",
    "            \n",
    "            if candidate_string == answer:\n",
    "                \n",
    "                answer_exists_in_candidates = True\n",
    "                \n",
    "                break\n",
    "        \n",
    "        if answer_exists_in_candidates:\n",
    "            correct.append(result_b)\n",
    "        else :\n",
    "            wrong.append(result_b)\n",
    "            \n",
    "    return (correct, wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_ner(name, data, stats=False):\n",
    "    print \"Processing ner: \", name\n",
    "    process_part_b(name, data)\n",
    "    if stats:\n",
    "        process_generic(name, data, \"ner\", evaluate_ner)\n",
    "        \n",
    "        correct_ner = len(data[name][\"ner_correct\"])\n",
    "        correct_ret = len(data[name][\"retrieval_correct\"])\n",
    "        \n",
    "        avg = correct_ner * 1.0 / correct_ret\n",
    "        \n",
    "        print \"ner\".capitalize() + \" Correct Average of Previous %: \", avg\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ner:  rapid\n",
      "\n",
      "Part B Output: \n",
      "[   {   'candidates': [   (u'They', 'O'),\n",
      "                          (u'had a', 'STOPWORD'),\n",
      "                          (u'playing time', u'O'),\n",
      "                          (u'of', 'STOPWORD'),\n",
      "                          (u'eight minutes', 'NUMBER'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'question_index': 0,\n",
      "        'sentence_index': 149,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Ner Correct:  67\n",
      "Ner Wrong:  337\n",
      "Ner Total:  404\n",
      "Ner Overall Average %:  0.165841584158\n",
      "Ner Correct Average of Previous %:  0.429487179487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_ner(\"rapid\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ner:  train\n",
      "\n",
      "Part B Output: \n",
      "[   {   'candidates': [   (u'They', 'O'),\n",
      "                          (u'had a', 'STOPWORD'),\n",
      "                          (u'playing time', u'O'),\n",
      "                          (u'of', 'STOPWORD'),\n",
      "                          (u'eight minutes', 'NUMBER'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'question_index': 0,\n",
      "        'sentence_index': 149,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Ner Correct:  20067\n",
      "Ner Wrong:  50092\n",
      "Ner Total:  70159\n",
      "Ner Overall Average %:  0.286021750595\n",
      "Ner Correct Average of Previous %:  0.459419858513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_ner(\"train\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ner:  dev\n",
      "\n",
      "Part B Output: \n",
      "[   {   'candidates': [   (u'Infrared', 'O'),\n",
      "                          (u'is', 'STOPWORD'),\n",
      "                          (u'used', u'O'),\n",
      "                          (u'in', 'STOPWORD'),\n",
      "                          (u'night vision equipment', u'O'),\n",
      "                          (u'when there is', 'STOPWORD'),\n",
      "                          (u'insufficient visible light', u'O'),\n",
      "                          (u'to', 'STOPWORD'),\n",
      "                          (u'see', u'O'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'question_index': 0,\n",
      "        'sentence_index': 71,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Ner Correct:  2415\n",
      "Ner Wrong:  6048\n",
      "Ner Total:  8463\n",
      "Ner Overall Average %:  0.285359801489\n",
      "Ner Correct Average of Previous %:  0.477272727273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_ner(\"dev\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ner:  test\n",
      "\n",
      "Part B Output: \n",
      "[   {   'candidates': [   (u'a', 'STOPWORD'),\n",
      "                          (u'forgotten theatre', u'O'),\n",
      "                          (u'of the', 'STOPWORD'),\n",
      "                          (u'Crimean War', 'OTHERCAP'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'question_index': 0,\n",
      "        'sentence_index': 283,\n",
      "        'set_index': 0}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_ner(\"test\", DATA, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shim function for later clean\n",
    "\n",
    "def getQuestionType(question):\n",
    "    if 'Who' in question:\n",
    "        return \"PERSON\"\n",
    "    if 'where' in question:\n",
    "        return \"LOCATION\"\n",
    "    if 'How many' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'How much' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'When' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'what year' in question:\n",
    "        return \"NUMBER\"\n",
    "    else:\n",
    "        return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, answers whose content words all appear in the question should be ranked lowest.\n",
    "\n",
    "def first_filter(question, answer_entities):\n",
    "   \n",
    "    ranked_list = []\n",
    "    \n",
    "    question = set(pre_process_tf_idf(question))\n",
    "    \n",
    "#     print question\n",
    "#     print\n",
    "    \n",
    "    for entity in answer_entities:\n",
    "\n",
    "        raw_span = entity[0]\n",
    "        span_tag = entity[1]\n",
    "        \n",
    "        set_span = set(pre_process_tf_idf(raw_span))\n",
    "        \n",
    "        if span_tag != \"O\" and span_tag != \"STOPWORD\" and span_tag !=\"PUNC\":\n",
    "            \n",
    "            if set_span.issubset(question):\n",
    "                \n",
    "                ranked_list.append([entity, 1])\n",
    "#                 print \"IN\", raw_span, span_tag, set_span, question\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                ranked_list.append([entity, 2])\n",
    "#                 print \"OUT\", raw_span, span_tag, set_span, question\n",
    "    \n",
    "    return sorted(ranked_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, answers whose content words all appear in the question should be ranked lowest.\n",
    "\n",
    "def first_filter_object(question, answer_entities):\n",
    "   \n",
    "    ranked_list = []\n",
    "    \n",
    "    question = set(pre_process_tf_idf(question))\n",
    "    \n",
    "#     print question\n",
    "#     print\n",
    "    \n",
    "    for entity in answer_entities:\n",
    "\n",
    "        raw_span = entity[0]\n",
    "        span_tag = entity[1]\n",
    "        \n",
    "        set_span = set(pre_process_tf_idf(raw_span))\n",
    "        \n",
    "        if span_tag != \"STOPWORD\" and span_tag !=\"PUNC\": #span_tag != \"O\" and\n",
    "            \n",
    "            if span_tag == \"O\":\n",
    "                \n",
    "                if len(set_span) > 2:\n",
    "                    ranked_list.append([entity, 0])\n",
    "            \n",
    "            elif set_span.issubset(question):\n",
    "                \n",
    "                ranked_list.append([entity, 1])\n",
    "#                 print \"IN\", raw_span, span_tag, set_span, question\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                ranked_list.append([entity, 2])\n",
    "#                 print \"OUT\", raw_span, span_tag, set_span, question\n",
    "    \n",
    "    return sorted(ranked_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, answers whose content words all appear in the question should be ranked lowest.\n",
    "\n",
    "def first_filter_object_stop(question, answer_entities):\n",
    "   \n",
    "    ranked_list = []\n",
    "    \n",
    "    question = set(pre_process_tf_idf(question))\n",
    "    \n",
    "#     print question\n",
    "#     print\n",
    "    \n",
    "    for entity in answer_entities:\n",
    "\n",
    "        raw_span = entity[0]\n",
    "        span_tag = entity[1]\n",
    "        \n",
    "        set_span = set(pre_process_tf_idf(raw_span))\n",
    "        \n",
    "        if span_tag !=\"PUNC\": #span_tag != \"O\" and\n",
    "            \n",
    "            if span_tag == \"O\" or span_tag == \"STOPWORD\":\n",
    "                \n",
    "                if len(set_span) > 2:\n",
    "                    \n",
    "                    ranked_list.append([entity, 0])\n",
    "            \n",
    "            elif set_span.issubset(question):\n",
    "                \n",
    "                ranked_list.append([entity, 1])\n",
    "#                 print \"IN\", raw_span, span_tag, set_span, question\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                ranked_list.append([entity, 2])\n",
    "#                 print \"OUT\", raw_span, span_tag, set_span, question\n",
    "    \n",
    "    return sorted(ranked_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second, answers which match the question type should be ranked higher than those that don't; for this, you\n",
    "# should build a simple rule-based question type classifier based on key words (e.g. questions which contain \"who\" are\n",
    "# people).\n",
    "\n",
    "# First, answers whose content words all appear in the question should be ranked lowest.\n",
    "\n",
    "def second_filter(question, ranked_list):\n",
    "   \n",
    "    question_type = getQuestionType(question)\n",
    "#     print question_type\n",
    "    \n",
    "    for index, answer in enumerate(ranked_list):\n",
    "        \n",
    "        entity_tag = answer[0][1]\n",
    "        \n",
    "        if entity_tag == question_type:\n",
    "#             print \"MATCH\", answer[0], question_type, question\n",
    "            ranked_list[index].append(2)\n",
    "#             ranked_list[index][1] += 1\n",
    "        else:\n",
    "            ranked_list[index].append(1)\n",
    "#             ranked_list[index][1] -= 1\n",
    "            \n",
    "    return ranked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_process_open_class(line):\n",
    "    tokenized_sentence = word_tokenizer.tokenize(line.lower())\n",
    "    lemmatized_sentence = [lemmatize(token) for token in tokenized_sentence]\n",
    "    filtered_sentence = [token for token in lemmatized_sentence if token not in filter_tokens]\n",
    "    tagged_sent = nltk.pos_tag(lemmatized_sentence)\n",
    "    final = []\n",
    "    for word, tag in tagged_sent:\n",
    "        if \"V\" in tag or \"NN\" in tag:\n",
    "#             final.append((word,tag))\n",
    "            final.append(word)\n",
    "            \n",
    "#     print \"RESULT: \", final\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Third, among entities of the same type, the prefered entity should be the one which is closer in the sentence to a\n",
    "# closed-class word from the question.\n",
    "\n",
    "def third_filter(question, possAnswers, ranked_list):\n",
    "    \n",
    "    question = pre_process_open_class(question)\n",
    "\n",
    "    answer_sent = \" \".join([x[0] for x in possAnswers])\n",
    "    answer_sent = pre_process_tf_idf(answer_sent)\n",
    "    raw_answer_sent = \" \".join(answer_sent)\n",
    "    \n",
    "#     print \"QUESTION: \"\n",
    "#     pp.pprint(question)\n",
    "#     print \"ANSWER: \"\n",
    "#     pp.pprint(answer_sent)\n",
    "#     pp.pprint(raw_answer_sent)\n",
    "    \n",
    "    for index, answer in enumerate(ranked_list):\n",
    "\n",
    "        span_tag = answer[0][1]\n",
    "        raw_span = answer[0][0]\n",
    "\n",
    "        proc_span = pre_process_tf_idf(raw_span)\n",
    "\n",
    "        raw_proc_span = \" \".join(proc_span)\n",
    "        new_raw_proc_span = \"-\".join(proc_span)\n",
    "\n",
    "        raw_answer_sent = raw_answer_sent.replace(raw_proc_span, new_raw_proc_span)\n",
    "    \n",
    "    answer_sent = raw_answer_sent.split(\" \")\n",
    "    \n",
    "    avg_dict = defaultdict(float)\n",
    "    \n",
    "    for open_class in question:\n",
    "        \n",
    "        if open_class in answer_sent:\n",
    "            \n",
    "            open_class_locations = [i for i, x in enumerate(answer_sent) if x == open_class]\n",
    "            \n",
    "#             print \"OPEN CLASS: \", repr(open_class)\n",
    "\n",
    "            for index, answer in enumerate(ranked_list):\n",
    "\n",
    "                span_tag = answer[0][1]\n",
    "                raw_span = answer[0][0]\n",
    "\n",
    "                proc_span = pre_process_tf_idf(raw_span)\n",
    "                \n",
    "                raw_proc_span = \" \".join(proc_span)\n",
    "                new_raw_proc_span = \"-\".join(proc_span)\n",
    "                \n",
    "                proc_span_locations = [i for i, x in enumerate(answer_sent) if x == new_raw_proc_span]\n",
    "                \n",
    "                min_dist = len(answer_sent)\n",
    "                min_dist_ind = (None, None)\n",
    "                \n",
    "                for loc1 in proc_span_locations:\n",
    "                    \n",
    "                    for loc2 in open_class_locations:\n",
    "                        \n",
    "                        dist = abs(loc1 - loc2)\n",
    "                        \n",
    "                        if dist < min_dist:\n",
    "                            \n",
    "                            min_dist = dist\n",
    "                            min_dist_ind = (loc1, loc2)\n",
    "                \n",
    "#                 print \"PROC: \", proc_span_locations\n",
    "#                 print \"OPEN CLASS: \", open_class_locations                \n",
    "                scale = (len(answer_sent) - min_dist) * 1.0 / len(answer_sent)\n",
    "#                 print \"JOINT: \", min_dist_ind, scale\n",
    "                avg_dict[index] += scale\n",
    "#                 ranked_list[index][1] *= scale\n",
    "    \n",
    "    for key, value in avg_dict.iteritems():\n",
    "        ranked_list[key].append(value / len(question))\n",
    "\n",
    "    return ranked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_rank(ranking_list):\n",
    "    \n",
    "    new_ranking = []\n",
    "    \n",
    "    for rank in ranking_list:\n",
    "        \n",
    "        new_rank = ( rank[1] + rank[2] )\n",
    "        \n",
    "        if len(rank) == 4:\n",
    "             new_rank *= rank[3]\n",
    "        \n",
    "        new_ranking.append([rank[0], new_rank])\n",
    "        \n",
    "    return sorted(new_ranking, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_part_c_output(name, data):\n",
    "    \n",
    "    part_c_output = []\n",
    "\n",
    "    question_set = data[name][\"question_set\"]\n",
    "    document_set = data[name][\"document_set\"]\n",
    "        \n",
    "    b_output_answer_set = data[name][\"b_output_answer_set\"]\n",
    "    \n",
    "    for result_b in b_output_answer_set:\n",
    "        \n",
    "        question = question_set[result_b[\"set_index\"]][result_b[\"question_index\"]][\"question\"]\n",
    "        \n",
    "        first_pass = first_filter(question, result_b[\"candidates\"])\n",
    "        \n",
    "        second_pass = second_filter(question, first_pass)\n",
    "        \n",
    "        third_pass = third_filter(question, result_b[\"candidates\"], second_pass)\n",
    "        \n",
    "        fourth_pass = reduce_rank(third_pass)\n",
    "                \n",
    "#         pp.pprint(third_pass)\n",
    "\n",
    "        predicted_answer = None\n",
    "\n",
    "        if len(fourth_pass) > 0:\n",
    "\n",
    "            top_answer = fourth_pass[0]\n",
    "    #         pp.pprint(top_answer)        \n",
    "            predicted_answer = top_answer[0][0]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            first_pass = first_filter_object(question, result_b[\"candidates\"])\n",
    "\n",
    "            second_pass = second_filter(question, first_pass)\n",
    "\n",
    "            third_pass = third_filter(question, result_b[\"candidates\"], second_pass)\n",
    "\n",
    "            fourth_pass = reduce_rank(third_pass)   \n",
    "            \n",
    "            if len(fourth_pass) > 0:\n",
    "\n",
    "                top_answer = fourth_pass[0]\n",
    "        #         pp.pprint(top_answer)        \n",
    "                predicted_answer = top_answer[0][0]\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                first_pass = first_filter_object_stop(question, result_b[\"candidates\"])\n",
    "\n",
    "                second_pass = second_filter(question, first_pass)\n",
    "\n",
    "                third_pass = third_filter(question, result_b[\"candidates\"], second_pass)\n",
    "\n",
    "                fourth_pass = reduce_rank(third_pass)   \n",
    "\n",
    "                if len(fourth_pass) > 0:\n",
    "\n",
    "                    top_answer = fourth_pass[0]\n",
    "            #         pp.pprint(top_answer)        \n",
    "                    predicted_answer = top_answer[0][0]                \n",
    "                \n",
    "                else:\n",
    "\n",
    "        #         pp.pprint(top_answer)        \n",
    "                    predicted_answer = random.choice(result_b[\"candidates\"])[0]\n",
    "            \n",
    "        \n",
    "        result_c = {\n",
    "            \"set_index\"  : result_b[\"set_index\"],\n",
    "            \"question_index\" : result_b[\"question_index\"],\n",
    "            \"sentence_index\" : result_b[\"sentence_index\"],\n",
    "            \"candidates\": result_b[\"candidates\"],\n",
    "            \"ranked_answers\": fourth_pass,\n",
    "            \"predicted_answer\" : predicted_answer\n",
    "        }\n",
    "        \n",
    "        part_c_output.append(result_c)        \n",
    "\n",
    "    return part_c_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_part_c(name, data):\n",
    "    \n",
    "    data[name][\"c_output_answer_set\"] = generate_part_c_output(name, data)\n",
    "    \n",
    "    print\n",
    "    print \"Part C Output: \"\n",
    "    pp.pprint(data[name][\"c_output_answer_set\"][:rapid_size])\n",
    "    print    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluate_rank(name, data):\n",
    "    \n",
    "    question_set = data[name][\"question_set\"]\n",
    "    document_set = data[name][\"document_set\"]\n",
    "    \n",
    "    correct = []\n",
    "    wrong = []\n",
    "    \n",
    "    c_output_answer_set = data[name][\"c_output_answer_set\"]\n",
    "    \n",
    "    for result_c in c_output_answer_set:\n",
    "        \n",
    "        question = question_set[result_c[\"set_index\"]][result_c[\"question_index\"]][\"question\"]\n",
    "        answer =  question_set[result_c[\"set_index\"]][result_c[\"question_index\"]][\"answer\"]\n",
    "        \n",
    "        predicted_answer = result_c[\"predicted_answer\"]\n",
    "\n",
    "        if (predicted_answer == answer):\n",
    "            correct.append(result_c)\n",
    "        else :\n",
    "            wrong.append(result_c)\n",
    "#         break\n",
    "        #print correct\n",
    "    return (correct, wrong)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_rank(name, data, stats=False):\n",
    "    print \"Processing rank: \", name\n",
    "    process_part_c(name, data)\n",
    "    if stats:\n",
    "        process_generic(name, data, \"rank\", evaluate_rank)\n",
    "        \n",
    "        \n",
    "        correct_rank = len(data[name][\"rank_correct\"])\n",
    "        correct_ner = len(data[name][\"ner_correct\"])\n",
    "        \n",
    "        avg = correct_rank * 1.0 / correct_ner\n",
    "        \n",
    "        print \"rank\".capitalize() + \" Correct Average of Previous %: \", avg        \n",
    "        \n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rank:  rapid\n",
      "\n",
      "Part C Output: \n",
      "[   {   'candidates': [   (u'They', 'O'),\n",
      "                          (u'had a', 'STOPWORD'),\n",
      "                          (u'playing time', u'O'),\n",
      "                          (u'of', 'STOPWORD'),\n",
      "                          (u'eight minutes', 'NUMBER'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'predicted_answer': u'eight minutes',\n",
      "        'question_index': 0,\n",
      "        'ranked_answers': [[(u'eight minutes', 'NUMBER'), 0.4]],\n",
      "        'sentence_index': 149,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Rank Correct:  36\n",
      "Rank Wrong:  368\n",
      "Rank Total:  404\n",
      "Rank Overall Average %:  0.0891089108911\n",
      "Rank Correct Average of Previous %:  0.537313432836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_rank(\"rapid\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rank:  train\n",
      "\n",
      "Part C Output: \n",
      "[   {   'candidates': [   (u'They', 'O'),\n",
      "                          (u'had a', 'STOPWORD'),\n",
      "                          (u'playing time', u'O'),\n",
      "                          (u'of', 'STOPWORD'),\n",
      "                          (u'eight minutes', 'NUMBER'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'predicted_answer': u'eight minutes',\n",
      "        'question_index': 0,\n",
      "        'ranked_answers': [[(u'eight minutes', 'NUMBER'), 0.4]],\n",
      "        'sentence_index': 149,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Rank Correct:  9895\n",
      "Rank Wrong:  60264\n",
      "Rank Total:  70159\n",
      "Rank Overall Average %:  0.141036787868\n",
      "Rank Correct Average of Previous %:  0.493098121294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_rank(\"train\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rank:  dev\n",
      "\n",
      "Part C Output: \n",
      "[   {   'candidates': [   (u'Infrared', 'O'),\n",
      "                          (u'is', 'STOPWORD'),\n",
      "                          (u'used', u'O'),\n",
      "                          (u'in', 'STOPWORD'),\n",
      "                          (u'night vision equipment', u'O'),\n",
      "                          (u'when there is', 'STOPWORD'),\n",
      "                          (u'insufficient visible light', u'O'),\n",
      "                          (u'to', 'STOPWORD'),\n",
      "                          (u'see', u'O'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'predicted_answer': u'night vision equipment',\n",
      "        'question_index': 0,\n",
      "        'ranked_answers': [   [   (u'night vision equipment', u'O'),\n",
      "                                  0.26666666666666666],\n",
      "                              [   (u'insufficient visible light', u'O'),\n",
      "                                  0.19999999999999998]],\n",
      "        'sentence_index': 71,\n",
      "        'set_index': 0}]\n",
      "\n",
      "Rank Correct:  1135\n",
      "Rank Wrong:  7328\n",
      "Rank Total:  8463\n",
      "Rank Overall Average %:  0.134113198629\n",
      "Rank Correct Average of Previous %:  0.469979296066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_rank(\"dev\", DATA, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_wrong_debug(name, data):\n",
    "    \n",
    "    question_set = data[name][\"question_set\"]\n",
    "    document_set = data[name][\"document_set\"]\n",
    "    rank_wrong = data[name][\"rank_wrong\"]\n",
    "    \n",
    "    for result_wrong in rank_wrong:\n",
    "        \n",
    "        question = question_set[result_wrong[\"set_index\"]][result_wrong[\"question_index\"]]\n",
    "        candidate_sentence = document_set[result_wrong[\"set_index\"]][result_wrong[\"sentence_index\"]]\n",
    "        correct_sentence = document_set[result_wrong[\"set_index\"]][question[\"answer_sentence\"]]\n",
    "        \n",
    "        candidates = result_wrong[\"candidates\"]\n",
    "        ranked_answers = result_wrong[\"ranked_answers\"]\n",
    "        predicted_answer = result_wrong[\"predicted_answer\"]\n",
    "        \n",
    "        if question[\"answer_sentence\"] == result_wrong[\"sentence_index\"]:\n",
    "            \n",
    "            print \"=\" * 20\n",
    "            print \"=\" * 20\n",
    "        \n",
    "            print \"Question: \"\n",
    "            print\n",
    "            pp.pprint(question[\"question\"])\n",
    "\n",
    "            print\n",
    "            print \"Correct Sentence: (Part A)\"\n",
    "            print\n",
    "            pp.pprint(correct_sentence)\n",
    "            print\n",
    "            print \"Chosen Sentence: (Part A)\"\n",
    "            print\n",
    "            pp.pprint(candidate_sentence)\n",
    "            print\n",
    "\n",
    "            print \"Candidate Answers: (Part B)\"\n",
    "            print\n",
    "            pp.pprint(candidates)\n",
    "            print\n",
    "            print \"Ranked Answers: (Part C)\"\n",
    "            print\n",
    "            pp.pprint(ranked_answers)\n",
    "            print\n",
    "            print \"Predicted Answer: (Part C)\"\n",
    "            print\n",
    "            pp.pprint(predicted_answer)\n",
    "            print\n",
    "            print \"Correct Answer: (Part C)\"\n",
    "            print\n",
    "            pp.pprint(question[\"answer\"])     \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "Question: \n",
      "\n",
      "u'What was the primary use of a phonographic disc record?'\n",
      "\n",
      "Correct Sentence: (Part A)\n",
      "\n",
      "u'The phonograph disc record was the primary medium used for music reproduction until late in the 20th century, replacing the phonograph cylinder record\\u2013with which it had co-existed from the late 1880s through to the 1920s\\u2013by the late 1920s.'\n",
      "\n",
      "Chosen Sentence: (Part A)\n",
      "\n",
      "u'The phonograph disc record was the primary medium used for music reproduction until late in the 20th century, replacing the phonograph cylinder record\\u2013with which it had co-existed from the late 1880s through to the 1920s\\u2013by the late 1920s.'\n",
      "\n",
      "Candidate Answers: (Part B)\n",
      "\n",
      "[   (u'The phonograph disc record', 'O'),\n",
      "    (u'was the', 'STOPWORD'),\n",
      "    (u'primary medium used', u'O'),\n",
      "    (u'for', 'STOPWORD'),\n",
      "    (u'music reproduction', u'O'),\n",
      "    (u'until', 'STOPWORD'),\n",
      "    (u'late', 'O'),\n",
      "    (u'in the', 'STOPWORD'),\n",
      "    (u'20th century', 'NUMBER'),\n",
      "    (u',', 'PUNC'),\n",
      "    (u'replacing', u'O'),\n",
      "    (u'the', 'STOPWORD'),\n",
      "    (u'phonograph cylinder record\\u2013with', u'O'),\n",
      "    (u'which it had', 'STOPWORD'),\n",
      "    (u'co-existed', u'O'),\n",
      "    (u'from the', 'STOPWORD'),\n",
      "    (u'late 1880s', 'NUMBER'),\n",
      "    (u'through to the', 'STOPWORD'),\n",
      "    (u'1920s\\u2013by', 'NUMBER'),\n",
      "    (u'the', 'STOPWORD'),\n",
      "    (u'late 1920s', 'NUMBER'),\n",
      "    (u'.', 'PUNC')]\n",
      "\n",
      "Ranked Answers: (Part C)\n",
      "\n",
      "[   [(u'20th century', 'NUMBER'), 1.6500000000000001],\n",
      "    [(u'late 1880s', 'NUMBER'), 1.05],\n",
      "    [(u'1920s\\u2013by', 'NUMBER'), 0.9375],\n",
      "    [(u'late 1920s', 'NUMBER'), 0.8250000000000001]]\n",
      "\n",
      "Predicted Answer: (Part C)\n",
      "\n",
      "u'20th century'\n",
      "\n",
      "Correct Answer: (Part C)\n",
      "\n",
      "u'music reproduction'\n"
     ]
    }
   ],
   "source": [
    "log_wrong_debug(\"rapid\", DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rank:  test\n",
      "\n",
      "Part C Output: \n",
      "[   {   'candidates': [   (u'a', 'STOPWORD'),\n",
      "                          (u'forgotten theatre', u'O'),\n",
      "                          (u'of the', 'STOPWORD'),\n",
      "                          (u'Crimean War', 'OTHERCAP'),\n",
      "                          (u'.', 'PUNC')],\n",
      "        'predicted_answer': u'Crimean War',\n",
      "        'question_index': 0,\n",
      "        'ranked_answers': [[(u'Crimean War', 'OTHERCAP'), 2]],\n",
      "        'sentence_index': 283,\n",
      "        'set_index': 0}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_rank(\"test\", DATA, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_submit(name, data):\n",
    "    \n",
    "    headers = ['id', 'answer']\n",
    "    \n",
    "    c_output_answer_set = data[name][\"c_output_answer_set\"]       \n",
    "\n",
    "    with open(name + '.submit.csv', 'w') as f:\n",
    "\n",
    "        f_csv = csv.DictWriter(f, headers)\n",
    "        f_csv.writeheader()\n",
    "\n",
    "        for index, result_c in enumerate(c_output_answer_set):\n",
    "            \n",
    "            predicted_answer = result_c[\"predicted_answer\"]\n",
    "            \n",
    "            if predicted_answer is not None:\n",
    "                f_csv.writerows([{'id':index+1,'answer':predicted_answer.encode(\"utf-8\")}])\n",
    "            else:\n",
    "                f_csv.writerows([{'id':index+1,'answer':\"NONE\"}])\n",
    "            \n",
    "#             if isinstance( answer_list[index]['answer'], int):\n",
    "                \n",
    "#                 f_csv.writerows([{'id':index+1,'answer':answer_list[index]['answer'][0][0]}])\n",
    "                \n",
    "#             else:\n",
    "                \n",
    "#                 f_csv.writerows([{'id':index+1,'answer':answer_list[index]['answer'][0][0].encode(\"utf-8\")}])        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_submit(\"rapid\", DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_submit(\"test\", DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
