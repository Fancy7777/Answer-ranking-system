{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the python script containing the same code as the load the data notebook\n",
    "%run loadData.py\n",
    "# now we can access train, dev, and test\n",
    "# along with trainSents, devSents testSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = testSents[0]\n",
    "questions = test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joshi part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning functions\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Follow lemmatize function from guide notebook: WSTA_N1B_preprocessing.ipynb\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "word_tokenizer = nltk.tokenize.WordPunctTokenizer() #word_tokenize #tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "def pre_process(line):\n",
    "    tokenized_sentence = word_tokenizer.tokenize(line.lower())\n",
    "    lemmatized_sentence = [lemmatize(token) for token in tokenized_sentence]\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "def vectorize_documents(text_documents):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', tokenizer=pre_process)\n",
    "    vector_documents = vectorizer.fit_transform(text_documents)\n",
    "    \n",
    "    return [vector_documents, vectorizer]\n",
    "\n",
    "def vectorize_query(vectorizer, text_query):\n",
    "    return vectorizer.transform([text_query])\n",
    "\n",
    "def process_neighbours(vector_documents):\n",
    "    \n",
    "    neighbours = NearestNeighbors(1, algorithm=\"brute\", metric=\"cosine\")\n",
    "    neighbours.fit(vector_documents)\n",
    "    \n",
    "    return neighbours\n",
    "\n",
    "def closest_document(neighbours, vector_query):\n",
    "\n",
    "    result = neighbours.kneighbors(vector_query, 1, return_distance=True)\n",
    "\n",
    "    result_index = result[1][0][0]\n",
    "    result_distance = result[0][0][0]\n",
    "    \n",
    "    return [result_distance, result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generatePartAOutput(qs, sents):\n",
    "    # Output for part A\n",
    "    partAOutput = []\n",
    "    for i in range (0, len(qs)):\n",
    "        documents = sents[i]\n",
    "        questions = qs[i]\n",
    "\n",
    "        vector_documents, vectorizer = vectorize_documents(documents)\n",
    "        analyze = vectorizer.build_analyzer()\n",
    "        neighbours = process_neighbours(vector_documents)\n",
    "\n",
    "        for j in range (0, len(questions)):\n",
    "            text_query = questions[j][\"question\"]\n",
    "            vector_query = vectorize_query(vectorizer, text_query)\n",
    "            result_similarity, result_index  = closest_document(neighbours, vector_query)\n",
    "            partAOutput.append((i,j,result_index))\n",
    "    return partAOutput\n",
    "\n",
    "partADevAnswers = generatePartAOutput(dev, devSents)\n",
    "partATestAnswers = generatePartAOutput(test, testSents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alex part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# The required jar files : https://nlp.stanford.edu/software/CRF-NER.shtml#Download\n",
    "# It's 171mb so I've added to the gitignore\n",
    "# If you download it, and rename the folder name \"stanford\" in the main directory\n",
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle # Useful for read / write of list file\n",
    "import os #Needed to check if file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets store the stanford tagger output in a file\n",
    "# This function returns the tagging output of stanford for each dataset\n",
    "# with datasetName - 'train', 'dev', test' \n",
    "\n",
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taggedTrain = getStanfordTagging('train')\n",
    "taggedDev = getStanfordTagging('dev')\n",
    "taggedTest = getStanfordTagging('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Given a stanford tagged list, refines the list by:\\n\",\n",
    "# Grouping all contiguous words with the same tag\\n\",\n",
    "# Relabels Organisations as Other\\n\",\n",
    "# Labels Number\\n\",\n",
    "def refineWordTags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif isCapitalised(word):\n",
    "                tag = 'OTHERCAP'\n",
    "            elif word in preUnits:\n",
    "                tag = 'PRENUM'\n",
    "            elif word in postUnits:\n",
    "                tag = 'POSTNUM'\n",
    "            elif isStopWord(word):\n",
    "                tag = 'STOPWORD'\n",
    "            elif isPunctuation(word):\n",
    "                tag = 'PUNC'\n",
    "\n",
    "        newWordTags.append((word, tag))\n",
    "    \n",
    "    newWordTags = combineTags (newWordTags)\n",
    "    return newWordTags\n",
    "        \n",
    "def combineTags(wordTags):\n",
    "    newTags = []\n",
    "    prevWord = wordTags[0][0]\n",
    "    prevTag = wordTags[0][1]\n",
    "    for (word, tag) in wordTags[1:]:\n",
    "        if tag == 'NUMBER' and prevTag == 'PRENUM':\n",
    "            prevTag = 'NUMBER'\n",
    "        elif prevTag == 'PRENUM':\n",
    "            prevTag = 'O'\n",
    "        if tag == 'POSTNUM' and prevTag == \"NUMBER\":\n",
    "            tag = \"NUMBER\"\n",
    "        elif tag == \"POSTNUM\":\n",
    "            tag = \"O\"\n",
    "        newTags.append((prevWord, prevTag))\n",
    "        prevWord = word\n",
    "        prevTag = tag\n",
    "    newTags.append((prevWord, prevTag))\n",
    "    \n",
    "    newNewTags = []\n",
    "    prevWord = newTags[0][0]\n",
    "    prevTag = newTags[0][1]\n",
    "    if (prevTag == \"OTHERCAP\"):\n",
    "        prevTag = \"O\"\n",
    "    for (word, tag) in wordTags[1:]:            \n",
    "        if tag == prevTag :\n",
    "            prevWord += ' ' + word\n",
    "        else :\n",
    "            newNewTags.append((prevWord, prevTag))\n",
    "            prevWord = word\n",
    "            prevTag = tag\n",
    "    newNewTags.append((prevWord, prevTag))\n",
    "    return newNewTags\n",
    "\n",
    "# Thanks for this list to save me typing it : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "punctuation = ['.',',',';',':']\n",
    "\n",
    "def isPunctuation(word):\n",
    "    return word in punctuation\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Obtained from training data\n",
    "postUnits = [u'%', u'century', u'years', u'percent', u'years ago', u'days', u'months', u'km', u'hours', u'times', u'inches', u'\\xb0C', u'minutes', u'acres', u'\\xb0F', u'weeks', u'people', u'sq mi', u'mi', u'ft', u'feet', u'metres', u'mm', u'square miles', u'miles', u'pm', u'per cent', u'year', u'copies', u'yuan', u'men', u'square feet', u'third', u'kilometres', u'nm', u'tonnes', u'species', u'decades', u'barrels', u'tons', u'largest', u'centuries', u'km2']\n",
    "preUnits = [u'$', u'around', u'late', u'early', u'nearly', u'since', u'approximately', u'number']\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word.lower() in stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the output for part B:\n",
    "# In the form [(docID, questID, entities)]\n",
    "def generatePartBOutput(qs, partAOutput, taggedSents):   \n",
    "    partBOutput = []\n",
    "    for (docIndex, questionIndex, sentenceIndex) in partAOutput:\n",
    "        stanfordTags = taggedSents[docIndex][sentenceIndex]\n",
    "        newTags = refineWordTags(stanfordTags)\n",
    "        question = qs[docIndex][questionIndex][\"question\"]\n",
    "        partBOutput.append((docIndex, questionIndex, sentenceIndex, newTags))\n",
    "    return partBOutput\n",
    "\n",
    "partBDevAnswers = generatePartBOutput(dev, partADevAnswers, taggedDev)\n",
    "partBTestAnswers = generatePartBOutput(test, partATestAnswers, taggedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 0,\n",
       " 1,\n",
       " [(u'Night-vision devices using active near-infrared illumination allow', 'O'),\n",
       "  (u'people', 'POSTNUM'),\n",
       "  (u'or', 'STOPWORD'),\n",
       "  (u'animals', u'O'),\n",
       "  (u'to be', 'STOPWORD'),\n",
       "  (u'observed without', u'O'),\n",
       "  (u'the', 'STOPWORD'),\n",
       "  (u'observer', u'O'),\n",
       "  (u'being', 'STOPWORD'),\n",
       "  (u'detected', u'O'),\n",
       "  (u'.', 'PUNC')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partBDevAnswers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Example output:\n",
    "print \"\\nDocIndex: \" + partBTestAnswers[0][0]\n",
    "print \"\\nQuestionIndex: \" + partBTestAnswers[0][1]\n",
    "print \n",
    "\n",
    "print \"\\nPossibleAnswers: \" \n",
    "print partBTestAnswers[0][3]\n",
    "\n",
    "#Okay so this one is wrong, but hey! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick evaluation of the first two parts on the dev set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateNERonDev():\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    for (docIndex, questionIndex, sentenceIndex, newTags) in partBDevAnswers:\n",
    "        answer = dev[docIndex][questionIndex][\"answer\"]\n",
    "        possAnswers = newTags \n",
    "        inThere = False\n",
    "        for possAnswer in newTags:\n",
    "            if possAnswer[0] == answer:\n",
    "                inThere = True\n",
    "                break\n",
    "        if inThere:\n",
    "            correct.append((docIndex, questionIndex, sentenceIndex, newTags))\n",
    "        else :\n",
    "            wrong.append((docIndex, questionIndex, sentenceIndex, newTags))\n",
    "    return (correct, wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2409\n",
      "6054\n"
     ]
    }
   ],
   "source": [
    "(nerDevCorrect, nerDevWrong) = evaluateNERonDev()\n",
    "print len(nerDevCorrect)\n",
    "print len(nerDevWrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluateAnswerRanking(questionsList,documentsList, numToEval):\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    #(corNER, wrongNER) = evaluateNER(questionsList,documentsList, numToEval)\n",
    "    (nerDevCorrect, nerDevWrong) = evaluateNERonDev()\n",
    "    entityListsWithAnswer = nerDevCorrect\n",
    "    for (i,j,possAnswers) in entityListsWithAnswer:\n",
    "        question = questionsList[i][j][\"question\"]\n",
    "        answer =  questionsList[i][j][\"answer\"]\n",
    "        print question\n",
    "        print possAnswers\n",
    "        #print answer\n",
    "        answerPredicited = third_filter(question,second_filter(question,first_filter(question, possAnswers)),second_filter(question,first_filter(question, possAnswers))['answer_entities_list']) \n",
    "        #print answerPredicited\n",
    "        #print '%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%'\n",
    "        # TODO add Dereks part here\n",
    "        if (answerPredicited == answer):\n",
    "            correct.append((i,j))\n",
    "        else :\n",
    "            wrong.append((i,j,answerPredicited))\n",
    "        #print correct\n",
    "    return (correct, wrong)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-c9eb44d1d380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mcorAns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrongAns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluateAnswerRanking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevSents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number Correct : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorAns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number incorrect: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrongAns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Average correct : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorAns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorAns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrongAns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-1132cdf03a56>\u001b[0m in \u001b[0;36mevaluateAnswerRanking\u001b[0;34m(questionsList, documentsList, numToEval)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mnerDevCorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnerDevWrong\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluateNERonDev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mentityListsWithAnswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnerDevCorrect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpossAnswers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentityListsWithAnswer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestionsList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mquestionsList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "(corAns, wrongAns) = evaluateAnswerRanking(dev, devSents,len(dev))\n",
    "print(\"Number Correct : \" + str(len(corAns)))\n",
    "print(\"Number incorrect: \" + str(len(wrongAns)))\n",
    "print (\"Average correct : \" + str((len(corAns) + 0.0) / (len(corAns)+len(wrongAns))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derek part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_openclass_word(sameword_list):\n",
    "    tagged_text = nltk.pos_tag(sameword_list)\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    is_verb = lambda pos: pos[:2] == 'VB'\n",
    "    is_adjective = lambda pos: pos[:2] == 'JJ'\n",
    "    is_adverb = lambda pos: pos[:2] == 'RB'\n",
    "    \n",
    "    nouns = [word for (word, pos) in tagged_text if is_noun(pos)] \n",
    "    verbs = [word for (word, pos) in tagged_text if is_verb(pos)] \n",
    "    adjectives = [word for (word, pos) in tagged_text if is_adjective(pos)] \n",
    "    adverbs = [word for (word, pos) in tagged_text if is_adverb(pos)] \n",
    "    return nouns+adjectives+adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a question, returns a tag for the answer form\n",
    "# From PERSON, LOCATION, NUMBER, OTHER \n",
    "# Assuming question is lowercased\n",
    "def getQuestionType(question):\n",
    "    if 'Who' in question:\n",
    "        return \"PERSON\"\n",
    "    if 'where' in question:\n",
    "        return \"LOCATION\"\n",
    "    if 'How many' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'How much' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'When' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'what year' in question:\n",
    "        return \"NUMBER\"\n",
    "    if 'What year' in question:\n",
    "        return \"NUMBER\"\n",
    "    else:\n",
    "        return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove the non-word entities from answer-entities\n",
    "def process_answer_entities(anwser_entities):\n",
    "    new_anwser_entities =[]\n",
    "    for (entity,entity_type) in anwser_entities:\n",
    "        tokenized_entity = word_tokenize(entity)\n",
    "        #print tokenized_entity\n",
    "        temp = []\n",
    "        for ele in tokenized_entity:\n",
    "            if ele not in non_words:\n",
    "                temp.append(ele)\n",
    "        temp = ' '.join(temp)\n",
    "        new_anwser_entities.append((temp,entity_type))\n",
    "    return new_anwser_entities      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#first:  whose anwsers all appear in the questions rank the lowest\n",
    "#assumption: input question in its dictionary value\n",
    "from string import punctuation  \n",
    "non_words = list(punctuation)\n",
    "\n",
    "def first_filter(question, anwser_entities):\n",
    "    ranking_dict_1 = {}\n",
    "    ranking_list = []\n",
    "    merge_list = []\n",
    "    answer_entities_list = []\n",
    "    anwser_entities = process_answer_entities(anwser_entities)\n",
    "    \n",
    "    for entity in anwser_entities:\n",
    "        answer_entities_list.append(entity[0])\n",
    "        if entity[0] in question:\n",
    "            #print entity[0]\n",
    "            #if entity[0].lower() not in stop_words and entity[0]!='' and entity[0].lower() not in non_words:\n",
    "            if entity[1] != 'STOPWORD' and entity[1] !='PUNC':   \n",
    "                #print entity[0]\n",
    "                merge_list.append(entity[0])\n",
    "        else:\n",
    "            #if entity[0].lower() not in stop_words and entity[0]!='' and entity[0].lower() not in non_words:\n",
    "            if entity[1] != 'STOPWORD' and entity[1] !='PUNC':  \n",
    "                ranking_list.append(entity)\n",
    "                \n",
    "    #get rid of the not noun phrases   \n",
    "    #print merge_list\n",
    "    \n",
    "    final_merge_list = []\n",
    "    for phrase in merge_list:\n",
    "        tokenized_phrase = word_tokenize(phrase)\n",
    "        for word in tokenized_phrase:\n",
    "            if word not in stop_words and word.lower() not in non_words:\n",
    "                #print phrase\n",
    "                final_merge_list.append(phrase)\n",
    "                \n",
    "    final_ranking_list = []            \n",
    "    for entity in ranking_list:\n",
    "        if entity[1] != 'STOPWORD':\n",
    "            final_ranking_list.append(entity)\n",
    "    \n",
    "    #detect the open-class word in here for easier process            \n",
    "    final_merge_list = set(detect_openclass_word(merge_list))\n",
    "    ranking_dict_1[\"ranking_list\"] = final_ranking_list\n",
    "    ranking_dict_1[\"same_word_list\"] = final_merge_list\n",
    "    ranking_dict_1[\"answer_entities_list\"] = answer_entities_list\n",
    "    return ranking_dict_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#second: answers which match the question type should be ranked higher than those that dont\n",
    "\n",
    "#assumption: save questions' type in the dictionary format quesiton1 = \n",
    "#{u'answer': u'long playing',u'answer_sentence': 2, u'question':......., 'question_type:'PERSON'}\n",
    "\n",
    "def second_filter(question, ranking_dict_1):\n",
    "    question_with_type ={}\n",
    "    question_with_type['question_type']= getQuestionType(question)\n",
    "    question_with_type['question'] = question\n",
    "    #print question_with_type\n",
    "    ranking_dict_2 = {}\n",
    "    ranking_list =[]\n",
    "    merge_list = []\n",
    "    for entity in ranking_dict_1[\"ranking_list\"]:\n",
    "        if question_with_type['question_type'] == 'O':\n",
    "            ranking_list.append(entity[0])\n",
    "        else:\n",
    "            if entity[1] == question_with_type['question_type']:\n",
    "                ranking_list.append(entity[0])\n",
    "            else:\n",
    "                merge_list.append(entity[0])\n",
    "    ranking_dict_2[\"same_word_list\"] = ranking_dict_1[\"same_word_list\"]\n",
    "    ranking_dict_2[\"ranking_list\"] = ranking_list\n",
    "    ranking_dict_2[\"Other_tags_list\"] = merge_list\n",
    "    ranking_dict_2[\"answer_entities_list\"] = ranking_dict_1[\"answer_entities_list\"]\n",
    "    return ranking_dict_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Thrid: based on second, the prefered entity should be the one which is close in \n",
    "#the sentence to a closed-class word form the question\n",
    "from collections import OrderedDict\n",
    "\n",
    "def cal_distance_words(entity,same_words, anwser_entities):\n",
    "    temp = 0\n",
    "    for same_word in same_words:\n",
    "        temp += abs(anwser_entities.index(entity) - anwser_entities.index(same_word))\n",
    "    return float(temp)/float(len(same_words))\n",
    "\n",
    "def sort_orderedDict(orderdict):\n",
    "    return OrderedDict(sorted(orderdict.items(), key = lambda x:x[1], reverse = False))\n",
    "        \n",
    "\n",
    "def third_filter(question,second_filter,anwser_entities):\n",
    "    dict_ranking ={}\n",
    "    if (len(second_filter[\"same_word_list\"])==0):\n",
    "        if len(second_filter['ranking_list']) != 0:\n",
    "            return second_filter['ranking_list'][0]\n",
    "        else:\n",
    "            return second_filter[\"Other_tags_list\"][0]\n",
    "            #for entity in second_filter[\"Other_tags_list\"]:\n",
    "                #dict_ranking[entity]= cal_distance_words(entity, second_filter[\"same_word_list\"],anwser_entities)\n",
    "            #dict_ranking = sort_orderedDict(dict_ranking)\n",
    "            #if len(dict_ranking.items()) ==0:\n",
    "                #return 0\n",
    "            #else:\n",
    "            #return dict_ranking.items()[0][0]\n",
    "    else:\n",
    "        #if len(second_filter['ranking_list']) != 0:\n",
    "        for entity in second_filter[\"ranking_list\"]:\n",
    "            dict_ranking[entity]= cal_distance_words(entity, second_filter[\"same_word_list\"],anwser_entities)\n",
    "        #else:\n",
    "            #if len(second_filter[\"Other_tags_list\"]) !=0:\n",
    "                #for entity in second_filter[\"Other_tags_list\"]:\n",
    "                    #dict_ranking[entity]= cal_distance_words(entity, second_filter[\"same_word_list\"],anwser_entities)\n",
    "            #print dict_ranking\n",
    "            #else:\n",
    "                #return 0\n",
    "        dict_ranking = sort_orderedDict(dict_ranking)\n",
    "            #print dict_ranking\n",
    "        if len(dict_ranking.items()) ==0:\n",
    "            return 0\n",
    "        else:\n",
    "            return dict_ranking.items()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Along with industrial and medical, in what applications is infrared radiation used?\n",
    "[(u'Infrared radiation', 'O'), (u'is', 'STOPWORD'), (u'used', u'O'), (u'in', 'STOPWORD'), (u'industrial', u'O'), (u',', 'PUNC'), (u'scientific', u'O'), (u',', 'PUNC'), (u'and', 'STOPWORD'), (u'medical applications', u'O'), (u'.', 'PUNC')]\n",
    "scientific\n",
    "Infrared radiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question = \"Along with industrial and medical, in what applications is infrared radiation used?\"\n",
    "a = [(u'Infrared radiation', 'O'), (u'is', 'STOPWORD'), (u'used', u'O'), (u'in', 'STOPWORD'), (u'industrial', u'O'), (u',', 'PUNC'), (u'scientific', u'O'), (u',', 'PUNC'), (u'and', 'STOPWORD'), (u'medical applications', u'O'), (u'.', 'PUNC')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used\n",
      "industrial\n",
      "{'ranking_list': [(u'Infrared radiation', 'O'), (u'scientific', u'O'), (u'medical applications', u'O')], 'answer_entities_list': [u'Infrared radiation', u'is', u'used', u'in', u'industrial', '', u'scientific', '', u'and', u'medical applications', ''], 'same_word_list': set([u'industrial'])}\n"
     ]
    }
   ],
   "source": [
    "test = first_filter(question,a)\n",
    "print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ranking_list': [u'Infrared radiation', u'scientific', u'medical applications'], 'Other_tags_list': [], 'answer_entities_list': [u'Infrared radiation', u'is', u'used', u'in', u'industrial', '', u'scientific', '', u'and', u'medical applications', ''], 'same_word_list': set([u'industrial'])}\n"
     ]
    }
   ],
   "source": [
    "test2 = second_filter(question, test)\n",
    "print test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scientific\n"
     ]
    }
   ],
   "source": [
    "#print cal_distance_words('Melbourne',test2[\"same_word_list\"],test2[\"answer_entities_list\"])\n",
    "test3 = third_filter(question, test2,test2[\"answer_entities_list\"])\n",
    "print test3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each question, evaluate if the answer is present as an entity\n",
    "\n",
    "def evaluateAnswerRanking(questionsList,documentsList, numToEval):\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    (nerDevCorrect, nerDevWrong) = evaluateNERonDev()\n",
    "    #(corNER, wrongNER) = evaluateNER()\n",
    "    entityListsWithAnswer = nerDevCorrect\n",
    "    for (i,j,something_not_sure,possAnswers) in entityListsWithAnswer:\n",
    "        question = questionsList[i][j][\"question\"]\n",
    "        answer =  questionsList[i][j][\"answer\"]\n",
    "        #print question\n",
    "        #print possAnswers\n",
    "        #print answer\n",
    "        answerPredicited = third_filter(question,second_filter(question,first_filter(question, possAnswers)),second_filter(question,first_filter(question, possAnswers))['answer_entities_list']) \n",
    "        #print answerPredicited\n",
    "        #print '%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%'\n",
    "        # TODO add Dereks part here\n",
    "        if (answerPredicited == answer):\n",
    "            correct.append((i,j))\n",
    "        else :\n",
    "            wrong.append((i,j,answerPredicited))\n",
    "        #print correct\n",
    "    return (correct, wrong)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-c9eb44d1d380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mcorAns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrongAns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluateAnswerRanking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevSents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number Correct : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorAns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number incorrect: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrongAns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Average correct : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorAns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorAns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrongAns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-a3872983fe16>\u001b[0m in \u001b[0;36mevaluateAnswerRanking\u001b[0;34m(questionsList, documentsList, numToEval)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print possAnswers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#print answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0manswerPredicited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthird_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msecond_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfirst_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossAnswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msecond_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfirst_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossAnswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer_entities_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m#print answerPredicited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#print '%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-a18b26799430>\u001b[0m in \u001b[0;36mfirst_filter\u001b[0;34m(question, anwser_entities)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m#detect the open-class word in here for easier process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mfinal_merge_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_openclass_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mranking_dict_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ranking_list\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_ranking_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mranking_dict_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"same_word_list\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_merge_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-9f7febe7f342>\u001b[0m in \u001b[0;36mdetect_openclass_word\u001b[0;34m(sameword_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_openclass_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msameword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtagged_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msameword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mis_noun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mis_verb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'VB'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mis_adjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'JJ'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \"\"\"\n\u001b[1;32m    126\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-ptb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m'!YEAR'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m'!DIGITS'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "(corAns, wrongAns) = evaluateAnswerRanking(dev, devSents,len(dev))\n",
    "print(\"Number Correct : \" + str(len(corAns)))\n",
    "print(\"Number incorrect: \" + str(len(wrongAns)))\n",
    "print (\"Average correct : \" + str((len(corAns) + 0.0) / (len(corAns)+len(wrongAns))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derek submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer_list = []\n",
    "for ele in partBOutput:\n",
    "    answer_dict = {}\n",
    "    question = ele[0]\n",
    "    possAnswers = ele[1]\n",
    "    answer = third_filter(question,second_filter(question,first_filter(question, possAnswers)),second_filter(question,first_filter(question, possAnswers))['answer_entities_list'])\n",
    "    answer_dict['sentence'] = possAnswers\n",
    "    answer_dict['question'] = question\n",
    "    answer_dict['answer'] = answer\n",
    "    answer_list.append(answer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "headers = ['id', 'answer']\n",
    "\n",
    "with open('submit.csv','w') as f:\n",
    "    f_csv = csv.DictWriter(f, headers)\n",
    "    f_csv.writeheader()\n",
    "    for index in range(len(answer_list)):\n",
    "        if isinstance( answer_list[index]['answer'], int):\n",
    "            f_csv.writerows([{'id':index+1,'answer':answer_list[index]['answer']}])\n",
    "        else:\n",
    "            f_csv.writerows([{'id':index+1,'answer':answer_list[index]['answer'].encode(\"utf-8\")}])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
