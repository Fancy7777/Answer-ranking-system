{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the python script containing the same code as the load the data notebook\n",
    "%run loadData.py\n",
    "# now we can access train, dev, and test\n",
    "# along with trainSents, devSents testSents\n",
    "documents = testSents[0]\n",
    "questions = test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning functions\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Follow lemmatize function from guide notebook: WSTA_N1B_preprocessing.ipynb\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "word_tokenizer = nltk.tokenize.WordPunctTokenizer() #word_tokenize #tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "def pre_process(line):\n",
    "    tokenized_sentence = word_tokenizer.tokenize(line.lower())\n",
    "    lemmatized_sentence = [lemmatize(token) for token in tokenized_sentence]\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Core functions\n",
    "\n",
    "def vectorize_documents(text_documents):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', tokenizer=pre_process)\n",
    "    vector_documents = vectorizer.fit_transform(text_documents)\n",
    "    \n",
    "    return [vector_documents, vectorizer]\n",
    "\n",
    "def vectorize_query(vectorizer, text_query):\n",
    "    return vectorizer.transform([text_query])\n",
    "\n",
    "def process_neighbours(vector_documents):\n",
    "    \n",
    "    neighbours = NearestNeighbors(1, algorithm=\"brute\", metric=\"cosine\")\n",
    "    neighbours.fit(vector_documents)\n",
    "    \n",
    "    return neighbours\n",
    "\n",
    "def closest_document(neighbours, vector_query):\n",
    "\n",
    "    result = neighbours.kneighbors(vector_query, 1, return_distance=True)\n",
    "\n",
    "    result_index = result[1][0][0]\n",
    "    result_distance = result[0][0][0]\n",
    "    \n",
    "    return [result_distance, result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generatePartAOutput(qs, sents):\n",
    "    # Output for part A\n",
    "    partAOutput = []\n",
    "    for i in range (0, len(qs)):\n",
    "        documents = sents[i]\n",
    "        questions = qs[i]\n",
    "\n",
    "        vector_documents, vectorizer = vectorize_documents(documents)\n",
    "        analyze = vectorizer.build_analyzer()\n",
    "        neighbours = process_neighbours(vector_documents)\n",
    "\n",
    "        for j in range (0, len(questions)):\n",
    "            text_query = questions[j][\"question\"]\n",
    "            vector_query = vectorize_query(vectorizer, text_query)\n",
    "            result_similarity, result_index  = closest_document(neighbours, vector_query)\n",
    "            partAOutput.append((i,j,result_index))\n",
    "    return partAOutput\n",
    "\n",
    "partADevAnswers = generatePartAOutput(dev, devSents)\n",
    "partATestAnswers = generatePartAOutput(test, testSents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets get the stanford taggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# The required jar files : https://nlp.stanford.edu/software/CRF-NER.shtml#Download\n",
    "# It's 171mb so I've added to the gitignore\n",
    "# If you download it, and rename the folder name \"stanford\" in the main directory\n",
    "classifier = './stanford/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = './stanford/stanford-ner.jar'\n",
    "\n",
    "sTagger = StanfordNERTagger(classifier,jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle # Useful for read / write of list file\n",
    "import os #Needed to check if file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets store the stanford tagger output in a file\n",
    "# This function returns the tagging output of stanford for each dataset\n",
    "# with datasetName - 'train', 'dev', test' \n",
    "\n",
    "def getStanfordTagging(datasetName):\n",
    "    fnameTrain = './preCompTags/stanfordTaggedTrain.txt'\n",
    "    fnameDev = './preCompTags/stanfordTaggedDev.txt'\n",
    "    fnameTest = './preCompTags/stanfordTaggedTest.txt'\n",
    "    \n",
    "    theFilePath = ''\n",
    "    theSents = []\n",
    "    if (datasetName == 'train'):\n",
    "        theFilePath = fnameTrain\n",
    "        theSents = trainSents\n",
    "    elif (datasetName == 'dev'):\n",
    "        theFilePath = fnameDev\n",
    "        theSents = devSents\n",
    "    elif (datasetName == 'test'):\n",
    "        theFilePath = fnameTest\n",
    "        theSents = testSents\n",
    "    else :\n",
    "        raise ValueError('Incorrect datasetName: ' + datasetName + ', choose from - \"train\", \"dev\", \"test\" ') \n",
    "    if (os.path.exists(theFilePath)):\n",
    "        with open(theFilePath, \"rb\") as fp:\n",
    "            stanfordTags = pickle.load(fp)\n",
    "            return stanfordTags\n",
    "    \n",
    "    else :\n",
    "        #Need to create taggings!\n",
    "        taggedSentsList = []\n",
    "        for sents in theSents:\n",
    "            tokenisedSents = [word_tokenize(sent) for sent in sents]\n",
    "            classifiedSents = sTagger.tag_sents(tokenisedSents)\n",
    "            taggedSentsList.append(classifiedSents)\n",
    "        #And save them\n",
    "        with open(theFilePath, \"wb\") as fp: \n",
    "            pickle.dump(taggedSentsList, fp)\n",
    "        return taggedSentsList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taggedTrain = getStanfordTagging('train')\n",
    "taggedDev = getStanfordTagging('dev')\n",
    "taggedTest = getStanfordTagging('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Given a stanford tagged list, refines the list by:\\n\",\n",
    "# Grouping all contiguous words with the same tag\\n\",\n",
    "# Relabels Organisations as Other\\n\",\n",
    "# Labels Number\\n\",\n",
    "def refineWordTags(taggedWordList):\n",
    "    newWordTags = []\n",
    "    for (word, tag) in taggedWordList:\n",
    "        if (tag == 'ORGANIZATION'):\n",
    "            tag = 'O'\n",
    "        if (tag == 'O'):\n",
    "            #Might be a number\n",
    "            if isNumber(word):\n",
    "                tag = 'NUMBER'\n",
    "            elif isCapitalised(word):\n",
    "                tag = 'OTHERCAP'\n",
    "            elif isStopWord(word):\n",
    "                tag = 'STOPWORD'\n",
    "            elif isPunctuation(word):\n",
    "                tag = 'PUNC'\n",
    "\n",
    "        newWordTags.append((word, tag))\n",
    "    \n",
    "    return newWordTags\n",
    "\n",
    "# Thanks for this list to save me typing it : http://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\\n\",\n",
    "numInWords = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"\n",
    "       , \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "punctuation = ['.',',',';',':']\n",
    "\n",
    "def isPunctuation(word):\n",
    "    return word in punctuation\n",
    "def isCapitalised (word):\n",
    "    if len(word) == 0:\n",
    "        return False\n",
    "    return word[0].isupper()\n",
    "\n",
    "# Obtained from training data\n",
    "postUnits = [u'%', u'century', u'years', u'percent', u'years ago', u'days', u'months', u'km', u'hours', u'times', u'inches', u'\\xb0C', u'minutes', u'acres', u'\\xb0F', u'weeks', u'people', u'sq mi', u'mi', u'ft', u'feet', u'metres', u'mm', u'square miles', u'miles', u'pm', u'per cent', u'year', u'copies', u'yuan', u'men', u'square feet', u'third', u'kilometres', u'nm', u'tonnes', u'species', u'decades', u'barrels', u'tons', u'largest', u'centuries', u'km2']\n",
    "preUnits = [u'$', u'around', u'late', u'early', u'nearly', u'since', u'approximately', u'number']\n",
    "\n",
    "# Returns true if the word represents a number\\n\",\n",
    "def isNumber(word):\n",
    "    pattern = \".?(\\\\d)+((,|.)(\\\\d)+)*\"\n",
    "    if re.match(pattern,word) :\n",
    "        return True\n",
    "    if word.lower() in numInWords:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word.lower() in stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def refineTheTags(dataset):\n",
    "    tempList = []\n",
    "    for doc in dataset:\n",
    "        innerList = []\n",
    "        for sent in doc:\n",
    "            innerList.append(refineWordTags(sent))\n",
    "        tempList.append(innerList)\n",
    "    return tempList\n",
    "\n",
    "refinedTrain = refineTheTags(taggedTrain)\n",
    "refinedDev = refineTheTags(taggedDev)\n",
    "refinedTest = refineTheTags(taggedTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bigram Creation Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def getAnswerDict(qss):\n",
    "    sentDicts = defaultdict(list)\n",
    "    for docID in range(0, len(qss)):\n",
    "        qs = qss[docID]\n",
    "        for q in qs:\n",
    "            answer = q[\"answer\"]\n",
    "            answerSent = (docID, q[\"answer_sentence\"])\n",
    "            sentDicts[answerSent].append(answer)\n",
    "    return sentDicts\n",
    "trainSentDicts = getAnswerDict(train)\n",
    "devSentDicts = getAnswerDict(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 25, 26, 27]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function returns a list of the indexes of words that are an answer to a question in the train set\n",
    "def getAnswerPos(docID, sentID, sents, answerDict):\n",
    "    sententence = sents[docID][sentID]\n",
    "    listAnswers = answerDict[(docID, sentID)]\n",
    "    tokenisedSent =  nltk.word_tokenize(sententence)\n",
    "    answerPosList = []\n",
    "    for ans in listAnswers:\n",
    "        tokenisedAns = nltk.word_tokenize(ans)\n",
    "        # Inefficient\n",
    "        \n",
    "        for i in range (0, len(tokenisedSent) - len(tokenisedAns)):\n",
    "            answerFragment = tokenisedSent[i:i+len(tokenisedAns)]\n",
    "            if (answerFragment == tokenisedAns):\n",
    "                for j in range (0, len(tokenisedAns)):\n",
    "                    answerPosList.append(i+ j)\n",
    "                break\n",
    "    return answerPosList\n",
    "\n",
    "getAnswerPos(0,0,trainSents, trainSentDicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModelData(docID, sentID, sents, stanfordTags, answerDict):\n",
    "    combTags = []\n",
    "    theSent = sents[docID][sentID]\n",
    "    posTags = nltk.pos_tag(nltk.word_tokenize(theSent))\n",
    "    stanfordTags =refineWordTags(stanfordTags[docID][sentID])\n",
    "    if len(posTags) != len(stanfordTags):\n",
    "        print(docID, sentID)\n",
    "        return[]\n",
    "    answerPos = getAnswerPos(docID, sentID, sents , answerDict)\n",
    "    #print answerPos\n",
    "    for i in range(0, len(posTags)):\n",
    "        word = posTags[i][0]\n",
    "        posTag = posTags[i][1]\n",
    "        stanfordTag = stanfordTags[i][1]\n",
    "        answer = i in answerPos\n",
    "        combTags.append((posTag, stanfordTag, answer))\n",
    "    return combTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 172)\n",
      "(209, 364)\n",
      "(329, 133)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "answerCounter = Counter()\n",
    "for i in range(0, len(trainSents)):\n",
    "    for j in range(0, len(trainSents[i])):\n",
    "        groupTags = getModelData(i,j,trainSents, taggedTrain, trainSentDicts)\n",
    "        for k in range(0, len(groupTags) - 2):\n",
    "            answerCounter[tuple(groupTags[k:k+2])] += 1\n",
    "        if len(groupTags) > 0:\n",
    "            answerCounter[('S',groupTags[0])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting From Model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupTags = getModelData(0,0,devSents, taggedDev, devSentDicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'scientific']\n",
      "Infrared radiation is used in industrial, scientific, and medical applications.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('VBN', 'OTHERCAP', False),\n",
       " ('NN', u'O', False),\n",
       " ('VBZ', 'STOPWORD', False),\n",
       " ('VBN', u'O', False),\n",
       " ('IN', 'STOPWORD', False),\n",
       " ('JJ', u'O', False),\n",
       " (',', 'PUNC', False),\n",
       " ('JJ', u'O', True),\n",
       " (',', 'PUNC', False),\n",
       " ('CC', 'STOPWORD', False),\n",
       " ('JJ', u'O', False),\n",
       " ('NNS', u'O', False),\n",
       " ('.', 'PUNC', False)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print devSentDicts[(0,0)]\n",
    "print devSents[0][0]\n",
    "groupTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "447\n"
     ]
    }
   ],
   "source": [
    "print answerCounter[('S',('VBN', 'OTHERCAP', True) )]\n",
    "print answerCounter[('S',('VBN', 'OTHERCAP', False) )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictAnswer(groupTags):\n",
    "    newTags = []\n",
    "    if len(groupTags) == 0:\n",
    "        return newTags\n",
    "    prevTag = 'S'\n",
    "    multiplier = 1\n",
    "    for tag in groupTags:\n",
    "        trueTag = (tag[0], tag[1], True)\n",
    "        falseTag = (tag[0], tag[1], False)\n",
    "        numTrue = answerCounter[(prevTag, trueTag)]\n",
    "        numFalse = answerCounter[(prevTag, falseTag)]\n",
    "        if numTrue * multiplier >= numFalse:\n",
    "            newTags.append(trueTag)\n",
    "            prevTag = trueTag\n",
    "            multiplier = 1\n",
    "        else :\n",
    "            newTags.append(falseTag)\n",
    "            prevTag = falseTag\n",
    "            multiplier = 50\n",
    "    return newTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Night-vision\n",
      "('NN', 'OTHERCAP', False)\n",
      "('NN', 'OTHERCAP', False)\n",
      "\n",
      "devices\n",
      "('NNS', u'O', False)\n",
      "('NNS', u'O', False)\n",
      "\n",
      "using\n",
      "('VBG', u'O', False)\n",
      "('VBG', u'O', False)\n",
      "\n",
      "active\n",
      "('JJ', u'O', True)\n",
      "('JJ', u'O', True)\n",
      "\n",
      "near-infrared\n",
      "('JJ', u'O', True)\n",
      "('JJ', u'O', True)\n",
      "\n",
      "illumination\n",
      "('NN', u'O', True)\n",
      "('NN', u'O', True)\n",
      "\n",
      "allow\n",
      "('IN', u'O', False)\n",
      "('IN', u'O', True)\n",
      "\n",
      "people\n",
      "('NNS', u'O', False)\n",
      "('NNS', u'O', True)\n",
      "\n",
      "or\n",
      "('CC', 'STOPWORD', False)\n",
      "('CC', 'STOPWORD', True)\n",
      "\n",
      "animals\n",
      "('NNS', u'O', False)\n",
      "('NNS', u'O', True)\n",
      "\n",
      "to\n",
      "('TO', 'STOPWORD', False)\n",
      "('TO', 'STOPWORD', True)\n",
      "\n",
      "be\n",
      "('VB', 'STOPWORD', False)\n",
      "('VB', 'STOPWORD', True)\n",
      "\n",
      "observed\n",
      "('VBN', u'O', False)\n",
      "('VBN', u'O', True)\n",
      "\n",
      "without\n",
      "('IN', u'O', False)\n",
      "('IN', u'O', True)\n",
      "\n",
      "the\n",
      "('DT', 'STOPWORD', False)\n",
      "('DT', 'STOPWORD', True)\n",
      "\n",
      "observer\n",
      "('NN', u'O', False)\n",
      "('NN', u'O', True)\n",
      "\n",
      "being\n",
      "('VBG', 'STOPWORD', False)\n",
      "('VBG', 'STOPWORD', True)\n",
      "\n",
      "detected\n",
      "('VBN', u'O', False)\n",
      "('VBN', u'O', True)\n",
      "\n",
      ".\n",
      "('.', 'PUNC', False)\n",
      "('.', 'PUNC', True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "theSent =  devSents[0][1]\n",
    "groupTags = getModelData(0,1,devSents, taggedDev, devSentDicts)\n",
    "prediction  = predictAnswer(groupTags)\n",
    "tokenSent = nltk.word_tokenize(theSent)\n",
    "for i in range (0, len(tokenSent)):\n",
    "    print tokenSent[i]\n",
    "    print groupTags[i]\n",
    "    print prediction[i]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
